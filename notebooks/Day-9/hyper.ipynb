{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating breakthrough features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'harmonic_mean_prop{j}'] = 1 / harmonic_mean\n",
      "/tmp/ipykernel_4390/243682087.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'geometric_mean_prop{j}'] = np.exp(log_geo_mean)\n",
      "/tmp/ipykernel_4390/243682087.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'dominant_prop{j}'] = df.apply(lambda row:\n",
      "/tmp/ipykernel_4390/243682087.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'blend_balance_prop{j}'] = 1 - df[frac_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'blend_diversity_prop{j}'] = df[frac_cols].std(axis=1) / (df[frac_cols].mean(axis=1) + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_sum'] = df[frac_cols].sum(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_std'] = df[frac_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_skew'] = df[frac_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_kurtosis'] = df[frac_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_entropy'] = -sum(df[f'Component{i}_fraction'] * np.log(df[f'Component{i}_fraction'] + 1e-8) for i in range(1, 6))\n",
      "/tmp/ipykernel_4390/243682087.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_gini'] = 1 - sum(df[f'Component{i}_fraction'] ** 2 for i in range(1, 6))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
      "/tmp/ipykernel_4390/243682087.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
      "/tmp/ipykernel_4390/243682087.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_mean_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'weighted_var_prop{j}'] = sum(\n",
      "/tmp/ipykernel_4390/243682087.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'harmonic_mean_prop{j}'] = 1 / harmonic_mean\n",
      "/tmp/ipykernel_4390/243682087.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'geometric_mean_prop{j}'] = np.exp(log_geo_mean)\n",
      "/tmp/ipykernel_4390/243682087.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'dominant_prop{j}'] = df.apply(lambda row:\n",
      "/tmp/ipykernel_4390/243682087.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'blend_balance_prop{j}'] = 1 - df[frac_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'blend_diversity_prop{j}'] = df[frac_cols].std(axis=1) / (df[frac_cols].mean(axis=1) + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
      "/tmp/ipykernel_4390/243682087.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'ron_like_blend_prop{j}'] = ron_blend\n",
      "/tmp/ipykernel_4390/243682087.py:106: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
      "/tmp/ipykernel_4390/243682087.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'density_blend_prop{j}'] = density_blend\n",
      "/tmp/ipykernel_4390/243682087.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'rvp_blend_prop{j}'] = rvp_blend\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
      "/tmp/ipykernel_4390/243682087.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
      "/tmp/ipykernel_4390/243682087.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_sum'] = df[frac_cols].sum(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_std'] = df[frac_cols].std(axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_skew'] = df[frac_cols].apply(lambda row: skew(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_kurtosis'] = df[frac_cols].apply(lambda row: kurtosis(row), axis=1)\n",
      "/tmp/ipykernel_4390/243682087.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_entropy'] = -sum(df[f'Component{i}_fraction'] * np.log(df[f'Component{i}_fraction'] + 1e-8) for i in range(1, 6))\n",
      "/tmp/ipykernel_4390/243682087.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['frac_gini'] = 1 - sum(df[f'Component{i}_fraction'] ** 2 for i in range(1, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling NaN values...\n",
      "Handling NaN values...\n",
      "Performing feature selection...\n",
      "Original features: 433\n",
      "Selected features: 227\n",
      "Training Breakthrough Ensemble...\n",
      "Features: 433 (selected: 227)\n",
      "\n",
      "Training for BlendProperty1...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.658822\n",
      "[400]\tvalid_0's l1: 0.555859\n",
      "[600]\tvalid_0's l1: 0.481634\n",
      "[800]\tvalid_0's l1: 0.426586\n",
      "[1000]\tvalid_0's l1: 0.383944\n",
      "[1200]\tvalid_0's l1: 0.350733\n",
      "[1400]\tvalid_0's l1: 0.325188\n",
      "[1600]\tvalid_0's l1: 0.304542\n",
      "[1800]\tvalid_0's l1: 0.287788\n",
      "[2000]\tvalid_0's l1: 0.274215\n",
      "[2200]\tvalid_0's l1: 0.2629\n",
      "[2400]\tvalid_0's l1: 0.253431\n",
      "[2600]\tvalid_0's l1: 0.245593\n",
      "[2800]\tvalid_0's l1: 0.238928\n",
      "[3000]\tvalid_0's l1: 0.233089\n",
      "[3200]\tvalid_0's l1: 0.228454\n",
      "[3400]\tvalid_0's l1: 0.224477\n",
      "[3600]\tvalid_0's l1: 0.221075\n",
      "[3800]\tvalid_0's l1: 0.218047\n",
      "[4000]\tvalid_0's l1: 0.215134\n",
      "[4200]\tvalid_0's l1: 0.212573\n",
      "[4400]\tvalid_0's l1: 0.210522\n",
      "[4600]\tvalid_0's l1: 0.207972\n",
      "[4800]\tvalid_0's l1: 0.205602\n",
      "[5000]\tvalid_0's l1: 0.203757\n",
      "[5200]\tvalid_0's l1: 0.202374\n",
      "[5400]\tvalid_0's l1: 0.201045\n",
      "[5600]\tvalid_0's l1: 0.199539\n",
      "[5800]\tvalid_0's l1: 0.197982\n",
      "[6000]\tvalid_0's l1: 0.196564\n",
      "[6200]\tvalid_0's l1: 0.195274\n",
      "[6400]\tvalid_0's l1: 0.193961\n",
      "[6600]\tvalid_0's l1: 0.192824\n",
      "[6800]\tvalid_0's l1: 0.191811\n",
      "[7000]\tvalid_0's l1: 0.190877\n",
      "[7200]\tvalid_0's l1: 0.189391\n",
      "[7400]\tvalid_0's l1: 0.187865\n",
      "[7600]\tvalid_0's l1: 0.186795\n",
      "[7800]\tvalid_0's l1: 0.186713\n",
      "[8000]\tvalid_0's l1: 0.185706\n",
      "[8200]\tvalid_0's l1: 0.184685\n",
      "[8400]\tvalid_0's l1: 0.184034\n",
      "[8600]\tvalid_0's l1: 0.183522\n",
      "[8800]\tvalid_0's l1: 0.183108\n",
      "[9000]\tvalid_0's l1: 0.182738\n",
      "[9200]\tvalid_0's l1: 0.182437\n",
      "[9400]\tvalid_0's l1: 0.182134\n",
      "[9600]\tvalid_0's l1: 0.181794\n",
      "[9800]\tvalid_0's l1: 0.181534\n",
      "[10000]\tvalid_0's l1: 0.181428\n",
      "[10200]\tvalid_0's l1: 0.181298\n",
      "[10400]\tvalid_0's l1: 0.181027\n",
      "[10600]\tvalid_0's l1: 0.180719\n",
      "[10800]\tvalid_0's l1: 0.180369\n",
      "[11000]\tvalid_0's l1: 0.18001\n",
      "[11200]\tvalid_0's l1: 0.17964\n",
      "[11400]\tvalid_0's l1: 0.179348\n",
      "[11600]\tvalid_0's l1: 0.179019\n",
      "[11800]\tvalid_0's l1: 0.178778\n",
      "[12000]\tvalid_0's l1: 0.178398\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.178398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.660942\n",
      "[400]\tvalid_0's l1: 0.558553\n",
      "[600]\tvalid_0's l1: 0.480034\n",
      "[800]\tvalid_0's l1: 0.417343\n",
      "[1000]\tvalid_0's l1: 0.370788\n",
      "[1200]\tvalid_0's l1: 0.335246\n",
      "[1400]\tvalid_0's l1: 0.307613\n",
      "[1600]\tvalid_0's l1: 0.285916\n",
      "[1800]\tvalid_0's l1: 0.268543\n",
      "[2000]\tvalid_0's l1: 0.255056\n",
      "[2200]\tvalid_0's l1: 0.24484\n",
      "[2400]\tvalid_0's l1: 0.236367\n",
      "[2600]\tvalid_0's l1: 0.229232\n",
      "[2800]\tvalid_0's l1: 0.223278\n",
      "[3000]\tvalid_0's l1: 0.218231\n",
      "[3200]\tvalid_0's l1: 0.214045\n",
      "[3400]\tvalid_0's l1: 0.210303\n",
      "[3600]\tvalid_0's l1: 0.207113\n",
      "[3800]\tvalid_0's l1: 0.204117\n",
      "[4000]\tvalid_0's l1: 0.201305\n",
      "[4200]\tvalid_0's l1: 0.198914\n",
      "[4400]\tvalid_0's l1: 0.1969\n",
      "[4600]\tvalid_0's l1: 0.194861\n",
      "[4800]\tvalid_0's l1: 0.193161\n",
      "[5000]\tvalid_0's l1: 0.191085\n",
      "[5200]\tvalid_0's l1: 0.18947\n",
      "[5400]\tvalid_0's l1: 0.188112\n",
      "[5600]\tvalid_0's l1: 0.187045\n",
      "[5800]\tvalid_0's l1: 0.185829\n",
      "[6000]\tvalid_0's l1: 0.18466\n",
      "[6200]\tvalid_0's l1: 0.183463\n",
      "[6400]\tvalid_0's l1: 0.182443\n",
      "[6600]\tvalid_0's l1: 0.181383\n",
      "[6800]\tvalid_0's l1: 0.180357\n",
      "[7000]\tvalid_0's l1: 0.179475\n",
      "[7200]\tvalid_0's l1: 0.178768\n",
      "[7400]\tvalid_0's l1: 0.178008\n",
      "[7600]\tvalid_0's l1: 0.177293\n",
      "[7800]\tvalid_0's l1: 0.176788\n",
      "[8000]\tvalid_0's l1: 0.176604\n",
      "[8200]\tvalid_0's l1: 0.176318\n",
      "[8400]\tvalid_0's l1: 0.175742\n",
      "[8600]\tvalid_0's l1: 0.175286\n",
      "[8800]\tvalid_0's l1: 0.174852\n",
      "[9000]\tvalid_0's l1: 0.174485\n",
      "[9200]\tvalid_0's l1: 0.174059\n",
      "[9400]\tvalid_0's l1: 0.173672\n",
      "[9600]\tvalid_0's l1: 0.173421\n",
      "[9800]\tvalid_0's l1: 0.17316\n",
      "[10000]\tvalid_0's l1: 0.172841\n",
      "[10200]\tvalid_0's l1: 0.17249\n",
      "[10400]\tvalid_0's l1: 0.172239\n",
      "[10600]\tvalid_0's l1: 0.172083\n",
      "[10800]\tvalid_0's l1: 0.171672\n",
      "[11000]\tvalid_0's l1: 0.171263\n",
      "[11200]\tvalid_0's l1: 0.170935\n",
      "[11400]\tvalid_0's l1: 0.170615\n",
      "[11600]\tvalid_0's l1: 0.170155\n",
      "[11800]\tvalid_0's l1: 0.169723\n",
      "[12000]\tvalid_0's l1: 0.169316\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.169316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.648012\n",
      "[400]\tvalid_0's l1: 0.544679\n",
      "[600]\tvalid_0's l1: 0.469677\n",
      "[800]\tvalid_0's l1: 0.413261\n",
      "[1000]\tvalid_0's l1: 0.36942\n",
      "[1200]\tvalid_0's l1: 0.335057\n",
      "[1400]\tvalid_0's l1: 0.308407\n",
      "[1600]\tvalid_0's l1: 0.287026\n",
      "[1800]\tvalid_0's l1: 0.269679\n",
      "[2000]\tvalid_0's l1: 0.256134\n",
      "[2200]\tvalid_0's l1: 0.244856\n",
      "[2400]\tvalid_0's l1: 0.235281\n",
      "[2600]\tvalid_0's l1: 0.227239\n",
      "[2800]\tvalid_0's l1: 0.2204\n",
      "[3000]\tvalid_0's l1: 0.214932\n",
      "[3200]\tvalid_0's l1: 0.21042\n",
      "[3400]\tvalid_0's l1: 0.206518\n",
      "[3600]\tvalid_0's l1: 0.203244\n",
      "[3800]\tvalid_0's l1: 0.20057\n",
      "[4000]\tvalid_0's l1: 0.197554\n",
      "[4200]\tvalid_0's l1: 0.194402\n",
      "[4400]\tvalid_0's l1: 0.192159\n",
      "[4600]\tvalid_0's l1: 0.190085\n",
      "[4800]\tvalid_0's l1: 0.188226\n",
      "[5000]\tvalid_0's l1: 0.186321\n",
      "[5200]\tvalid_0's l1: 0.184659\n",
      "[5400]\tvalid_0's l1: 0.183415\n",
      "[5600]\tvalid_0's l1: 0.182489\n",
      "[5800]\tvalid_0's l1: 0.181362\n",
      "[6000]\tvalid_0's l1: 0.17945\n",
      "[6200]\tvalid_0's l1: 0.177482\n",
      "[6400]\tvalid_0's l1: 0.175522\n",
      "[6600]\tvalid_0's l1: 0.173971\n",
      "[6800]\tvalid_0's l1: 0.172631\n",
      "[7000]\tvalid_0's l1: 0.171535\n",
      "[7200]\tvalid_0's l1: 0.170767\n",
      "[7400]\tvalid_0's l1: 0.170211\n",
      "[7600]\tvalid_0's l1: 0.170066\n",
      "[7800]\tvalid_0's l1: 0.169851\n",
      "[8000]\tvalid_0's l1: 0.169442\n",
      "[8200]\tvalid_0's l1: 0.169086\n",
      "[8400]\tvalid_0's l1: 0.168869\n",
      "[8600]\tvalid_0's l1: 0.168641\n",
      "[8800]\tvalid_0's l1: 0.168302\n",
      "[9000]\tvalid_0's l1: 0.168007\n",
      "[9200]\tvalid_0's l1: 0.167731\n",
      "[9400]\tvalid_0's l1: 0.167515\n",
      "[9600]\tvalid_0's l1: 0.167361\n",
      "[9800]\tvalid_0's l1: 0.167213\n",
      "[10000]\tvalid_0's l1: 0.167054\n",
      "[10200]\tvalid_0's l1: 0.166893\n",
      "[10400]\tvalid_0's l1: 0.166749\n",
      "[10600]\tvalid_0's l1: 0.166629\n",
      "[10800]\tvalid_0's l1: 0.166104\n",
      "[11000]\tvalid_0's l1: 0.165504\n",
      "[11200]\tvalid_0's l1: 0.164885\n",
      "[11400]\tvalid_0's l1: 0.164202\n",
      "[11600]\tvalid_0's l1: 0.163382\n",
      "[11800]\tvalid_0's l1: 0.162557\n",
      "[12000]\tvalid_0's l1: 0.161959\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.161959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.712716\n",
      "[400]\tvalid_0's l1: 0.607226\n",
      "[600]\tvalid_0's l1: 0.530774\n",
      "[800]\tvalid_0's l1: 0.472251\n",
      "[1000]\tvalid_0's l1: 0.425682\n",
      "[1200]\tvalid_0's l1: 0.390274\n",
      "[1400]\tvalid_0's l1: 0.362332\n",
      "[1600]\tvalid_0's l1: 0.340518\n",
      "[1800]\tvalid_0's l1: 0.321997\n",
      "[2000]\tvalid_0's l1: 0.306671\n",
      "[2200]\tvalid_0's l1: 0.293699\n",
      "[2400]\tvalid_0's l1: 0.282756\n",
      "[2600]\tvalid_0's l1: 0.273584\n",
      "[2800]\tvalid_0's l1: 0.265954\n",
      "[3000]\tvalid_0's l1: 0.259879\n",
      "[3200]\tvalid_0's l1: 0.254421\n",
      "[3400]\tvalid_0's l1: 0.249973\n",
      "[3600]\tvalid_0's l1: 0.245513\n",
      "[3800]\tvalid_0's l1: 0.241097\n",
      "[4000]\tvalid_0's l1: 0.237331\n",
      "[4200]\tvalid_0's l1: 0.234615\n",
      "[4400]\tvalid_0's l1: 0.231754\n",
      "[4600]\tvalid_0's l1: 0.229085\n",
      "[4800]\tvalid_0's l1: 0.226868\n",
      "[5000]\tvalid_0's l1: 0.225154\n",
      "[5200]\tvalid_0's l1: 0.22357\n",
      "[5400]\tvalid_0's l1: 0.222055\n",
      "[5600]\tvalid_0's l1: 0.220208\n",
      "[5800]\tvalid_0's l1: 0.217375\n",
      "[6000]\tvalid_0's l1: 0.214984\n",
      "[6200]\tvalid_0's l1: 0.213422\n",
      "[6400]\tvalid_0's l1: 0.212264\n",
      "[6600]\tvalid_0's l1: 0.210847\n",
      "[6800]\tvalid_0's l1: 0.209536\n",
      "[7000]\tvalid_0's l1: 0.20853\n",
      "[7200]\tvalid_0's l1: 0.207858\n",
      "[7400]\tvalid_0's l1: 0.207389\n",
      "[7600]\tvalid_0's l1: 0.206718\n",
      "[7800]\tvalid_0's l1: 0.205802\n",
      "[8000]\tvalid_0's l1: 0.205003\n",
      "[8200]\tvalid_0's l1: 0.20407\n",
      "[8400]\tvalid_0's l1: 0.203184\n",
      "[8600]\tvalid_0's l1: 0.202503\n",
      "[8800]\tvalid_0's l1: 0.201753\n",
      "[9000]\tvalid_0's l1: 0.201244\n",
      "[9200]\tvalid_0's l1: 0.200838\n",
      "[9400]\tvalid_0's l1: 0.200495\n",
      "[9600]\tvalid_0's l1: 0.200098\n",
      "[9800]\tvalid_0's l1: 0.199191\n",
      "[10000]\tvalid_0's l1: 0.198219\n",
      "[10200]\tvalid_0's l1: 0.197162\n",
      "[10400]\tvalid_0's l1: 0.195923\n",
      "[10600]\tvalid_0's l1: 0.195036\n",
      "[10800]\tvalid_0's l1: 0.194136\n",
      "[11000]\tvalid_0's l1: 0.193335\n",
      "[11200]\tvalid_0's l1: 0.192751\n",
      "[11400]\tvalid_0's l1: 0.19227\n",
      "[11600]\tvalid_0's l1: 0.191835\n",
      "[11800]\tvalid_0's l1: 0.191432\n",
      "[12000]\tvalid_0's l1: 0.191107\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.191107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.709438\n",
      "[400]\tvalid_0's l1: 0.597026\n",
      "[600]\tvalid_0's l1: 0.515192\n",
      "[800]\tvalid_0's l1: 0.453945\n",
      "[1000]\tvalid_0's l1: 0.405469\n",
      "[1200]\tvalid_0's l1: 0.368286\n",
      "[1400]\tvalid_0's l1: 0.339386\n",
      "[1600]\tvalid_0's l1: 0.317356\n",
      "[1800]\tvalid_0's l1: 0.299241\n",
      "[2000]\tvalid_0's l1: 0.284846\n",
      "[2200]\tvalid_0's l1: 0.272585\n",
      "[2400]\tvalid_0's l1: 0.262263\n",
      "[2600]\tvalid_0's l1: 0.253795\n",
      "[2800]\tvalid_0's l1: 0.246671\n",
      "[3000]\tvalid_0's l1: 0.240753\n",
      "[3200]\tvalid_0's l1: 0.235564\n",
      "[3400]\tvalid_0's l1: 0.231049\n",
      "[3600]\tvalid_0's l1: 0.227361\n",
      "[3800]\tvalid_0's l1: 0.224279\n",
      "[4000]\tvalid_0's l1: 0.221627\n",
      "[4200]\tvalid_0's l1: 0.218024\n",
      "[4400]\tvalid_0's l1: 0.214724\n",
      "[4600]\tvalid_0's l1: 0.211932\n",
      "[4800]\tvalid_0's l1: 0.209414\n",
      "[5000]\tvalid_0's l1: 0.207498\n",
      "[5200]\tvalid_0's l1: 0.205717\n",
      "[5400]\tvalid_0's l1: 0.203958\n",
      "[5600]\tvalid_0's l1: 0.202707\n",
      "[5800]\tvalid_0's l1: 0.201553\n",
      "[6000]\tvalid_0's l1: 0.200567\n",
      "[6200]\tvalid_0's l1: 0.199652\n",
      "[6400]\tvalid_0's l1: 0.198814\n",
      "[6600]\tvalid_0's l1: 0.198143\n",
      "[6800]\tvalid_0's l1: 0.197503\n",
      "[7000]\tvalid_0's l1: 0.196968\n",
      "[7200]\tvalid_0's l1: 0.196394\n",
      "[7400]\tvalid_0's l1: 0.195309\n",
      "[7600]\tvalid_0's l1: 0.194053\n",
      "[7800]\tvalid_0's l1: 0.192356\n",
      "[8000]\tvalid_0's l1: 0.190913\n",
      "[8200]\tvalid_0's l1: 0.18944\n",
      "[8400]\tvalid_0's l1: 0.188093\n",
      "[8600]\tvalid_0's l1: 0.187144\n",
      "[8800]\tvalid_0's l1: 0.186359\n",
      "[9000]\tvalid_0's l1: 0.185719\n",
      "[9200]\tvalid_0's l1: 0.185207\n",
      "[9400]\tvalid_0's l1: 0.184777\n",
      "[9600]\tvalid_0's l1: 0.184312\n",
      "[9800]\tvalid_0's l1: 0.183811\n",
      "[10000]\tvalid_0's l1: 0.183341\n",
      "[10200]\tvalid_0's l1: 0.182936\n",
      "[10400]\tvalid_0's l1: 0.182558\n",
      "[10600]\tvalid_0's l1: 0.182095\n",
      "[10800]\tvalid_0's l1: 0.181926\n",
      "[11000]\tvalid_0's l1: 0.181823\n",
      "[11200]\tvalid_0's l1: 0.181733\n",
      "[11400]\tvalid_0's l1: 0.181591\n",
      "[11600]\tvalid_0's l1: 0.181418\n",
      "[11800]\tvalid_0's l1: 0.181214\n",
      "[12000]\tvalid_0's l1: 0.18102\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.18102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.4942 (weight: 0.000)\n",
      "Random Forest MAPE: 6.5267 (weight: 0.000)\n",
      "Extra Trees MAPE: 7.5550 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 2.5302 (weight: 0.000)\n",
      "Ridge MAPE: 0.0402 (weight: 0.467)\n",
      "Elastic Net MAPE: 0.0269 (weight: 0.533)\n",
      "Huber MAPE: 2.6134 (weight: 0.000)\n",
      "Ensemble MAPE: 0.0227\n",
      "\n",
      "Training for BlendProperty2...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.651718\n",
      "[400]\tvalid_0's l1: 0.548617\n",
      "[600]\tvalid_0's l1: 0.471194\n",
      "[800]\tvalid_0's l1: 0.411585\n",
      "[1000]\tvalid_0's l1: 0.365949\n",
      "[1200]\tvalid_0's l1: 0.331945\n",
      "[1400]\tvalid_0's l1: 0.305387\n",
      "[1600]\tvalid_0's l1: 0.284471\n",
      "[1800]\tvalid_0's l1: 0.266589\n",
      "[2000]\tvalid_0's l1: 0.25287\n",
      "[2200]\tvalid_0's l1: 0.242125\n",
      "[2400]\tvalid_0's l1: 0.233402\n",
      "[2600]\tvalid_0's l1: 0.226293\n",
      "[2800]\tvalid_0's l1: 0.220329\n",
      "[3000]\tvalid_0's l1: 0.214876\n",
      "[3200]\tvalid_0's l1: 0.210475\n",
      "[3400]\tvalid_0's l1: 0.206732\n",
      "[3600]\tvalid_0's l1: 0.203553\n",
      "[3800]\tvalid_0's l1: 0.200407\n",
      "[4000]\tvalid_0's l1: 0.197539\n",
      "[4200]\tvalid_0's l1: 0.195079\n",
      "[4400]\tvalid_0's l1: 0.193\n",
      "[4600]\tvalid_0's l1: 0.190909\n",
      "[4800]\tvalid_0's l1: 0.189234\n",
      "[5000]\tvalid_0's l1: 0.187732\n",
      "[5200]\tvalid_0's l1: 0.186685\n",
      "[5400]\tvalid_0's l1: 0.185701\n",
      "[5600]\tvalid_0's l1: 0.184314\n",
      "[5800]\tvalid_0's l1: 0.182923\n",
      "[6000]\tvalid_0's l1: 0.181195\n",
      "[6200]\tvalid_0's l1: 0.179591\n",
      "[6400]\tvalid_0's l1: 0.178089\n",
      "[6600]\tvalid_0's l1: 0.17695\n",
      "[6800]\tvalid_0's l1: 0.175988\n",
      "[7000]\tvalid_0's l1: 0.175065\n",
      "[7200]\tvalid_0's l1: 0.174329\n",
      "[7400]\tvalid_0's l1: 0.17382\n",
      "[7600]\tvalid_0's l1: 0.173337\n",
      "[7800]\tvalid_0's l1: 0.172916\n",
      "[8000]\tvalid_0's l1: 0.172582\n",
      "[8200]\tvalid_0's l1: 0.172262\n",
      "[8400]\tvalid_0's l1: 0.171805\n",
      "[8600]\tvalid_0's l1: 0.171312\n",
      "[8800]\tvalid_0's l1: 0.170525\n",
      "[9000]\tvalid_0's l1: 0.169854\n",
      "[9200]\tvalid_0's l1: 0.169264\n",
      "[9400]\tvalid_0's l1: 0.168974\n",
      "[9600]\tvalid_0's l1: 0.168495\n",
      "[9800]\tvalid_0's l1: 0.167989\n",
      "[10000]\tvalid_0's l1: 0.167537\n",
      "[10200]\tvalid_0's l1: 0.167234\n",
      "[10400]\tvalid_0's l1: 0.166929\n",
      "[10600]\tvalid_0's l1: 0.16674\n",
      "[10800]\tvalid_0's l1: 0.166601\n",
      "[11000]\tvalid_0's l1: 0.166376\n",
      "[11200]\tvalid_0's l1: 0.166079\n",
      "[11400]\tvalid_0's l1: 0.165852\n",
      "[11600]\tvalid_0's l1: 0.165564\n",
      "[11800]\tvalid_0's l1: 0.16532\n",
      "[12000]\tvalid_0's l1: 0.16506\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.16506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.687827\n",
      "[400]\tvalid_0's l1: 0.580542\n",
      "[600]\tvalid_0's l1: 0.499776\n",
      "[800]\tvalid_0's l1: 0.437794\n",
      "[1000]\tvalid_0's l1: 0.389428\n",
      "[1200]\tvalid_0's l1: 0.351115\n",
      "[1400]\tvalid_0's l1: 0.32161\n",
      "[1600]\tvalid_0's l1: 0.298017\n",
      "[1800]\tvalid_0's l1: 0.278932\n",
      "[2000]\tvalid_0's l1: 0.263555\n",
      "[2200]\tvalid_0's l1: 0.250433\n",
      "[2400]\tvalid_0's l1: 0.239452\n",
      "[2600]\tvalid_0's l1: 0.230264\n",
      "[2800]\tvalid_0's l1: 0.222607\n",
      "[3000]\tvalid_0's l1: 0.216403\n",
      "[3200]\tvalid_0's l1: 0.211032\n",
      "[3400]\tvalid_0's l1: 0.20651\n",
      "[3600]\tvalid_0's l1: 0.202597\n",
      "[3800]\tvalid_0's l1: 0.19903\n",
      "[4000]\tvalid_0's l1: 0.195823\n",
      "[4200]\tvalid_0's l1: 0.193147\n",
      "[4400]\tvalid_0's l1: 0.190635\n",
      "[4600]\tvalid_0's l1: 0.188354\n",
      "[4800]\tvalid_0's l1: 0.186328\n",
      "[5000]\tvalid_0's l1: 0.184619\n",
      "[5200]\tvalid_0's l1: 0.183111\n",
      "[5400]\tvalid_0's l1: 0.180964\n",
      "[5600]\tvalid_0's l1: 0.178797\n",
      "[5800]\tvalid_0's l1: 0.176965\n",
      "[6000]\tvalid_0's l1: 0.175318\n",
      "[6200]\tvalid_0's l1: 0.17395\n",
      "[6400]\tvalid_0's l1: 0.172735\n",
      "[6600]\tvalid_0's l1: 0.171595\n",
      "[6800]\tvalid_0's l1: 0.170776\n",
      "[7000]\tvalid_0's l1: 0.170261\n",
      "[7200]\tvalid_0's l1: 0.169438\n",
      "[7400]\tvalid_0's l1: 0.168765\n",
      "[7600]\tvalid_0's l1: 0.168322\n",
      "[7800]\tvalid_0's l1: 0.167758\n",
      "[8000]\tvalid_0's l1: 0.167162\n",
      "[8200]\tvalid_0's l1: 0.16619\n",
      "[8400]\tvalid_0's l1: 0.165416\n",
      "[8600]\tvalid_0's l1: 0.164777\n",
      "[8800]\tvalid_0's l1: 0.164404\n",
      "[9000]\tvalid_0's l1: 0.163999\n",
      "[9200]\tvalid_0's l1: 0.163696\n",
      "[9400]\tvalid_0's l1: 0.163507\n",
      "[9600]\tvalid_0's l1: 0.163018\n",
      "[9800]\tvalid_0's l1: 0.162152\n",
      "[10000]\tvalid_0's l1: 0.161307\n",
      "[10200]\tvalid_0's l1: 0.160505\n",
      "[10400]\tvalid_0's l1: 0.159524\n",
      "[10600]\tvalid_0's l1: 0.158847\n",
      "[10800]\tvalid_0's l1: 0.158209\n",
      "[11000]\tvalid_0's l1: 0.157782\n",
      "[11200]\tvalid_0's l1: 0.157411\n",
      "[11400]\tvalid_0's l1: 0.157094\n",
      "[11600]\tvalid_0's l1: 0.156811\n",
      "[11800]\tvalid_0's l1: 0.156525\n",
      "[12000]\tvalid_0's l1: 0.156284\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.156284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.633863\n",
      "[400]\tvalid_0's l1: 0.536823\n",
      "[600]\tvalid_0's l1: 0.466136\n",
      "[800]\tvalid_0's l1: 0.411686\n",
      "[1000]\tvalid_0's l1: 0.369295\n",
      "[1200]\tvalid_0's l1: 0.335891\n",
      "[1400]\tvalid_0's l1: 0.308269\n",
      "[1600]\tvalid_0's l1: 0.286284\n",
      "[1800]\tvalid_0's l1: 0.268654\n",
      "[2000]\tvalid_0's l1: 0.254767\n",
      "[2200]\tvalid_0's l1: 0.243427\n",
      "[2400]\tvalid_0's l1: 0.234066\n",
      "[2600]\tvalid_0's l1: 0.226152\n",
      "[2800]\tvalid_0's l1: 0.219331\n",
      "[3000]\tvalid_0's l1: 0.21352\n",
      "[3200]\tvalid_0's l1: 0.208594\n",
      "[3400]\tvalid_0's l1: 0.204453\n",
      "[3600]\tvalid_0's l1: 0.20079\n",
      "[3800]\tvalid_0's l1: 0.197483\n",
      "[4000]\tvalid_0's l1: 0.194709\n",
      "[4200]\tvalid_0's l1: 0.192466\n",
      "[4400]\tvalid_0's l1: 0.190216\n",
      "[4600]\tvalid_0's l1: 0.18802\n",
      "[4800]\tvalid_0's l1: 0.186297\n",
      "[5000]\tvalid_0's l1: 0.184663\n",
      "[5200]\tvalid_0's l1: 0.182773\n",
      "[5400]\tvalid_0's l1: 0.180799\n",
      "[5600]\tvalid_0's l1: 0.179235\n",
      "[5800]\tvalid_0's l1: 0.177831\n",
      "[6000]\tvalid_0's l1: 0.17668\n",
      "[6200]\tvalid_0's l1: 0.175566\n",
      "[6400]\tvalid_0's l1: 0.174601\n",
      "[6600]\tvalid_0's l1: 0.173984\n",
      "[6800]\tvalid_0's l1: 0.173053\n",
      "[7000]\tvalid_0's l1: 0.172065\n",
      "[7200]\tvalid_0's l1: 0.171394\n",
      "[7400]\tvalid_0's l1: 0.170741\n",
      "[7600]\tvalid_0's l1: 0.170094\n",
      "[7800]\tvalid_0's l1: 0.169538\n",
      "[8000]\tvalid_0's l1: 0.16898\n",
      "[8200]\tvalid_0's l1: 0.168589\n",
      "[8400]\tvalid_0's l1: 0.168236\n",
      "[8600]\tvalid_0's l1: 0.167943\n",
      "[8800]\tvalid_0's l1: 0.167754\n",
      "[9000]\tvalid_0's l1: 0.167575\n",
      "[9200]\tvalid_0's l1: 0.167139\n",
      "[9400]\tvalid_0's l1: 0.166649\n",
      "[9600]\tvalid_0's l1: 0.166066\n",
      "[9800]\tvalid_0's l1: 0.165631\n",
      "[10000]\tvalid_0's l1: 0.165005\n",
      "[10200]\tvalid_0's l1: 0.164396\n",
      "[10400]\tvalid_0's l1: 0.163811\n",
      "[10600]\tvalid_0's l1: 0.163328\n",
      "[10800]\tvalid_0's l1: 0.162908\n",
      "[11000]\tvalid_0's l1: 0.162507\n",
      "[11200]\tvalid_0's l1: 0.162196\n",
      "[11400]\tvalid_0's l1: 0.161969\n",
      "[11600]\tvalid_0's l1: 0.161806\n",
      "[11800]\tvalid_0's l1: 0.161628\n",
      "[12000]\tvalid_0's l1: 0.161489\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11997]\tvalid_0's l1: 0.161489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.720632\n",
      "[400]\tvalid_0's l1: 0.615343\n",
      "[600]\tvalid_0's l1: 0.53594\n",
      "[800]\tvalid_0's l1: 0.474622\n",
      "[1000]\tvalid_0's l1: 0.428984\n",
      "[1200]\tvalid_0's l1: 0.392854\n",
      "[1400]\tvalid_0's l1: 0.363027\n",
      "[1600]\tvalid_0's l1: 0.338941\n",
      "[1800]\tvalid_0's l1: 0.319457\n",
      "[2000]\tvalid_0's l1: 0.303113\n",
      "[2200]\tvalid_0's l1: 0.289517\n",
      "[2400]\tvalid_0's l1: 0.278062\n",
      "[2600]\tvalid_0's l1: 0.268061\n",
      "[2800]\tvalid_0's l1: 0.260205\n",
      "[3000]\tvalid_0's l1: 0.253354\n",
      "[3200]\tvalid_0's l1: 0.246994\n",
      "[3400]\tvalid_0's l1: 0.241879\n",
      "[3600]\tvalid_0's l1: 0.237662\n",
      "[3800]\tvalid_0's l1: 0.233553\n",
      "[4000]\tvalid_0's l1: 0.229606\n",
      "[4200]\tvalid_0's l1: 0.226114\n",
      "[4400]\tvalid_0's l1: 0.223479\n",
      "[4600]\tvalid_0's l1: 0.220827\n",
      "[4800]\tvalid_0's l1: 0.218354\n",
      "[5000]\tvalid_0's l1: 0.215909\n",
      "[5200]\tvalid_0's l1: 0.213978\n",
      "[5400]\tvalid_0's l1: 0.212245\n",
      "[5600]\tvalid_0's l1: 0.2104\n",
      "[5800]\tvalid_0's l1: 0.20822\n",
      "[6000]\tvalid_0's l1: 0.206082\n",
      "[6200]\tvalid_0's l1: 0.204069\n",
      "[6400]\tvalid_0's l1: 0.202448\n",
      "[6600]\tvalid_0's l1: 0.200913\n",
      "[6800]\tvalid_0's l1: 0.199779\n",
      "[7000]\tvalid_0's l1: 0.198743\n",
      "[7200]\tvalid_0's l1: 0.198097\n",
      "[7400]\tvalid_0's l1: 0.197502\n",
      "[7600]\tvalid_0's l1: 0.196568\n",
      "[7800]\tvalid_0's l1: 0.195541\n",
      "[8000]\tvalid_0's l1: 0.194379\n",
      "[8200]\tvalid_0's l1: 0.193464\n",
      "[8400]\tvalid_0's l1: 0.192625\n",
      "[8600]\tvalid_0's l1: 0.191516\n",
      "[8800]\tvalid_0's l1: 0.190671\n",
      "[9000]\tvalid_0's l1: 0.190038\n",
      "[9200]\tvalid_0's l1: 0.189545\n",
      "[9400]\tvalid_0's l1: 0.189154\n",
      "[9600]\tvalid_0's l1: 0.189046\n",
      "[9800]\tvalid_0's l1: 0.188872\n",
      "[10000]\tvalid_0's l1: 0.188594\n",
      "[10200]\tvalid_0's l1: 0.188043\n",
      "[10400]\tvalid_0's l1: 0.187564\n",
      "[10600]\tvalid_0's l1: 0.186979\n",
      "[10800]\tvalid_0's l1: 0.186286\n",
      "[11000]\tvalid_0's l1: 0.185418\n",
      "[11200]\tvalid_0's l1: 0.184698\n",
      "[11400]\tvalid_0's l1: 0.184213\n",
      "[11600]\tvalid_0's l1: 0.183762\n",
      "[11800]\tvalid_0's l1: 0.183444\n",
      "[12000]\tvalid_0's l1: 0.183081\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11999]\tvalid_0's l1: 0.183081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.708894\n",
      "[400]\tvalid_0's l1: 0.600324\n",
      "[600]\tvalid_0's l1: 0.520889\n",
      "[800]\tvalid_0's l1: 0.457792\n",
      "[1000]\tvalid_0's l1: 0.408543\n",
      "[1200]\tvalid_0's l1: 0.370549\n",
      "[1400]\tvalid_0's l1: 0.340378\n",
      "[1600]\tvalid_0's l1: 0.316177\n",
      "[1800]\tvalid_0's l1: 0.297097\n",
      "[2000]\tvalid_0's l1: 0.282112\n",
      "[2200]\tvalid_0's l1: 0.269501\n",
      "[2400]\tvalid_0's l1: 0.258974\n",
      "[2600]\tvalid_0's l1: 0.250249\n",
      "[2800]\tvalid_0's l1: 0.242705\n",
      "[3000]\tvalid_0's l1: 0.236267\n",
      "[3200]\tvalid_0's l1: 0.23095\n",
      "[3400]\tvalid_0's l1: 0.226318\n",
      "[3600]\tvalid_0's l1: 0.221717\n",
      "[3800]\tvalid_0's l1: 0.218076\n",
      "[4000]\tvalid_0's l1: 0.214664\n",
      "[4200]\tvalid_0's l1: 0.211729\n",
      "[4400]\tvalid_0's l1: 0.208828\n",
      "[4600]\tvalid_0's l1: 0.206388\n",
      "[4800]\tvalid_0's l1: 0.203686\n",
      "[5000]\tvalid_0's l1: 0.20128\n",
      "[5200]\tvalid_0's l1: 0.199173\n",
      "[5400]\tvalid_0's l1: 0.197097\n",
      "[5600]\tvalid_0's l1: 0.195443\n",
      "[5800]\tvalid_0's l1: 0.194632\n",
      "[6000]\tvalid_0's l1: 0.193262\n",
      "[6200]\tvalid_0's l1: 0.191561\n",
      "[6400]\tvalid_0's l1: 0.189838\n",
      "[6600]\tvalid_0's l1: 0.187973\n",
      "[6800]\tvalid_0's l1: 0.186662\n",
      "[7000]\tvalid_0's l1: 0.185274\n",
      "[7200]\tvalid_0's l1: 0.184225\n",
      "[7400]\tvalid_0's l1: 0.183432\n",
      "[7600]\tvalid_0's l1: 0.182821\n",
      "[7800]\tvalid_0's l1: 0.182105\n",
      "[8000]\tvalid_0's l1: 0.181473\n",
      "[8200]\tvalid_0's l1: 0.181084\n",
      "[8400]\tvalid_0's l1: 0.180742\n",
      "[8600]\tvalid_0's l1: 0.180262\n",
      "[8800]\tvalid_0's l1: 0.179366\n",
      "[9000]\tvalid_0's l1: 0.178537\n",
      "[9200]\tvalid_0's l1: 0.17756\n",
      "[9400]\tvalid_0's l1: 0.176964\n",
      "[9600]\tvalid_0's l1: 0.176355\n",
      "[9800]\tvalid_0's l1: 0.175678\n",
      "[10000]\tvalid_0's l1: 0.175122\n",
      "[10200]\tvalid_0's l1: 0.174664\n",
      "[10400]\tvalid_0's l1: 0.174244\n",
      "[10600]\tvalid_0's l1: 0.174\n",
      "[10800]\tvalid_0's l1: 0.17372\n",
      "[11000]\tvalid_0's l1: 0.173536\n",
      "[11200]\tvalid_0's l1: 0.173395\n",
      "[11400]\tvalid_0's l1: 0.1733\n",
      "[11600]\tvalid_0's l1: 0.173148\n",
      "[11800]\tvalid_0's l1: 0.173002\n",
      "[12000]\tvalid_0's l1: 0.172903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.172903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.0923 (weight: 0.000)\n",
      "Random Forest MAPE: 1.7452 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.5231 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 1.2285 (weight: 0.000)\n",
      "Ridge MAPE: 0.2432 (weight: 0.442)\n",
      "Elastic Net MAPE: 0.2200 (weight: 0.558)\n",
      "Huber MAPE: 1.9676 (weight: 0.000)\n",
      "Ensemble MAPE: 0.2198\n",
      "\n",
      "Training for BlendProperty3...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.623789\n",
      "[400]\tvalid_0's l1: 0.515291\n",
      "[600]\tvalid_0's l1: 0.4388\n",
      "[800]\tvalid_0's l1: 0.382125\n",
      "[1000]\tvalid_0's l1: 0.339082\n",
      "[1200]\tvalid_0's l1: 0.306785\n",
      "[1400]\tvalid_0's l1: 0.281187\n",
      "[1600]\tvalid_0's l1: 0.262326\n",
      "[1800]\tvalid_0's l1: 0.248328\n",
      "[2000]\tvalid_0's l1: 0.236711\n",
      "[2200]\tvalid_0's l1: 0.227458\n",
      "[2400]\tvalid_0's l1: 0.219789\n",
      "[2600]\tvalid_0's l1: 0.213948\n",
      "[2800]\tvalid_0's l1: 0.208953\n",
      "[3000]\tvalid_0's l1: 0.205105\n",
      "[3200]\tvalid_0's l1: 0.201808\n",
      "[3400]\tvalid_0's l1: 0.198918\n",
      "[3600]\tvalid_0's l1: 0.196534\n",
      "[3800]\tvalid_0's l1: 0.194416\n",
      "[4000]\tvalid_0's l1: 0.192732\n",
      "[4200]\tvalid_0's l1: 0.191316\n",
      "[4400]\tvalid_0's l1: 0.189085\n",
      "[4600]\tvalid_0's l1: 0.187195\n",
      "[4800]\tvalid_0's l1: 0.185632\n",
      "[5000]\tvalid_0's l1: 0.18438\n",
      "[5200]\tvalid_0's l1: 0.183385\n",
      "[5400]\tvalid_0's l1: 0.182644\n",
      "[5600]\tvalid_0's l1: 0.181976\n",
      "[5800]\tvalid_0's l1: 0.181424\n",
      "[6000]\tvalid_0's l1: 0.181014\n",
      "[6200]\tvalid_0's l1: 0.18025\n",
      "[6400]\tvalid_0's l1: 0.179269\n",
      "[6600]\tvalid_0's l1: 0.178369\n",
      "[6800]\tvalid_0's l1: 0.17752\n",
      "[7000]\tvalid_0's l1: 0.176658\n",
      "[7200]\tvalid_0's l1: 0.175999\n",
      "[7400]\tvalid_0's l1: 0.175555\n",
      "[7600]\tvalid_0's l1: 0.175164\n",
      "[7800]\tvalid_0's l1: 0.174771\n",
      "[8000]\tvalid_0's l1: 0.17454\n",
      "[8200]\tvalid_0's l1: 0.174354\n",
      "[8400]\tvalid_0's l1: 0.173943\n",
      "[8600]\tvalid_0's l1: 0.17351\n",
      "[8800]\tvalid_0's l1: 0.173053\n",
      "[9000]\tvalid_0's l1: 0.172474\n",
      "[9200]\tvalid_0's l1: 0.171988\n",
      "[9400]\tvalid_0's l1: 0.171639\n",
      "[9600]\tvalid_0's l1: 0.171284\n",
      "[9800]\tvalid_0's l1: 0.170911\n",
      "[10000]\tvalid_0's l1: 0.170527\n",
      "[10200]\tvalid_0's l1: 0.170233\n",
      "[10400]\tvalid_0's l1: 0.169994\n",
      "[10600]\tvalid_0's l1: 0.169808\n",
      "[10800]\tvalid_0's l1: 0.169656\n",
      "[11000]\tvalid_0's l1: 0.169598\n",
      "Early stopping, best iteration is:\n",
      "[10993]\tvalid_0's l1: 0.169597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.680391\n",
      "[400]\tvalid_0's l1: 0.552707\n",
      "[600]\tvalid_0's l1: 0.459154\n",
      "[800]\tvalid_0's l1: 0.390298\n",
      "[1000]\tvalid_0's l1: 0.341386\n",
      "[1200]\tvalid_0's l1: 0.305272\n",
      "[1400]\tvalid_0's l1: 0.278047\n",
      "[1600]\tvalid_0's l1: 0.256346\n",
      "[1800]\tvalid_0's l1: 0.240645\n",
      "[2000]\tvalid_0's l1: 0.228807\n",
      "[2200]\tvalid_0's l1: 0.219422\n",
      "[2400]\tvalid_0's l1: 0.212496\n",
      "[2600]\tvalid_0's l1: 0.20699\n",
      "[2800]\tvalid_0's l1: 0.202656\n",
      "[3000]\tvalid_0's l1: 0.19911\n",
      "[3200]\tvalid_0's l1: 0.196161\n",
      "[3400]\tvalid_0's l1: 0.193619\n",
      "[3600]\tvalid_0's l1: 0.191349\n",
      "[3800]\tvalid_0's l1: 0.189144\n",
      "[4000]\tvalid_0's l1: 0.186913\n",
      "[4200]\tvalid_0's l1: 0.185007\n",
      "[4400]\tvalid_0's l1: 0.183751\n",
      "[4600]\tvalid_0's l1: 0.182505\n",
      "[4800]\tvalid_0's l1: 0.181488\n",
      "[5000]\tvalid_0's l1: 0.180764\n",
      "[5200]\tvalid_0's l1: 0.180136\n",
      "[5400]\tvalid_0's l1: 0.179546\n",
      "[5600]\tvalid_0's l1: 0.179115\n",
      "[5800]\tvalid_0's l1: 0.178745\n",
      "[6000]\tvalid_0's l1: 0.178438\n",
      "[6200]\tvalid_0's l1: 0.177862\n",
      "[6400]\tvalid_0's l1: 0.17653\n",
      "[6600]\tvalid_0's l1: 0.175213\n",
      "[6800]\tvalid_0's l1: 0.174422\n",
      "[7000]\tvalid_0's l1: 0.173515\n",
      "[7200]\tvalid_0's l1: 0.172091\n",
      "[7400]\tvalid_0's l1: 0.170533\n",
      "[7600]\tvalid_0's l1: 0.169795\n",
      "[7800]\tvalid_0's l1: 0.169144\n",
      "[8000]\tvalid_0's l1: 0.168532\n",
      "[8200]\tvalid_0's l1: 0.168028\n",
      "[8400]\tvalid_0's l1: 0.167568\n",
      "[8600]\tvalid_0's l1: 0.167129\n",
      "[8800]\tvalid_0's l1: 0.166839\n",
      "[9000]\tvalid_0's l1: 0.166801\n",
      "Early stopping, best iteration is:\n",
      "[8925]\tvalid_0's l1: 0.166799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.620619\n",
      "[400]\tvalid_0's l1: 0.513633\n",
      "[600]\tvalid_0's l1: 0.438717\n",
      "[800]\tvalid_0's l1: 0.384622\n",
      "[1000]\tvalid_0's l1: 0.34419\n",
      "[1200]\tvalid_0's l1: 0.315203\n",
      "[1400]\tvalid_0's l1: 0.292526\n",
      "[1600]\tvalid_0's l1: 0.27459\n",
      "[1800]\tvalid_0's l1: 0.259848\n",
      "[2000]\tvalid_0's l1: 0.248292\n",
      "[2200]\tvalid_0's l1: 0.238664\n",
      "[2400]\tvalid_0's l1: 0.231059\n",
      "[2600]\tvalid_0's l1: 0.224286\n",
      "[2800]\tvalid_0's l1: 0.218061\n",
      "[3000]\tvalid_0's l1: 0.212379\n",
      "[3200]\tvalid_0's l1: 0.207885\n",
      "[3400]\tvalid_0's l1: 0.204127\n",
      "[3600]\tvalid_0's l1: 0.201326\n",
      "[3800]\tvalid_0's l1: 0.198654\n",
      "[4000]\tvalid_0's l1: 0.196516\n",
      "[4200]\tvalid_0's l1: 0.19486\n",
      "[4400]\tvalid_0's l1: 0.193109\n",
      "[4600]\tvalid_0's l1: 0.191872\n",
      "[4800]\tvalid_0's l1: 0.190863\n",
      "[5000]\tvalid_0's l1: 0.189952\n",
      "[5200]\tvalid_0's l1: 0.189247\n",
      "[5400]\tvalid_0's l1: 0.188611\n",
      "[5600]\tvalid_0's l1: 0.188101\n",
      "[5800]\tvalid_0's l1: 0.18758\n",
      "[6000]\tvalid_0's l1: 0.186791\n",
      "[6200]\tvalid_0's l1: 0.185178\n",
      "[6400]\tvalid_0's l1: 0.183276\n",
      "[6600]\tvalid_0's l1: 0.181578\n",
      "[6800]\tvalid_0's l1: 0.180034\n",
      "[7000]\tvalid_0's l1: 0.178674\n",
      "[7200]\tvalid_0's l1: 0.177521\n",
      "[7400]\tvalid_0's l1: 0.176509\n",
      "[7600]\tvalid_0's l1: 0.175684\n",
      "[7800]\tvalid_0's l1: 0.17505\n",
      "[8000]\tvalid_0's l1: 0.174632\n",
      "[8200]\tvalid_0's l1: 0.174325\n",
      "[8400]\tvalid_0's l1: 0.173876\n",
      "[8600]\tvalid_0's l1: 0.173642\n",
      "[8800]\tvalid_0's l1: 0.173548\n",
      "[9000]\tvalid_0's l1: 0.173449\n",
      "[9200]\tvalid_0's l1: 0.173144\n",
      "[9400]\tvalid_0's l1: 0.172869\n",
      "[9600]\tvalid_0's l1: 0.17245\n",
      "[9800]\tvalid_0's l1: 0.172132\n",
      "[10000]\tvalid_0's l1: 0.171957\n",
      "[10200]\tvalid_0's l1: 0.171746\n",
      "[10400]\tvalid_0's l1: 0.171542\n",
      "[10600]\tvalid_0's l1: 0.171356\n",
      "[10800]\tvalid_0's l1: 0.171156\n",
      "[11000]\tvalid_0's l1: 0.171028\n",
      "[11200]\tvalid_0's l1: 0.170882\n",
      "[11400]\tvalid_0's l1: 0.170774\n",
      "[11600]\tvalid_0's l1: 0.170731\n",
      "[11800]\tvalid_0's l1: 0.170699\n",
      "[12000]\tvalid_0's l1: 0.170646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11999]\tvalid_0's l1: 0.170645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.640596\n",
      "[400]\tvalid_0's l1: 0.53408\n",
      "[600]\tvalid_0's l1: 0.45762\n",
      "[800]\tvalid_0's l1: 0.400289\n",
      "[1000]\tvalid_0's l1: 0.357043\n",
      "[1200]\tvalid_0's l1: 0.324695\n",
      "[1400]\tvalid_0's l1: 0.299983\n",
      "[1600]\tvalid_0's l1: 0.280762\n",
      "[1800]\tvalid_0's l1: 0.265548\n",
      "[2000]\tvalid_0's l1: 0.253733\n",
      "[2200]\tvalid_0's l1: 0.244631\n",
      "[2400]\tvalid_0's l1: 0.237308\n",
      "[2600]\tvalid_0's l1: 0.231304\n",
      "[2800]\tvalid_0's l1: 0.226444\n",
      "[3000]\tvalid_0's l1: 0.222497\n",
      "[3200]\tvalid_0's l1: 0.219023\n",
      "[3400]\tvalid_0's l1: 0.215909\n",
      "[3600]\tvalid_0's l1: 0.213091\n",
      "[3800]\tvalid_0's l1: 0.210511\n",
      "[4000]\tvalid_0's l1: 0.207419\n",
      "[4200]\tvalid_0's l1: 0.204313\n",
      "[4400]\tvalid_0's l1: 0.201648\n",
      "[4600]\tvalid_0's l1: 0.200197\n",
      "[4800]\tvalid_0's l1: 0.198613\n",
      "[5000]\tvalid_0's l1: 0.196905\n",
      "[5200]\tvalid_0's l1: 0.195697\n",
      "[5400]\tvalid_0's l1: 0.194663\n",
      "[5600]\tvalid_0's l1: 0.193694\n",
      "[5800]\tvalid_0's l1: 0.192931\n",
      "[6000]\tvalid_0's l1: 0.192122\n",
      "[6200]\tvalid_0's l1: 0.19148\n",
      "[6400]\tvalid_0's l1: 0.191066\n",
      "[6600]\tvalid_0's l1: 0.190372\n",
      "[6800]\tvalid_0's l1: 0.189513\n",
      "[7000]\tvalid_0's l1: 0.188267\n",
      "[7200]\tvalid_0's l1: 0.186642\n",
      "[7400]\tvalid_0's l1: 0.184872\n",
      "[7600]\tvalid_0's l1: 0.183457\n",
      "[7800]\tvalid_0's l1: 0.182458\n",
      "[8000]\tvalid_0's l1: 0.181615\n",
      "[8200]\tvalid_0's l1: 0.180987\n",
      "[8400]\tvalid_0's l1: 0.180359\n",
      "[8600]\tvalid_0's l1: 0.179859\n",
      "[8800]\tvalid_0's l1: 0.179499\n",
      "[9000]\tvalid_0's l1: 0.179421\n",
      "[9200]\tvalid_0's l1: 0.179364\n",
      "[9400]\tvalid_0's l1: 0.179237\n",
      "[9600]\tvalid_0's l1: 0.179107\n",
      "[9800]\tvalid_0's l1: 0.178966\n",
      "[10000]\tvalid_0's l1: 0.178805\n",
      "[10200]\tvalid_0's l1: 0.178596\n",
      "[10400]\tvalid_0's l1: 0.178399\n",
      "[10600]\tvalid_0's l1: 0.17819\n",
      "[10800]\tvalid_0's l1: 0.17799\n",
      "[11000]\tvalid_0's l1: 0.177833\n",
      "[11200]\tvalid_0's l1: 0.177694\n",
      "[11400]\tvalid_0's l1: 0.177559\n",
      "[11600]\tvalid_0's l1: 0.177454\n",
      "[11800]\tvalid_0's l1: 0.177355\n",
      "[12000]\tvalid_0's l1: 0.177274\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.177274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.618109\n",
      "[400]\tvalid_0's l1: 0.502389\n",
      "[600]\tvalid_0's l1: 0.42047\n",
      "[800]\tvalid_0's l1: 0.36017\n",
      "[1000]\tvalid_0's l1: 0.316987\n",
      "[1200]\tvalid_0's l1: 0.283558\n",
      "[1400]\tvalid_0's l1: 0.260216\n",
      "[1600]\tvalid_0's l1: 0.240868\n",
      "[1800]\tvalid_0's l1: 0.225728\n",
      "[2000]\tvalid_0's l1: 0.214374\n",
      "[2200]\tvalid_0's l1: 0.205176\n",
      "[2400]\tvalid_0's l1: 0.198456\n",
      "[2600]\tvalid_0's l1: 0.193265\n",
      "[2800]\tvalid_0's l1: 0.189219\n",
      "[3000]\tvalid_0's l1: 0.185897\n",
      "[3200]\tvalid_0's l1: 0.183109\n",
      "[3400]\tvalid_0's l1: 0.180769\n",
      "[3600]\tvalid_0's l1: 0.178754\n",
      "[3800]\tvalid_0's l1: 0.177133\n",
      "[4000]\tvalid_0's l1: 0.175803\n",
      "[4200]\tvalid_0's l1: 0.174163\n",
      "[4400]\tvalid_0's l1: 0.172812\n",
      "[4600]\tvalid_0's l1: 0.171525\n",
      "[4800]\tvalid_0's l1: 0.170592\n",
      "[5000]\tvalid_0's l1: 0.169823\n",
      "[5200]\tvalid_0's l1: 0.16917\n",
      "[5400]\tvalid_0's l1: 0.168627\n",
      "[5600]\tvalid_0's l1: 0.167998\n",
      "[5800]\tvalid_0's l1: 0.16729\n",
      "[6000]\tvalid_0's l1: 0.167062\n",
      "[6200]\tvalid_0's l1: 0.166347\n",
      "[6400]\tvalid_0's l1: 0.165816\n",
      "[6600]\tvalid_0's l1: 0.165542\n",
      "[6800]\tvalid_0's l1: 0.165212\n",
      "[7000]\tvalid_0's l1: 0.164946\n",
      "[7200]\tvalid_0's l1: 0.164763\n",
      "[7400]\tvalid_0's l1: 0.164576\n",
      "[7600]\tvalid_0's l1: 0.164439\n",
      "[7800]\tvalid_0's l1: 0.164343\n",
      "[8000]\tvalid_0's l1: 0.164247\n",
      "[8200]\tvalid_0's l1: 0.163907\n",
      "[8400]\tvalid_0's l1: 0.163266\n",
      "[8600]\tvalid_0's l1: 0.162658\n",
      "[8800]\tvalid_0's l1: 0.16208\n",
      "[9000]\tvalid_0's l1: 0.16156\n",
      "[9200]\tvalid_0's l1: 0.160997\n",
      "[9400]\tvalid_0's l1: 0.16054\n",
      "[9600]\tvalid_0's l1: 0.160088\n",
      "[9800]\tvalid_0's l1: 0.159729\n",
      "[10000]\tvalid_0's l1: 0.159437\n",
      "[10200]\tvalid_0's l1: 0.159186\n",
      "[10400]\tvalid_0's l1: 0.158951\n",
      "[10600]\tvalid_0's l1: 0.158781\n",
      "[10800]\tvalid_0's l1: 0.158614\n",
      "[11000]\tvalid_0's l1: 0.158564\n",
      "Early stopping, best iteration is:\n",
      "[10966]\tvalid_0's l1: 0.15856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.3395 (weight: 0.016)\n",
      "Random Forest MAPE: 1.7078 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.5256 (weight: 0.003)\n",
      "Gradient Boosting MAPE: 1.3284 (weight: 0.018)\n",
      "Ridge MAPE: 1.1278 (weight: 0.137)\n",
      "Elastic Net MAPE: 0.9481 (weight: 0.825)\n",
      "Huber MAPE: 1.8197 (weight: 0.000)\n",
      "Ensemble MAPE: 0.9468\n",
      "\n",
      "Training for BlendProperty4...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.665573\n",
      "[400]\tvalid_0's l1: 0.567144\n",
      "[600]\tvalid_0's l1: 0.494174\n",
      "[800]\tvalid_0's l1: 0.437712\n",
      "[1000]\tvalid_0's l1: 0.395124\n",
      "[1200]\tvalid_0's l1: 0.362003\n",
      "[1400]\tvalid_0's l1: 0.335874\n",
      "[1600]\tvalid_0's l1: 0.314341\n",
      "[1800]\tvalid_0's l1: 0.296501\n",
      "[2000]\tvalid_0's l1: 0.28121\n",
      "[2200]\tvalid_0's l1: 0.268376\n",
      "[2400]\tvalid_0's l1: 0.258103\n",
      "[2600]\tvalid_0's l1: 0.2492\n",
      "[2800]\tvalid_0's l1: 0.241683\n",
      "[3000]\tvalid_0's l1: 0.235391\n",
      "[3200]\tvalid_0's l1: 0.230038\n",
      "[3400]\tvalid_0's l1: 0.225538\n",
      "[3600]\tvalid_0's l1: 0.221665\n",
      "[3800]\tvalid_0's l1: 0.218442\n",
      "[4000]\tvalid_0's l1: 0.215312\n",
      "[4200]\tvalid_0's l1: 0.212646\n",
      "[4400]\tvalid_0's l1: 0.210425\n",
      "[4600]\tvalid_0's l1: 0.20822\n",
      "[4800]\tvalid_0's l1: 0.205248\n",
      "[5000]\tvalid_0's l1: 0.203228\n",
      "[5200]\tvalid_0's l1: 0.20139\n",
      "[5400]\tvalid_0's l1: 0.199895\n",
      "[5600]\tvalid_0's l1: 0.198677\n",
      "[5800]\tvalid_0's l1: 0.19762\n",
      "[6000]\tvalid_0's l1: 0.196002\n",
      "[6200]\tvalid_0's l1: 0.194409\n",
      "[6400]\tvalid_0's l1: 0.192895\n",
      "[6600]\tvalid_0's l1: 0.191672\n",
      "[6800]\tvalid_0's l1: 0.190596\n",
      "[7000]\tvalid_0's l1: 0.18964\n",
      "[7200]\tvalid_0's l1: 0.188866\n",
      "[7400]\tvalid_0's l1: 0.188312\n",
      "[7600]\tvalid_0's l1: 0.187797\n",
      "[7800]\tvalid_0's l1: 0.187369\n",
      "[8000]\tvalid_0's l1: 0.187005\n",
      "[8200]\tvalid_0's l1: 0.186596\n",
      "[8400]\tvalid_0's l1: 0.185319\n",
      "[8600]\tvalid_0's l1: 0.183937\n",
      "[8800]\tvalid_0's l1: 0.183075\n",
      "[9000]\tvalid_0's l1: 0.182456\n",
      "[9200]\tvalid_0's l1: 0.181451\n",
      "[9400]\tvalid_0's l1: 0.180653\n",
      "[9600]\tvalid_0's l1: 0.179983\n",
      "[9800]\tvalid_0's l1: 0.179441\n",
      "[10000]\tvalid_0's l1: 0.178954\n",
      "[10200]\tvalid_0's l1: 0.178531\n",
      "[10400]\tvalid_0's l1: 0.17813\n",
      "[10600]\tvalid_0's l1: 0.177864\n",
      "[10800]\tvalid_0's l1: 0.177596\n",
      "[11000]\tvalid_0's l1: 0.177276\n",
      "[11200]\tvalid_0's l1: 0.176778\n",
      "[11400]\tvalid_0's l1: 0.176365\n",
      "[11600]\tvalid_0's l1: 0.175895\n",
      "[11800]\tvalid_0's l1: 0.175594\n",
      "[12000]\tvalid_0's l1: 0.17527\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.17527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.68282\n",
      "[400]\tvalid_0's l1: 0.574099\n",
      "[600]\tvalid_0's l1: 0.491617\n",
      "[800]\tvalid_0's l1: 0.42886\n",
      "[1000]\tvalid_0's l1: 0.381929\n",
      "[1200]\tvalid_0's l1: 0.344799\n",
      "[1400]\tvalid_0's l1: 0.315638\n",
      "[1600]\tvalid_0's l1: 0.292311\n",
      "[1800]\tvalid_0's l1: 0.273742\n",
      "[2000]\tvalid_0's l1: 0.259311\n",
      "[2200]\tvalid_0's l1: 0.247153\n",
      "[2400]\tvalid_0's l1: 0.237456\n",
      "[2600]\tvalid_0's l1: 0.229327\n",
      "[2800]\tvalid_0's l1: 0.222517\n",
      "[3000]\tvalid_0's l1: 0.216841\n",
      "[3200]\tvalid_0's l1: 0.212036\n",
      "[3400]\tvalid_0's l1: 0.20823\n",
      "[3600]\tvalid_0's l1: 0.204685\n",
      "[3800]\tvalid_0's l1: 0.201561\n",
      "[4000]\tvalid_0's l1: 0.198796\n",
      "[4200]\tvalid_0's l1: 0.195908\n",
      "[4400]\tvalid_0's l1: 0.193349\n",
      "[4600]\tvalid_0's l1: 0.191282\n",
      "[4800]\tvalid_0's l1: 0.189617\n",
      "[5000]\tvalid_0's l1: 0.188137\n",
      "[5200]\tvalid_0's l1: 0.186908\n",
      "[5400]\tvalid_0's l1: 0.184931\n",
      "[5600]\tvalid_0's l1: 0.182938\n",
      "[5800]\tvalid_0's l1: 0.181292\n",
      "[6000]\tvalid_0's l1: 0.179943\n",
      "[6200]\tvalid_0's l1: 0.178945\n",
      "[6400]\tvalid_0's l1: 0.177992\n",
      "[6600]\tvalid_0's l1: 0.177314\n",
      "[6800]\tvalid_0's l1: 0.176776\n",
      "[7000]\tvalid_0's l1: 0.175665\n",
      "[7200]\tvalid_0's l1: 0.17434\n",
      "[7400]\tvalid_0's l1: 0.173432\n",
      "[7600]\tvalid_0's l1: 0.172669\n",
      "[7800]\tvalid_0's l1: 0.17197\n",
      "[8000]\tvalid_0's l1: 0.171301\n",
      "[8200]\tvalid_0's l1: 0.170641\n",
      "[8400]\tvalid_0's l1: 0.170119\n",
      "[8600]\tvalid_0's l1: 0.169684\n",
      "[8800]\tvalid_0's l1: 0.169198\n",
      "[9000]\tvalid_0's l1: 0.168644\n",
      "[9200]\tvalid_0's l1: 0.168265\n",
      "[9400]\tvalid_0's l1: 0.16788\n",
      "[9600]\tvalid_0's l1: 0.167663\n",
      "[9800]\tvalid_0's l1: 0.167559\n",
      "[10000]\tvalid_0's l1: 0.167351\n",
      "[10200]\tvalid_0's l1: 0.16705\n",
      "[10400]\tvalid_0's l1: 0.166728\n",
      "[10600]\tvalid_0's l1: 0.166313\n",
      "[10800]\tvalid_0's l1: 0.165954\n",
      "[11000]\tvalid_0's l1: 0.165595\n",
      "[11200]\tvalid_0's l1: 0.165267\n",
      "[11400]\tvalid_0's l1: 0.164968\n",
      "[11600]\tvalid_0's l1: 0.164715\n",
      "[11800]\tvalid_0's l1: 0.164497\n",
      "[12000]\tvalid_0's l1: 0.164143\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.164143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.668244\n",
      "[400]\tvalid_0's l1: 0.561587\n",
      "[600]\tvalid_0's l1: 0.482398\n",
      "[800]\tvalid_0's l1: 0.420882\n",
      "[1000]\tvalid_0's l1: 0.374868\n",
      "[1200]\tvalid_0's l1: 0.340241\n",
      "[1400]\tvalid_0's l1: 0.31384\n",
      "[1600]\tvalid_0's l1: 0.29249\n",
      "[1800]\tvalid_0's l1: 0.275855\n",
      "[2000]\tvalid_0's l1: 0.261811\n",
      "[2200]\tvalid_0's l1: 0.250661\n",
      "[2400]\tvalid_0's l1: 0.241214\n",
      "[2600]\tvalid_0's l1: 0.233665\n",
      "[2800]\tvalid_0's l1: 0.227149\n",
      "[3000]\tvalid_0's l1: 0.221509\n",
      "[3200]\tvalid_0's l1: 0.216898\n",
      "[3400]\tvalid_0's l1: 0.213011\n",
      "[3600]\tvalid_0's l1: 0.209548\n",
      "[3800]\tvalid_0's l1: 0.206422\n",
      "[4000]\tvalid_0's l1: 0.20367\n",
      "[4200]\tvalid_0's l1: 0.201126\n",
      "[4400]\tvalid_0's l1: 0.199356\n",
      "[4600]\tvalid_0's l1: 0.197974\n",
      "[4800]\tvalid_0's l1: 0.196828\n",
      "[5000]\tvalid_0's l1: 0.195697\n",
      "[5200]\tvalid_0's l1: 0.193616\n",
      "[5400]\tvalid_0's l1: 0.190933\n",
      "[5600]\tvalid_0's l1: 0.188711\n",
      "[5800]\tvalid_0's l1: 0.186556\n",
      "[6000]\tvalid_0's l1: 0.184809\n",
      "[6200]\tvalid_0's l1: 0.183314\n",
      "[6400]\tvalid_0's l1: 0.182258\n",
      "[6600]\tvalid_0's l1: 0.18127\n",
      "[6800]\tvalid_0's l1: 0.180537\n",
      "[7000]\tvalid_0's l1: 0.180085\n",
      "[7200]\tvalid_0's l1: 0.179574\n",
      "[7400]\tvalid_0's l1: 0.179146\n",
      "[7600]\tvalid_0's l1: 0.178694\n",
      "[7800]\tvalid_0's l1: 0.178362\n",
      "[8000]\tvalid_0's l1: 0.178135\n",
      "[8200]\tvalid_0's l1: 0.177886\n",
      "[8400]\tvalid_0's l1: 0.177569\n",
      "[8600]\tvalid_0's l1: 0.177314\n",
      "[8800]\tvalid_0's l1: 0.177046\n",
      "[9000]\tvalid_0's l1: 0.176901\n",
      "[9200]\tvalid_0's l1: 0.1767\n",
      "[9400]\tvalid_0's l1: 0.176481\n",
      "[9600]\tvalid_0's l1: 0.176285\n",
      "[9800]\tvalid_0's l1: 0.176126\n",
      "[10000]\tvalid_0's l1: 0.175689\n",
      "[10200]\tvalid_0's l1: 0.175227\n",
      "[10400]\tvalid_0's l1: 0.174674\n",
      "[10600]\tvalid_0's l1: 0.174046\n",
      "[10800]\tvalid_0's l1: 0.17317\n",
      "[11000]\tvalid_0's l1: 0.172623\n",
      "[11200]\tvalid_0's l1: 0.17216\n",
      "[11400]\tvalid_0's l1: 0.171708\n",
      "[11600]\tvalid_0's l1: 0.171345\n",
      "[11800]\tvalid_0's l1: 0.170974\n",
      "[12000]\tvalid_0's l1: 0.170614\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.170614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.672863\n",
      "[400]\tvalid_0's l1: 0.56789\n",
      "[600]\tvalid_0's l1: 0.490275\n",
      "[800]\tvalid_0's l1: 0.429687\n",
      "[1000]\tvalid_0's l1: 0.383421\n",
      "[1200]\tvalid_0's l1: 0.349085\n",
      "[1400]\tvalid_0's l1: 0.32258\n",
      "[1600]\tvalid_0's l1: 0.301705\n",
      "[1800]\tvalid_0's l1: 0.284772\n",
      "[2000]\tvalid_0's l1: 0.270663\n",
      "[2200]\tvalid_0's l1: 0.25908\n",
      "[2400]\tvalid_0's l1: 0.249386\n",
      "[2600]\tvalid_0's l1: 0.241155\n",
      "[2800]\tvalid_0's l1: 0.233489\n",
      "[3000]\tvalid_0's l1: 0.227251\n",
      "[3200]\tvalid_0's l1: 0.222184\n",
      "[3400]\tvalid_0's l1: 0.217931\n",
      "[3600]\tvalid_0's l1: 0.214314\n",
      "[3800]\tvalid_0's l1: 0.211488\n",
      "[4000]\tvalid_0's l1: 0.208312\n",
      "[4200]\tvalid_0's l1: 0.205698\n",
      "[4400]\tvalid_0's l1: 0.203008\n",
      "[4600]\tvalid_0's l1: 0.200456\n",
      "[4800]\tvalid_0's l1: 0.198765\n",
      "[5000]\tvalid_0's l1: 0.197368\n",
      "[5200]\tvalid_0's l1: 0.195426\n",
      "[5400]\tvalid_0's l1: 0.193375\n",
      "[5600]\tvalid_0's l1: 0.191045\n",
      "[5800]\tvalid_0's l1: 0.189088\n",
      "[6000]\tvalid_0's l1: 0.187483\n",
      "[6200]\tvalid_0's l1: 0.186283\n",
      "[6400]\tvalid_0's l1: 0.18508\n",
      "[6600]\tvalid_0's l1: 0.184246\n",
      "[6800]\tvalid_0's l1: 0.183568\n",
      "[7000]\tvalid_0's l1: 0.182492\n",
      "[7200]\tvalid_0's l1: 0.181492\n",
      "[7400]\tvalid_0's l1: 0.180809\n",
      "[7600]\tvalid_0's l1: 0.180134\n",
      "[7800]\tvalid_0's l1: 0.17948\n",
      "[8000]\tvalid_0's l1: 0.179055\n",
      "[8200]\tvalid_0's l1: 0.178662\n",
      "[8400]\tvalid_0's l1: 0.178277\n",
      "[8600]\tvalid_0's l1: 0.177934\n",
      "[8800]\tvalid_0's l1: 0.177562\n",
      "[9000]\tvalid_0's l1: 0.177215\n",
      "[9200]\tvalid_0's l1: 0.176379\n",
      "[9400]\tvalid_0's l1: 0.175778\n",
      "[9600]\tvalid_0's l1: 0.175394\n",
      "[9800]\tvalid_0's l1: 0.174879\n",
      "[10000]\tvalid_0's l1: 0.174241\n",
      "[10200]\tvalid_0's l1: 0.173323\n",
      "[10400]\tvalid_0's l1: 0.172498\n",
      "[10600]\tvalid_0's l1: 0.171759\n",
      "[10800]\tvalid_0's l1: 0.171228\n",
      "[11000]\tvalid_0's l1: 0.170881\n",
      "[11200]\tvalid_0's l1: 0.170397\n",
      "[11400]\tvalid_0's l1: 0.170148\n",
      "[11600]\tvalid_0's l1: 0.169861\n",
      "[11800]\tvalid_0's l1: 0.169608\n",
      "[12000]\tvalid_0's l1: 0.169277\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.169277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.72373\n",
      "[400]\tvalid_0's l1: 0.614909\n",
      "[600]\tvalid_0's l1: 0.53236\n",
      "[800]\tvalid_0's l1: 0.46767\n",
      "[1000]\tvalid_0's l1: 0.41744\n",
      "[1200]\tvalid_0's l1: 0.378487\n",
      "[1400]\tvalid_0's l1: 0.347344\n",
      "[1600]\tvalid_0's l1: 0.323796\n",
      "[1800]\tvalid_0's l1: 0.304979\n",
      "[2000]\tvalid_0's l1: 0.289108\n",
      "[2200]\tvalid_0's l1: 0.275785\n",
      "[2400]\tvalid_0's l1: 0.264501\n",
      "[2600]\tvalid_0's l1: 0.255394\n",
      "[2800]\tvalid_0's l1: 0.247359\n",
      "[3000]\tvalid_0's l1: 0.240818\n",
      "[3200]\tvalid_0's l1: 0.235309\n",
      "[3400]\tvalid_0's l1: 0.230527\n",
      "[3600]\tvalid_0's l1: 0.226352\n",
      "[3800]\tvalid_0's l1: 0.223052\n",
      "[4000]\tvalid_0's l1: 0.219784\n",
      "[4200]\tvalid_0's l1: 0.215679\n",
      "[4400]\tvalid_0's l1: 0.212395\n",
      "[4600]\tvalid_0's l1: 0.210219\n",
      "[4800]\tvalid_0's l1: 0.208393\n",
      "[5000]\tvalid_0's l1: 0.206352\n",
      "[5200]\tvalid_0's l1: 0.204205\n",
      "[5400]\tvalid_0's l1: 0.202228\n",
      "[5600]\tvalid_0's l1: 0.200585\n",
      "[5800]\tvalid_0's l1: 0.199132\n",
      "[6000]\tvalid_0's l1: 0.197909\n",
      "[6200]\tvalid_0's l1: 0.197072\n",
      "[6400]\tvalid_0's l1: 0.196272\n",
      "[6600]\tvalid_0's l1: 0.194388\n",
      "[6800]\tvalid_0's l1: 0.192849\n",
      "[7000]\tvalid_0's l1: 0.191574\n",
      "[7200]\tvalid_0's l1: 0.190303\n",
      "[7400]\tvalid_0's l1: 0.189221\n",
      "[7600]\tvalid_0's l1: 0.188357\n",
      "[7800]\tvalid_0's l1: 0.187603\n",
      "[8000]\tvalid_0's l1: 0.18693\n",
      "[8200]\tvalid_0's l1: 0.186393\n",
      "[8400]\tvalid_0's l1: 0.185967\n",
      "[8600]\tvalid_0's l1: 0.185468\n",
      "[8800]\tvalid_0's l1: 0.185109\n",
      "[9000]\tvalid_0's l1: 0.184865\n",
      "[9200]\tvalid_0's l1: 0.184099\n",
      "[9400]\tvalid_0's l1: 0.182923\n",
      "[9600]\tvalid_0's l1: 0.181874\n",
      "[9800]\tvalid_0's l1: 0.180924\n",
      "[10000]\tvalid_0's l1: 0.180028\n",
      "[10200]\tvalid_0's l1: 0.179323\n",
      "[10400]\tvalid_0's l1: 0.17861\n",
      "[10600]\tvalid_0's l1: 0.178159\n",
      "[10800]\tvalid_0's l1: 0.17776\n",
      "[11000]\tvalid_0's l1: 0.177409\n",
      "[11200]\tvalid_0's l1: 0.177074\n",
      "[11400]\tvalid_0's l1: 0.176709\n",
      "[11600]\tvalid_0's l1: 0.176298\n",
      "[11800]\tvalid_0's l1: 0.176047\n",
      "[12000]\tvalid_0's l1: 0.175827\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.175827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.1984 (weight: 0.000)\n",
      "Random Forest MAPE: 2.1203 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.5720 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 1.4058 (weight: 0.000)\n",
      "Ridge MAPE: 0.4059 (weight: 0.474)\n",
      "Elastic Net MAPE: 0.3954 (weight: 0.526)\n",
      "Huber MAPE: 2.2123 (weight: 0.000)\n",
      "Ensemble MAPE: 0.3922\n",
      "\n",
      "Training for BlendProperty5...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.611771\n",
      "[400]\tvalid_0's l1: 0.493539\n",
      "[600]\tvalid_0's l1: 0.401977\n",
      "[800]\tvalid_0's l1: 0.330681\n",
      "[1000]\tvalid_0's l1: 0.273453\n",
      "[1200]\tvalid_0's l1: 0.229959\n",
      "[1400]\tvalid_0's l1: 0.196342\n",
      "[1600]\tvalid_0's l1: 0.169574\n",
      "[1800]\tvalid_0's l1: 0.148179\n",
      "[2000]\tvalid_0's l1: 0.130275\n",
      "[2200]\tvalid_0's l1: 0.117432\n",
      "[2400]\tvalid_0's l1: 0.110027\n",
      "[2600]\tvalid_0's l1: 0.105121\n",
      "[2800]\tvalid_0's l1: 0.0990846\n",
      "[3000]\tvalid_0's l1: 0.0951496\n",
      "[3200]\tvalid_0's l1: 0.0934141\n",
      "[3400]\tvalid_0's l1: 0.090317\n",
      "[3600]\tvalid_0's l1: 0.0878347\n",
      "[3800]\tvalid_0's l1: 0.0863475\n",
      "[4000]\tvalid_0's l1: 0.0850046\n",
      "[4200]\tvalid_0's l1: 0.0837343\n",
      "[4400]\tvalid_0's l1: 0.0824469\n",
      "[4600]\tvalid_0's l1: 0.0808473\n",
      "[4800]\tvalid_0's l1: 0.0798897\n",
      "[5000]\tvalid_0's l1: 0.0785338\n",
      "[5200]\tvalid_0's l1: 0.0772517\n",
      "[5400]\tvalid_0's l1: 0.076135\n",
      "[5600]\tvalid_0's l1: 0.0752023\n",
      "[5800]\tvalid_0's l1: 0.0744721\n",
      "[6000]\tvalid_0's l1: 0.0731374\n",
      "[6200]\tvalid_0's l1: 0.0719783\n",
      "[6400]\tvalid_0's l1: 0.0713757\n",
      "[6600]\tvalid_0's l1: 0.0708892\n",
      "[6800]\tvalid_0's l1: 0.0705439\n",
      "[7000]\tvalid_0's l1: 0.0701819\n",
      "[7200]\tvalid_0's l1: 0.0699242\n",
      "[7400]\tvalid_0's l1: 0.0697637\n",
      "[7600]\tvalid_0's l1: 0.0697496\n",
      "[7800]\tvalid_0's l1: 0.0693378\n",
      "[8000]\tvalid_0's l1: 0.069159\n",
      "[8200]\tvalid_0's l1: 0.0691427\n",
      "Early stopping, best iteration is:\n",
      "[8192]\tvalid_0's l1: 0.069141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.575e-01, tolerance: 1.514e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.533294\n",
      "[400]\tvalid_0's l1: 0.41781\n",
      "[600]\tvalid_0's l1: 0.332429\n",
      "[800]\tvalid_0's l1: 0.264371\n",
      "[1000]\tvalid_0's l1: 0.212193\n",
      "[1200]\tvalid_0's l1: 0.170946\n",
      "[1400]\tvalid_0's l1: 0.14138\n",
      "[1600]\tvalid_0's l1: 0.12169\n",
      "[1800]\tvalid_0's l1: 0.10703\n",
      "[2000]\tvalid_0's l1: 0.0971026\n",
      "[2200]\tvalid_0's l1: 0.0896061\n",
      "[2400]\tvalid_0's l1: 0.0842111\n",
      "[2600]\tvalid_0's l1: 0.0796162\n",
      "[2800]\tvalid_0's l1: 0.0759127\n",
      "[3000]\tvalid_0's l1: 0.0715096\n",
      "[3200]\tvalid_0's l1: 0.0692332\n",
      "[3400]\tvalid_0's l1: 0.066374\n",
      "[3600]\tvalid_0's l1: 0.0649769\n",
      "[3800]\tvalid_0's l1: 0.0635328\n",
      "[4000]\tvalid_0's l1: 0.0621285\n",
      "[4200]\tvalid_0's l1: 0.0613165\n",
      "[4400]\tvalid_0's l1: 0.0596078\n",
      "[4600]\tvalid_0's l1: 0.058395\n",
      "[4800]\tvalid_0's l1: 0.0573634\n",
      "[5000]\tvalid_0's l1: 0.056336\n",
      "[5200]\tvalid_0's l1: 0.0559584\n",
      "[5400]\tvalid_0's l1: 0.0556751\n",
      "[5600]\tvalid_0's l1: 0.055452\n",
      "[5800]\tvalid_0's l1: 0.0552675\n",
      "[6000]\tvalid_0's l1: 0.0551133\n",
      "[6200]\tvalid_0's l1: 0.0549776\n",
      "[6400]\tvalid_0's l1: 0.0548739\n",
      "[6600]\tvalid_0's l1: 0.0547344\n",
      "[6800]\tvalid_0's l1: 0.0539589\n",
      "[7000]\tvalid_0's l1: 0.0532278\n",
      "[7200]\tvalid_0's l1: 0.052795\n",
      "[7400]\tvalid_0's l1: 0.0525797\n",
      "[7600]\tvalid_0's l1: 0.0525003\n",
      "[7800]\tvalid_0's l1: 0.0524453\n",
      "[8000]\tvalid_0's l1: 0.0524069\n",
      "[8200]\tvalid_0's l1: 0.0523408\n",
      "[8400]\tvalid_0's l1: 0.0521419\n",
      "[8600]\tvalid_0's l1: 0.0518501\n",
      "[8800]\tvalid_0's l1: 0.0517565\n",
      "[9000]\tvalid_0's l1: 0.0517621\n",
      "Early stopping, best iteration is:\n",
      "[8887]\tvalid_0's l1: 0.0517152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.598448\n",
      "[400]\tvalid_0's l1: 0.473428\n",
      "[600]\tvalid_0's l1: 0.37544\n",
      "[800]\tvalid_0's l1: 0.298688\n",
      "[1000]\tvalid_0's l1: 0.241291\n",
      "[1200]\tvalid_0's l1: 0.196935\n",
      "[1400]\tvalid_0's l1: 0.164095\n",
      "[1600]\tvalid_0's l1: 0.140802\n",
      "[1800]\tvalid_0's l1: 0.122037\n",
      "[2000]\tvalid_0's l1: 0.107583\n",
      "[2200]\tvalid_0's l1: 0.096161\n",
      "[2400]\tvalid_0's l1: 0.0894682\n",
      "[2600]\tvalid_0's l1: 0.0850559\n",
      "[2800]\tvalid_0's l1: 0.0810627\n",
      "[3000]\tvalid_0's l1: 0.0781545\n",
      "[3200]\tvalid_0's l1: 0.0751487\n",
      "[3400]\tvalid_0's l1: 0.0719574\n",
      "[3600]\tvalid_0's l1: 0.0702144\n",
      "[3800]\tvalid_0's l1: 0.0688443\n",
      "[4000]\tvalid_0's l1: 0.0675931\n",
      "[4200]\tvalid_0's l1: 0.0656595\n",
      "[4400]\tvalid_0's l1: 0.0641684\n",
      "[4600]\tvalid_0's l1: 0.063438\n",
      "[4800]\tvalid_0's l1: 0.0630413\n",
      "[5000]\tvalid_0's l1: 0.062822\n",
      "[5200]\tvalid_0's l1: 0.0626795\n",
      "[5400]\tvalid_0's l1: 0.0626046\n",
      "[5600]\tvalid_0's l1: 0.0625062\n",
      "[5800]\tvalid_0's l1: 0.0623007\n",
      "[6000]\tvalid_0's l1: 0.0620416\n",
      "[6200]\tvalid_0's l1: 0.0610427\n",
      "[6400]\tvalid_0's l1: 0.060734\n",
      "[6600]\tvalid_0's l1: 0.0606709\n",
      "[6800]\tvalid_0's l1: 0.0604306\n",
      "[7000]\tvalid_0's l1: 0.0602421\n",
      "[7200]\tvalid_0's l1: 0.0597763\n",
      "[7400]\tvalid_0's l1: 0.0595921\n",
      "[7600]\tvalid_0's l1: 0.0594882\n",
      "[7800]\tvalid_0's l1: 0.0589809\n",
      "[8000]\tvalid_0's l1: 0.0586621\n",
      "[8200]\tvalid_0's l1: 0.0583549\n",
      "[8400]\tvalid_0's l1: 0.0575157\n",
      "[8600]\tvalid_0's l1: 0.056933\n",
      "[8800]\tvalid_0's l1: 0.0566206\n",
      "[9000]\tvalid_0's l1: 0.0565008\n",
      "[9200]\tvalid_0's l1: 0.0564331\n",
      "[9400]\tvalid_0's l1: 0.0563987\n",
      "[9600]\tvalid_0's l1: 0.0563548\n",
      "Early stopping, best iteration is:\n",
      "[9594]\tvalid_0's l1: 0.0563544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.572372\n",
      "[400]\tvalid_0's l1: 0.452757\n",
      "[600]\tvalid_0's l1: 0.364499\n",
      "[800]\tvalid_0's l1: 0.295368\n",
      "[1000]\tvalid_0's l1: 0.241663\n",
      "[1200]\tvalid_0's l1: 0.19815\n",
      "[1400]\tvalid_0's l1: 0.165651\n",
      "[1600]\tvalid_0's l1: 0.143136\n",
      "[1800]\tvalid_0's l1: 0.125401\n",
      "[2000]\tvalid_0's l1: 0.112915\n",
      "[2200]\tvalid_0's l1: 0.104431\n",
      "[2400]\tvalid_0's l1: 0.0973732\n",
      "[2600]\tvalid_0's l1: 0.0927327\n",
      "[2800]\tvalid_0's l1: 0.0890161\n",
      "[3000]\tvalid_0's l1: 0.0856277\n",
      "[3200]\tvalid_0's l1: 0.0826132\n",
      "[3400]\tvalid_0's l1: 0.0796467\n",
      "[3600]\tvalid_0's l1: 0.0756813\n",
      "[3800]\tvalid_0's l1: 0.0723282\n",
      "[4000]\tvalid_0's l1: 0.070312\n",
      "[4200]\tvalid_0's l1: 0.0684994\n",
      "[4400]\tvalid_0's l1: 0.067508\n",
      "[4600]\tvalid_0's l1: 0.0658933\n",
      "[4800]\tvalid_0's l1: 0.0649448\n",
      "[5000]\tvalid_0's l1: 0.0639251\n",
      "[5200]\tvalid_0's l1: 0.0634846\n",
      "[5400]\tvalid_0's l1: 0.0623235\n",
      "[5600]\tvalid_0's l1: 0.0614938\n",
      "[5800]\tvalid_0's l1: 0.0607703\n",
      "[6000]\tvalid_0's l1: 0.0597228\n",
      "[6200]\tvalid_0's l1: 0.0590061\n",
      "[6400]\tvalid_0's l1: 0.0587873\n",
      "[6600]\tvalid_0's l1: 0.0584901\n",
      "[6800]\tvalid_0's l1: 0.0575867\n",
      "[7000]\tvalid_0's l1: 0.0571371\n",
      "[7200]\tvalid_0's l1: 0.0568145\n",
      "[7400]\tvalid_0's l1: 0.0562641\n",
      "[7600]\tvalid_0's l1: 0.0558208\n",
      "[7800]\tvalid_0's l1: 0.0554121\n",
      "[8000]\tvalid_0's l1: 0.0551702\n",
      "[8200]\tvalid_0's l1: 0.0550442\n",
      "[8400]\tvalid_0's l1: 0.0549014\n",
      "[8600]\tvalid_0's l1: 0.0547251\n",
      "[8800]\tvalid_0's l1: 0.054569\n",
      "Early stopping, best iteration is:\n",
      "[8785]\tvalid_0's l1: 0.054559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.522753\n",
      "[400]\tvalid_0's l1: 0.404103\n",
      "[600]\tvalid_0's l1: 0.3147\n",
      "[800]\tvalid_0's l1: 0.246433\n",
      "[1000]\tvalid_0's l1: 0.196868\n",
      "[1200]\tvalid_0's l1: 0.162111\n",
      "[1400]\tvalid_0's l1: 0.139009\n",
      "[1600]\tvalid_0's l1: 0.122946\n",
      "[1800]\tvalid_0's l1: 0.111041\n",
      "[2000]\tvalid_0's l1: 0.102119\n",
      "[2200]\tvalid_0's l1: 0.0951275\n",
      "[2400]\tvalid_0's l1: 0.0894873\n",
      "[2600]\tvalid_0's l1: 0.0851027\n",
      "[2800]\tvalid_0's l1: 0.0822718\n",
      "[3000]\tvalid_0's l1: 0.0799195\n",
      "[3200]\tvalid_0's l1: 0.0773565\n",
      "[3400]\tvalid_0's l1: 0.075532\n",
      "[3600]\tvalid_0's l1: 0.0741353\n",
      "[3800]\tvalid_0's l1: 0.0726722\n",
      "[4000]\tvalid_0's l1: 0.0717999\n",
      "[4200]\tvalid_0's l1: 0.0713613\n",
      "[4400]\tvalid_0's l1: 0.0708258\n",
      "[4600]\tvalid_0's l1: 0.0702616\n",
      "[4800]\tvalid_0's l1: 0.0694932\n",
      "[5000]\tvalid_0's l1: 0.0690592\n",
      "Early stopping, best iteration is:\n",
      "[4936]\tvalid_0's l1: 0.0690378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 0.1832 (weight: 0.096)\n",
      "Random Forest MAPE: 0.0658 (weight: 0.311)\n",
      "Extra Trees MAPE: 0.0816 (weight: 0.265)\n",
      "Gradient Boosting MAPE: 0.0603 (weight: 0.328)\n",
      "Ridge MAPE: 4.3565 (weight: 0.000)\n",
      "Elastic Net MAPE: 4.3156 (weight: 0.000)\n",
      "Huber MAPE: 2.9261 (weight: 0.000)\n",
      "Ensemble MAPE: 0.0656\n",
      "\n",
      "Training for BlendProperty6...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.656722\n",
      "[400]\tvalid_0's l1: 0.558793\n",
      "[600]\tvalid_0's l1: 0.485053\n",
      "[800]\tvalid_0's l1: 0.427165\n",
      "[1000]\tvalid_0's l1: 0.380827\n",
      "[1200]\tvalid_0's l1: 0.344885\n",
      "[1400]\tvalid_0's l1: 0.316387\n",
      "[1600]\tvalid_0's l1: 0.293969\n",
      "[1800]\tvalid_0's l1: 0.276859\n",
      "[2000]\tvalid_0's l1: 0.263312\n",
      "[2200]\tvalid_0's l1: 0.252625\n",
      "[2400]\tvalid_0's l1: 0.243367\n",
      "[2600]\tvalid_0's l1: 0.235555\n",
      "[2800]\tvalid_0's l1: 0.228794\n",
      "[3000]\tvalid_0's l1: 0.223271\n",
      "[3200]\tvalid_0's l1: 0.218358\n",
      "[3400]\tvalid_0's l1: 0.214306\n",
      "[3600]\tvalid_0's l1: 0.210805\n",
      "[3800]\tvalid_0's l1: 0.207711\n",
      "[4000]\tvalid_0's l1: 0.205033\n",
      "[4200]\tvalid_0's l1: 0.202244\n",
      "[4400]\tvalid_0's l1: 0.200161\n",
      "[4600]\tvalid_0's l1: 0.198255\n",
      "[4800]\tvalid_0's l1: 0.196098\n",
      "[5000]\tvalid_0's l1: 0.194027\n",
      "[5200]\tvalid_0's l1: 0.192309\n",
      "[5400]\tvalid_0's l1: 0.190521\n",
      "[5600]\tvalid_0's l1: 0.18828\n",
      "[5800]\tvalid_0's l1: 0.186405\n",
      "[6000]\tvalid_0's l1: 0.18476\n",
      "[6200]\tvalid_0's l1: 0.183529\n",
      "[6400]\tvalid_0's l1: 0.182432\n",
      "[6600]\tvalid_0's l1: 0.181804\n",
      "[6800]\tvalid_0's l1: 0.181221\n",
      "[7000]\tvalid_0's l1: 0.180619\n",
      "[7200]\tvalid_0's l1: 0.179933\n",
      "[7400]\tvalid_0's l1: 0.179251\n",
      "[7600]\tvalid_0's l1: 0.178619\n",
      "[7800]\tvalid_0's l1: 0.177948\n",
      "[8000]\tvalid_0's l1: 0.177567\n",
      "[8200]\tvalid_0's l1: 0.177204\n",
      "[8400]\tvalid_0's l1: 0.176799\n",
      "[8600]\tvalid_0's l1: 0.176256\n",
      "[8800]\tvalid_0's l1: 0.17554\n",
      "[9000]\tvalid_0's l1: 0.174872\n",
      "[9200]\tvalid_0's l1: 0.17424\n",
      "[9400]\tvalid_0's l1: 0.173724\n",
      "[9600]\tvalid_0's l1: 0.173368\n",
      "[9800]\tvalid_0's l1: 0.173008\n",
      "[10000]\tvalid_0's l1: 0.172662\n",
      "[10200]\tvalid_0's l1: 0.172197\n",
      "[10400]\tvalid_0's l1: 0.171799\n",
      "[10600]\tvalid_0's l1: 0.171423\n",
      "[10800]\tvalid_0's l1: 0.1711\n",
      "[11000]\tvalid_0's l1: 0.170843\n",
      "[11200]\tvalid_0's l1: 0.170571\n",
      "[11400]\tvalid_0's l1: 0.170296\n",
      "[11600]\tvalid_0's l1: 0.170141\n",
      "[11800]\tvalid_0's l1: 0.17012\n",
      "Early stopping, best iteration is:\n",
      "[11731]\tvalid_0's l1: 0.170117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.693585\n",
      "[400]\tvalid_0's l1: 0.588784\n",
      "[600]\tvalid_0's l1: 0.511281\n",
      "[800]\tvalid_0's l1: 0.452666\n",
      "[1000]\tvalid_0's l1: 0.406549\n",
      "[1200]\tvalid_0's l1: 0.369218\n",
      "[1400]\tvalid_0's l1: 0.340919\n",
      "[1600]\tvalid_0's l1: 0.318568\n",
      "[1800]\tvalid_0's l1: 0.299438\n",
      "[2000]\tvalid_0's l1: 0.284389\n",
      "[2200]\tvalid_0's l1: 0.271475\n",
      "[2400]\tvalid_0's l1: 0.260664\n",
      "[2600]\tvalid_0's l1: 0.251869\n",
      "[2800]\tvalid_0's l1: 0.244497\n",
      "[3000]\tvalid_0's l1: 0.237897\n",
      "[3200]\tvalid_0's l1: 0.232506\n",
      "[3400]\tvalid_0's l1: 0.227904\n",
      "[3600]\tvalid_0's l1: 0.224106\n",
      "[3800]\tvalid_0's l1: 0.219883\n",
      "[4000]\tvalid_0's l1: 0.216189\n",
      "[4200]\tvalid_0's l1: 0.21398\n",
      "[4400]\tvalid_0's l1: 0.211567\n",
      "[4600]\tvalid_0's l1: 0.209333\n",
      "[4800]\tvalid_0's l1: 0.207614\n",
      "[5000]\tvalid_0's l1: 0.206266\n",
      "[5200]\tvalid_0's l1: 0.202801\n",
      "[5400]\tvalid_0's l1: 0.199937\n",
      "[5600]\tvalid_0's l1: 0.197078\n",
      "[5800]\tvalid_0's l1: 0.194772\n",
      "[6000]\tvalid_0's l1: 0.192718\n",
      "[6200]\tvalid_0's l1: 0.191292\n",
      "[6400]\tvalid_0's l1: 0.190079\n",
      "[6600]\tvalid_0's l1: 0.18933\n",
      "[6800]\tvalid_0's l1: 0.187922\n",
      "[7000]\tvalid_0's l1: 0.186636\n",
      "[7200]\tvalid_0's l1: 0.185683\n",
      "[7400]\tvalid_0's l1: 0.184855\n",
      "[7600]\tvalid_0's l1: 0.18413\n",
      "[7800]\tvalid_0's l1: 0.183553\n",
      "[8000]\tvalid_0's l1: 0.183063\n",
      "[8200]\tvalid_0's l1: 0.182719\n",
      "[8400]\tvalid_0's l1: 0.182265\n",
      "[8600]\tvalid_0's l1: 0.181737\n",
      "[8800]\tvalid_0's l1: 0.181283\n",
      "[9000]\tvalid_0's l1: 0.180886\n",
      "[9200]\tvalid_0's l1: 0.180419\n",
      "[9400]\tvalid_0's l1: 0.18002\n",
      "[9600]\tvalid_0's l1: 0.179586\n",
      "[9800]\tvalid_0's l1: 0.179151\n",
      "[10000]\tvalid_0's l1: 0.178767\n",
      "[10200]\tvalid_0's l1: 0.178444\n",
      "[10400]\tvalid_0's l1: 0.178148\n",
      "[10600]\tvalid_0's l1: 0.177926\n",
      "[10800]\tvalid_0's l1: 0.177781\n",
      "[11000]\tvalid_0's l1: 0.177662\n",
      "[11200]\tvalid_0's l1: 0.177454\n",
      "[11400]\tvalid_0's l1: 0.177161\n",
      "[11600]\tvalid_0's l1: 0.176706\n",
      "[11800]\tvalid_0's l1: 0.176183\n",
      "[12000]\tvalid_0's l1: 0.17569\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.17569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.643151\n",
      "[400]\tvalid_0's l1: 0.545318\n",
      "[600]\tvalid_0's l1: 0.471642\n",
      "[800]\tvalid_0's l1: 0.416609\n",
      "[1000]\tvalid_0's l1: 0.372926\n",
      "[1200]\tvalid_0's l1: 0.33956\n",
      "[1400]\tvalid_0's l1: 0.313325\n",
      "[1600]\tvalid_0's l1: 0.291833\n",
      "[1800]\tvalid_0's l1: 0.274275\n",
      "[2000]\tvalid_0's l1: 0.2596\n",
      "[2200]\tvalid_0's l1: 0.246822\n",
      "[2400]\tvalid_0's l1: 0.236522\n",
      "[2600]\tvalid_0's l1: 0.228108\n",
      "[2800]\tvalid_0's l1: 0.220931\n",
      "[3000]\tvalid_0's l1: 0.214246\n",
      "[3200]\tvalid_0's l1: 0.208562\n",
      "[3400]\tvalid_0's l1: 0.203258\n",
      "[3600]\tvalid_0's l1: 0.198982\n",
      "[3800]\tvalid_0's l1: 0.195481\n",
      "[4000]\tvalid_0's l1: 0.192796\n",
      "[4200]\tvalid_0's l1: 0.190795\n",
      "[4400]\tvalid_0's l1: 0.188824\n",
      "[4600]\tvalid_0's l1: 0.186769\n",
      "[4800]\tvalid_0's l1: 0.184845\n",
      "[5000]\tvalid_0's l1: 0.18245\n",
      "[5200]\tvalid_0's l1: 0.180027\n",
      "[5400]\tvalid_0's l1: 0.178488\n",
      "[5600]\tvalid_0's l1: 0.177405\n",
      "[5800]\tvalid_0's l1: 0.176356\n",
      "[6000]\tvalid_0's l1: 0.175244\n",
      "[6200]\tvalid_0's l1: 0.173441\n",
      "[6400]\tvalid_0's l1: 0.172112\n",
      "[6600]\tvalid_0's l1: 0.170774\n",
      "[6800]\tvalid_0's l1: 0.168617\n",
      "[7000]\tvalid_0's l1: 0.167338\n",
      "[7200]\tvalid_0's l1: 0.165774\n",
      "[7400]\tvalid_0's l1: 0.164465\n",
      "[7600]\tvalid_0's l1: 0.163357\n",
      "[7800]\tvalid_0's l1: 0.162331\n",
      "[8000]\tvalid_0's l1: 0.161546\n",
      "[8200]\tvalid_0's l1: 0.161307\n",
      "[8400]\tvalid_0's l1: 0.161\n",
      "[8600]\tvalid_0's l1: 0.160746\n",
      "[8800]\tvalid_0's l1: 0.160529\n",
      "[9000]\tvalid_0's l1: 0.160238\n",
      "[9200]\tvalid_0's l1: 0.160016\n",
      "[9400]\tvalid_0's l1: 0.159734\n",
      "[9600]\tvalid_0's l1: 0.159559\n",
      "[9800]\tvalid_0's l1: 0.159487\n",
      "[10000]\tvalid_0's l1: 0.159363\n",
      "[10200]\tvalid_0's l1: 0.159194\n",
      "[10400]\tvalid_0's l1: 0.159052\n",
      "[10600]\tvalid_0's l1: 0.158861\n",
      "[10800]\tvalid_0's l1: 0.15865\n",
      "[11000]\tvalid_0's l1: 0.158503\n",
      "[11200]\tvalid_0's l1: 0.158336\n",
      "[11400]\tvalid_0's l1: 0.158187\n",
      "[11600]\tvalid_0's l1: 0.158042\n",
      "[11800]\tvalid_0's l1: 0.157911\n",
      "[12000]\tvalid_0's l1: 0.157786\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.157786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.713847\n",
      "[400]\tvalid_0's l1: 0.612139\n",
      "[600]\tvalid_0's l1: 0.533211\n",
      "[800]\tvalid_0's l1: 0.47155\n",
      "[1000]\tvalid_0's l1: 0.423107\n",
      "[1200]\tvalid_0's l1: 0.383962\n",
      "[1400]\tvalid_0's l1: 0.351637\n",
      "[1600]\tvalid_0's l1: 0.326184\n",
      "[1800]\tvalid_0's l1: 0.305703\n",
      "[2000]\tvalid_0's l1: 0.288868\n",
      "[2200]\tvalid_0's l1: 0.275147\n",
      "[2400]\tvalid_0's l1: 0.263481\n",
      "[2600]\tvalid_0's l1: 0.253693\n",
      "[2800]\tvalid_0's l1: 0.245872\n",
      "[3000]\tvalid_0's l1: 0.239224\n",
      "[3200]\tvalid_0's l1: 0.233708\n",
      "[3400]\tvalid_0's l1: 0.228724\n",
      "[3600]\tvalid_0's l1: 0.224827\n",
      "[3800]\tvalid_0's l1: 0.221442\n",
      "[4000]\tvalid_0's l1: 0.218365\n",
      "[4200]\tvalid_0's l1: 0.214155\n",
      "[4400]\tvalid_0's l1: 0.210624\n",
      "[4600]\tvalid_0's l1: 0.208129\n",
      "[4800]\tvalid_0's l1: 0.205778\n",
      "[5000]\tvalid_0's l1: 0.203768\n",
      "[5200]\tvalid_0's l1: 0.201849\n",
      "[5400]\tvalid_0's l1: 0.200366\n",
      "[5600]\tvalid_0's l1: 0.198144\n",
      "[5800]\tvalid_0's l1: 0.196238\n",
      "[6000]\tvalid_0's l1: 0.194494\n",
      "[6200]\tvalid_0's l1: 0.192942\n",
      "[6400]\tvalid_0's l1: 0.191666\n",
      "[6600]\tvalid_0's l1: 0.190582\n",
      "[6800]\tvalid_0's l1: 0.189596\n",
      "[7000]\tvalid_0's l1: 0.188765\n",
      "[7200]\tvalid_0's l1: 0.187991\n",
      "[7400]\tvalid_0's l1: 0.18732\n",
      "[7600]\tvalid_0's l1: 0.186617\n",
      "[7800]\tvalid_0's l1: 0.185936\n",
      "[8000]\tvalid_0's l1: 0.185353\n",
      "[8200]\tvalid_0's l1: 0.18511\n",
      "[8400]\tvalid_0's l1: 0.184798\n",
      "[8600]\tvalid_0's l1: 0.184507\n",
      "[8800]\tvalid_0's l1: 0.18405\n",
      "[9000]\tvalid_0's l1: 0.183599\n",
      "[9200]\tvalid_0's l1: 0.183081\n",
      "[9400]\tvalid_0's l1: 0.182675\n",
      "[9600]\tvalid_0's l1: 0.18233\n",
      "[9800]\tvalid_0's l1: 0.182058\n",
      "[10000]\tvalid_0's l1: 0.181866\n",
      "[10200]\tvalid_0's l1: 0.181682\n",
      "[10400]\tvalid_0's l1: 0.181489\n",
      "[10600]\tvalid_0's l1: 0.1812\n",
      "[10800]\tvalid_0's l1: 0.180929\n",
      "[11000]\tvalid_0's l1: 0.180696\n",
      "[11200]\tvalid_0's l1: 0.180542\n",
      "[11400]\tvalid_0's l1: 0.180373\n",
      "[11600]\tvalid_0's l1: 0.18009\n",
      "[11800]\tvalid_0's l1: 0.179661\n",
      "[12000]\tvalid_0's l1: 0.179229\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.179229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.673659\n",
      "[400]\tvalid_0's l1: 0.573798\n",
      "[600]\tvalid_0's l1: 0.495282\n",
      "[800]\tvalid_0's l1: 0.436398\n",
      "[1000]\tvalid_0's l1: 0.391165\n",
      "[1200]\tvalid_0's l1: 0.35574\n",
      "[1400]\tvalid_0's l1: 0.328055\n",
      "[1600]\tvalid_0's l1: 0.304887\n",
      "[1800]\tvalid_0's l1: 0.286385\n",
      "[2000]\tvalid_0's l1: 0.271559\n",
      "[2200]\tvalid_0's l1: 0.259782\n",
      "[2400]\tvalid_0's l1: 0.250172\n",
      "[2600]\tvalid_0's l1: 0.242386\n",
      "[2800]\tvalid_0's l1: 0.235749\n",
      "[3000]\tvalid_0's l1: 0.230382\n",
      "[3200]\tvalid_0's l1: 0.225625\n",
      "[3400]\tvalid_0's l1: 0.221206\n",
      "[3600]\tvalid_0's l1: 0.217709\n",
      "[3800]\tvalid_0's l1: 0.214523\n",
      "[4000]\tvalid_0's l1: 0.211504\n",
      "[4200]\tvalid_0's l1: 0.208546\n",
      "[4400]\tvalid_0's l1: 0.205684\n",
      "[4600]\tvalid_0's l1: 0.203268\n",
      "[4800]\tvalid_0's l1: 0.201167\n",
      "[5000]\tvalid_0's l1: 0.199185\n",
      "[5200]\tvalid_0's l1: 0.196742\n",
      "[5400]\tvalid_0's l1: 0.194831\n",
      "[5600]\tvalid_0's l1: 0.193334\n",
      "[5800]\tvalid_0's l1: 0.191531\n",
      "[6000]\tvalid_0's l1: 0.189576\n",
      "[6200]\tvalid_0's l1: 0.188037\n",
      "[6400]\tvalid_0's l1: 0.186853\n",
      "[6600]\tvalid_0's l1: 0.186055\n",
      "[6800]\tvalid_0's l1: 0.185125\n",
      "[7000]\tvalid_0's l1: 0.184244\n",
      "[7200]\tvalid_0's l1: 0.183472\n",
      "[7400]\tvalid_0's l1: 0.182835\n",
      "[7600]\tvalid_0's l1: 0.181913\n",
      "[7800]\tvalid_0's l1: 0.180889\n",
      "[8000]\tvalid_0's l1: 0.180116\n",
      "[8200]\tvalid_0's l1: 0.179433\n",
      "[8400]\tvalid_0's l1: 0.178816\n",
      "[8600]\tvalid_0's l1: 0.178272\n",
      "[8800]\tvalid_0's l1: 0.177707\n",
      "[9000]\tvalid_0's l1: 0.177205\n",
      "[9200]\tvalid_0's l1: 0.176808\n",
      "[9400]\tvalid_0's l1: 0.176468\n",
      "[9600]\tvalid_0's l1: 0.176219\n",
      "[9800]\tvalid_0's l1: 0.176062\n",
      "[10000]\tvalid_0's l1: 0.175927\n",
      "[10200]\tvalid_0's l1: 0.175444\n",
      "[10400]\tvalid_0's l1: 0.175038\n",
      "[10600]\tvalid_0's l1: 0.174625\n",
      "[10800]\tvalid_0's l1: 0.174162\n",
      "[11000]\tvalid_0's l1: 0.173931\n",
      "[11200]\tvalid_0's l1: 0.173499\n",
      "[11400]\tvalid_0's l1: 0.173204\n",
      "[11600]\tvalid_0's l1: 0.172992\n",
      "[11800]\tvalid_0's l1: 0.17278\n",
      "[12000]\tvalid_0's l1: 0.172613\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.172613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 0.9761 (weight: 0.000)\n",
      "Random Forest MAPE: 1.3239 (weight: 0.000)\n",
      "Extra Trees MAPE: 0.9889 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 1.1097 (weight: 0.000)\n",
      "Ridge MAPE: 0.0124 (weight: 0.521)\n",
      "Elastic Net MAPE: 0.0207 (weight: 0.479)\n",
      "Huber MAPE: 2.0396 (weight: 0.000)\n",
      "Ensemble MAPE: 0.0148\n",
      "\n",
      "Training for BlendProperty7...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.622243\n",
      "[400]\tvalid_0's l1: 0.515958\n",
      "[600]\tvalid_0's l1: 0.440921\n",
      "[800]\tvalid_0's l1: 0.384849\n",
      "[1000]\tvalid_0's l1: 0.342614\n",
      "[1200]\tvalid_0's l1: 0.310425\n",
      "[1400]\tvalid_0's l1: 0.285642\n",
      "[1600]\tvalid_0's l1: 0.266833\n",
      "[1800]\tvalid_0's l1: 0.253459\n",
      "[2000]\tvalid_0's l1: 0.24212\n",
      "[2200]\tvalid_0's l1: 0.232585\n",
      "[2400]\tvalid_0's l1: 0.22493\n",
      "[2600]\tvalid_0's l1: 0.218814\n",
      "[2800]\tvalid_0's l1: 0.214061\n",
      "[3000]\tvalid_0's l1: 0.209699\n",
      "[3200]\tvalid_0's l1: 0.206039\n",
      "[3400]\tvalid_0's l1: 0.203006\n",
      "[3600]\tvalid_0's l1: 0.200578\n",
      "[3800]\tvalid_0's l1: 0.198395\n",
      "[4000]\tvalid_0's l1: 0.196571\n",
      "[4200]\tvalid_0's l1: 0.194872\n",
      "[4400]\tvalid_0's l1: 0.193362\n",
      "[4600]\tvalid_0's l1: 0.192193\n",
      "[4800]\tvalid_0's l1: 0.190217\n",
      "[5000]\tvalid_0's l1: 0.188191\n",
      "[5200]\tvalid_0's l1: 0.18633\n",
      "[5400]\tvalid_0's l1: 0.184872\n",
      "[5600]\tvalid_0's l1: 0.183795\n",
      "[5800]\tvalid_0's l1: 0.182741\n",
      "[6000]\tvalid_0's l1: 0.181746\n",
      "[6200]\tvalid_0's l1: 0.181003\n",
      "[6400]\tvalid_0's l1: 0.180394\n",
      "[6600]\tvalid_0's l1: 0.180049\n",
      "[6800]\tvalid_0's l1: 0.179605\n",
      "[7000]\tvalid_0's l1: 0.178978\n",
      "[7200]\tvalid_0's l1: 0.178311\n",
      "[7400]\tvalid_0's l1: 0.177404\n",
      "[7600]\tvalid_0's l1: 0.176613\n",
      "[7800]\tvalid_0's l1: 0.175859\n",
      "[8000]\tvalid_0's l1: 0.175202\n",
      "[8200]\tvalid_0's l1: 0.174703\n",
      "[8400]\tvalid_0's l1: 0.174356\n",
      "[8600]\tvalid_0's l1: 0.174044\n",
      "[8800]\tvalid_0's l1: 0.173697\n",
      "[9000]\tvalid_0's l1: 0.173521\n",
      "[9200]\tvalid_0's l1: 0.173218\n",
      "[9400]\tvalid_0's l1: 0.172689\n",
      "[9600]\tvalid_0's l1: 0.17224\n",
      "[9800]\tvalid_0's l1: 0.171615\n",
      "[10000]\tvalid_0's l1: 0.170944\n",
      "[10200]\tvalid_0's l1: 0.17031\n",
      "[10400]\tvalid_0's l1: 0.16972\n",
      "[10600]\tvalid_0's l1: 0.169271\n",
      "[10800]\tvalid_0's l1: 0.168925\n",
      "[11000]\tvalid_0's l1: 0.168642\n",
      "[11200]\tvalid_0's l1: 0.168285\n",
      "[11400]\tvalid_0's l1: 0.168037\n",
      "[11600]\tvalid_0's l1: 0.167858\n",
      "[11800]\tvalid_0's l1: 0.167639\n",
      "[12000]\tvalid_0's l1: 0.167388\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.167388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.672036\n",
      "[400]\tvalid_0's l1: 0.546605\n",
      "[600]\tvalid_0's l1: 0.454782\n",
      "[800]\tvalid_0's l1: 0.387549\n",
      "[1000]\tvalid_0's l1: 0.340035\n",
      "[1200]\tvalid_0's l1: 0.305383\n",
      "[1400]\tvalid_0's l1: 0.277429\n",
      "[1600]\tvalid_0's l1: 0.256502\n",
      "[1800]\tvalid_0's l1: 0.242001\n",
      "[2000]\tvalid_0's l1: 0.229862\n",
      "[2200]\tvalid_0's l1: 0.220404\n",
      "[2400]\tvalid_0's l1: 0.213472\n",
      "[2600]\tvalid_0's l1: 0.20782\n",
      "[2800]\tvalid_0's l1: 0.203366\n",
      "[3000]\tvalid_0's l1: 0.199481\n",
      "[3200]\tvalid_0's l1: 0.196375\n",
      "[3400]\tvalid_0's l1: 0.193686\n",
      "[3600]\tvalid_0's l1: 0.191435\n",
      "[3800]\tvalid_0's l1: 0.18949\n",
      "[4000]\tvalid_0's l1: 0.187045\n",
      "[4200]\tvalid_0's l1: 0.184572\n",
      "[4400]\tvalid_0's l1: 0.182474\n",
      "[4600]\tvalid_0's l1: 0.180484\n",
      "[4800]\tvalid_0's l1: 0.179288\n",
      "[5000]\tvalid_0's l1: 0.178065\n",
      "[5200]\tvalid_0's l1: 0.176748\n",
      "[5400]\tvalid_0's l1: 0.17557\n",
      "[5600]\tvalid_0's l1: 0.174591\n",
      "[5800]\tvalid_0's l1: 0.173813\n",
      "[6000]\tvalid_0's l1: 0.173273\n",
      "[6200]\tvalid_0's l1: 0.172785\n",
      "[6400]\tvalid_0's l1: 0.172237\n",
      "[6600]\tvalid_0's l1: 0.171682\n",
      "[6800]\tvalid_0's l1: 0.171175\n",
      "[7000]\tvalid_0's l1: 0.170695\n",
      "[7200]\tvalid_0's l1: 0.170293\n",
      "[7400]\tvalid_0's l1: 0.169989\n",
      "[7600]\tvalid_0's l1: 0.1693\n",
      "[7800]\tvalid_0's l1: 0.168701\n",
      "[8000]\tvalid_0's l1: 0.167467\n",
      "[8200]\tvalid_0's l1: 0.1666\n",
      "[8400]\tvalid_0's l1: 0.16559\n",
      "[8600]\tvalid_0's l1: 0.164662\n",
      "[8800]\tvalid_0's l1: 0.164093\n",
      "[9000]\tvalid_0's l1: 0.163685\n",
      "[9200]\tvalid_0's l1: 0.163337\n",
      "[9400]\tvalid_0's l1: 0.162957\n",
      "[9600]\tvalid_0's l1: 0.162513\n",
      "[9800]\tvalid_0's l1: 0.162036\n",
      "[10000]\tvalid_0's l1: 0.161458\n",
      "[10200]\tvalid_0's l1: 0.161028\n",
      "[10400]\tvalid_0's l1: 0.16079\n",
      "[10600]\tvalid_0's l1: 0.160548\n",
      "[10800]\tvalid_0's l1: 0.160384\n",
      "[11000]\tvalid_0's l1: 0.160201\n",
      "[11200]\tvalid_0's l1: 0.160062\n",
      "[11400]\tvalid_0's l1: 0.159945\n",
      "[11600]\tvalid_0's l1: 0.159845\n",
      "[11800]\tvalid_0's l1: 0.159706\n",
      "[12000]\tvalid_0's l1: 0.159643\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.159643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.615476\n",
      "[400]\tvalid_0's l1: 0.511269\n",
      "[600]\tvalid_0's l1: 0.437903\n",
      "[800]\tvalid_0's l1: 0.38512\n",
      "[1000]\tvalid_0's l1: 0.345578\n",
      "[1200]\tvalid_0's l1: 0.317028\n",
      "[1400]\tvalid_0's l1: 0.295235\n",
      "[1600]\tvalid_0's l1: 0.277244\n",
      "[1800]\tvalid_0's l1: 0.263135\n",
      "[2000]\tvalid_0's l1: 0.251419\n",
      "[2200]\tvalid_0's l1: 0.241494\n",
      "[2400]\tvalid_0's l1: 0.233948\n",
      "[2600]\tvalid_0's l1: 0.227165\n",
      "[2800]\tvalid_0's l1: 0.2205\n",
      "[3000]\tvalid_0's l1: 0.214823\n",
      "[3200]\tvalid_0's l1: 0.210974\n",
      "[3400]\tvalid_0's l1: 0.20759\n",
      "[3600]\tvalid_0's l1: 0.204832\n",
      "[3800]\tvalid_0's l1: 0.202299\n",
      "[4000]\tvalid_0's l1: 0.200012\n",
      "[4200]\tvalid_0's l1: 0.198125\n",
      "[4400]\tvalid_0's l1: 0.196206\n",
      "[4600]\tvalid_0's l1: 0.194739\n",
      "[4800]\tvalid_0's l1: 0.193251\n",
      "[5000]\tvalid_0's l1: 0.191934\n",
      "[5200]\tvalid_0's l1: 0.190835\n",
      "[5400]\tvalid_0's l1: 0.189948\n",
      "[5600]\tvalid_0's l1: 0.189066\n",
      "[5800]\tvalid_0's l1: 0.187586\n",
      "[6000]\tvalid_0's l1: 0.185673\n",
      "[6200]\tvalid_0's l1: 0.183959\n",
      "[6400]\tvalid_0's l1: 0.181921\n",
      "[6600]\tvalid_0's l1: 0.180355\n",
      "[6800]\tvalid_0's l1: 0.179246\n",
      "[7000]\tvalid_0's l1: 0.17824\n",
      "[7200]\tvalid_0's l1: 0.177462\n",
      "[7400]\tvalid_0's l1: 0.176826\n",
      "[7600]\tvalid_0's l1: 0.176315\n",
      "[7800]\tvalid_0's l1: 0.175705\n",
      "[8000]\tvalid_0's l1: 0.175572\n",
      "[8200]\tvalid_0's l1: 0.175392\n",
      "[8400]\tvalid_0's l1: 0.175031\n",
      "[8600]\tvalid_0's l1: 0.174432\n",
      "[8800]\tvalid_0's l1: 0.173905\n",
      "[9000]\tvalid_0's l1: 0.173457\n",
      "[9200]\tvalid_0's l1: 0.172971\n",
      "[9400]\tvalid_0's l1: 0.172619\n",
      "[9600]\tvalid_0's l1: 0.17216\n",
      "[9800]\tvalid_0's l1: 0.171744\n",
      "[10000]\tvalid_0's l1: 0.171502\n",
      "[10200]\tvalid_0's l1: 0.171281\n",
      "[10400]\tvalid_0's l1: 0.171071\n",
      "[10600]\tvalid_0's l1: 0.170917\n",
      "[10800]\tvalid_0's l1: 0.170807\n",
      "[11000]\tvalid_0's l1: 0.170724\n",
      "[11200]\tvalid_0's l1: 0.170664\n",
      "[11400]\tvalid_0's l1: 0.170577\n",
      "[11600]\tvalid_0's l1: 0.170414\n",
      "[11800]\tvalid_0's l1: 0.170175\n",
      "[12000]\tvalid_0's l1: 0.169859\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.169859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.650274\n",
      "[400]\tvalid_0's l1: 0.54558\n",
      "[600]\tvalid_0's l1: 0.470607\n",
      "[800]\tvalid_0's l1: 0.415078\n",
      "[1000]\tvalid_0's l1: 0.372063\n",
      "[1200]\tvalid_0's l1: 0.340447\n",
      "[1400]\tvalid_0's l1: 0.316068\n",
      "[1600]\tvalid_0's l1: 0.29717\n",
      "[1800]\tvalid_0's l1: 0.282465\n",
      "[2000]\tvalid_0's l1: 0.270693\n",
      "[2200]\tvalid_0's l1: 0.261382\n",
      "[2400]\tvalid_0's l1: 0.254051\n",
      "[2600]\tvalid_0's l1: 0.248101\n",
      "[2800]\tvalid_0's l1: 0.243092\n",
      "[3000]\tvalid_0's l1: 0.238952\n",
      "[3200]\tvalid_0's l1: 0.235357\n",
      "[3400]\tvalid_0's l1: 0.23196\n",
      "[3600]\tvalid_0's l1: 0.229264\n",
      "[3800]\tvalid_0's l1: 0.226987\n",
      "[4000]\tvalid_0's l1: 0.224718\n",
      "[4200]\tvalid_0's l1: 0.22224\n",
      "[4400]\tvalid_0's l1: 0.219444\n",
      "[4600]\tvalid_0's l1: 0.216149\n",
      "[4800]\tvalid_0's l1: 0.213229\n",
      "[5000]\tvalid_0's l1: 0.21117\n",
      "[5200]\tvalid_0's l1: 0.209315\n",
      "[5400]\tvalid_0's l1: 0.207939\n",
      "[5600]\tvalid_0's l1: 0.206797\n",
      "[5800]\tvalid_0's l1: 0.206105\n",
      "[6000]\tvalid_0's l1: 0.204823\n",
      "[6200]\tvalid_0's l1: 0.203417\n",
      "[6400]\tvalid_0's l1: 0.201769\n",
      "[6600]\tvalid_0's l1: 0.200318\n",
      "[6800]\tvalid_0's l1: 0.19918\n",
      "[7000]\tvalid_0's l1: 0.198365\n",
      "[7200]\tvalid_0's l1: 0.197747\n",
      "[7400]\tvalid_0's l1: 0.197186\n",
      "[7600]\tvalid_0's l1: 0.196719\n",
      "[7800]\tvalid_0's l1: 0.196362\n",
      "[8000]\tvalid_0's l1: 0.196064\n",
      "[8200]\tvalid_0's l1: 0.195857\n",
      "[8400]\tvalid_0's l1: 0.195594\n",
      "[8600]\tvalid_0's l1: 0.195123\n",
      "[8800]\tvalid_0's l1: 0.1944\n",
      "[9000]\tvalid_0's l1: 0.19361\n",
      "[9200]\tvalid_0's l1: 0.192629\n",
      "[9400]\tvalid_0's l1: 0.191752\n",
      "[9600]\tvalid_0's l1: 0.19053\n",
      "[9800]\tvalid_0's l1: 0.189788\n",
      "[10000]\tvalid_0's l1: 0.189384\n",
      "[10200]\tvalid_0's l1: 0.188994\n",
      "[10400]\tvalid_0's l1: 0.188571\n",
      "[10600]\tvalid_0's l1: 0.188105\n",
      "[10800]\tvalid_0's l1: 0.187745\n",
      "[11000]\tvalid_0's l1: 0.187357\n",
      "[11200]\tvalid_0's l1: 0.187002\n",
      "[11400]\tvalid_0's l1: 0.186863\n",
      "[11600]\tvalid_0's l1: 0.186818\n",
      "[11800]\tvalid_0's l1: 0.186708\n",
      "[12000]\tvalid_0's l1: 0.186455\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.186455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.614416\n",
      "[400]\tvalid_0's l1: 0.501101\n",
      "[600]\tvalid_0's l1: 0.420182\n",
      "[800]\tvalid_0's l1: 0.360921\n",
      "[1000]\tvalid_0's l1: 0.318801\n",
      "[1200]\tvalid_0's l1: 0.286597\n",
      "[1400]\tvalid_0's l1: 0.263347\n",
      "[1600]\tvalid_0's l1: 0.244347\n",
      "[1800]\tvalid_0's l1: 0.229565\n",
      "[2000]\tvalid_0's l1: 0.218111\n",
      "[2200]\tvalid_0's l1: 0.209156\n",
      "[2400]\tvalid_0's l1: 0.202437\n",
      "[2600]\tvalid_0's l1: 0.197031\n",
      "[2800]\tvalid_0's l1: 0.192803\n",
      "[3000]\tvalid_0's l1: 0.189623\n",
      "[3200]\tvalid_0's l1: 0.18703\n",
      "[3400]\tvalid_0's l1: 0.184789\n",
      "[3600]\tvalid_0's l1: 0.182666\n",
      "[3800]\tvalid_0's l1: 0.180784\n",
      "[4000]\tvalid_0's l1: 0.179472\n",
      "[4200]\tvalid_0's l1: 0.17751\n",
      "[4400]\tvalid_0's l1: 0.175395\n",
      "[4600]\tvalid_0's l1: 0.173513\n",
      "[4800]\tvalid_0's l1: 0.172224\n",
      "[5000]\tvalid_0's l1: 0.171119\n",
      "[5200]\tvalid_0's l1: 0.170131\n",
      "[5400]\tvalid_0's l1: 0.16947\n",
      "[5600]\tvalid_0's l1: 0.168846\n",
      "[5800]\tvalid_0's l1: 0.168124\n",
      "[6000]\tvalid_0's l1: 0.167397\n",
      "[6200]\tvalid_0's l1: 0.167114\n",
      "[6400]\tvalid_0's l1: 0.166341\n",
      "[6600]\tvalid_0's l1: 0.165903\n",
      "[6800]\tvalid_0's l1: 0.165449\n",
      "[7000]\tvalid_0's l1: 0.165126\n",
      "[7200]\tvalid_0's l1: 0.164819\n",
      "[7400]\tvalid_0's l1: 0.164402\n",
      "[7600]\tvalid_0's l1: 0.164154\n",
      "[7800]\tvalid_0's l1: 0.163899\n",
      "[8000]\tvalid_0's l1: 0.163633\n",
      "[8200]\tvalid_0's l1: 0.163422\n",
      "[8400]\tvalid_0's l1: 0.163202\n",
      "[8600]\tvalid_0's l1: 0.162951\n",
      "[8800]\tvalid_0's l1: 0.162522\n",
      "[9000]\tvalid_0's l1: 0.16212\n",
      "[9200]\tvalid_0's l1: 0.161764\n",
      "[9400]\tvalid_0's l1: 0.161435\n",
      "[9600]\tvalid_0's l1: 0.161095\n",
      "[9800]\tvalid_0's l1: 0.160828\n",
      "[10000]\tvalid_0's l1: 0.160525\n",
      "[10200]\tvalid_0's l1: 0.160325\n",
      "[10400]\tvalid_0's l1: 0.160124\n",
      "[10600]\tvalid_0's l1: 0.15992\n",
      "[10800]\tvalid_0's l1: 0.159717\n",
      "[11000]\tvalid_0's l1: 0.159508\n",
      "[11200]\tvalid_0's l1: 0.159333\n",
      "[11400]\tvalid_0's l1: 0.159162\n",
      "[11600]\tvalid_0's l1: 0.159019\n",
      "[11800]\tvalid_0's l1: 0.15895\n",
      "Early stopping, best iteration is:\n",
      "[11834]\tvalid_0's l1: 0.158946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.8017 (weight: 0.128)\n",
      "Random Forest MAPE: 2.9278 (weight: 0.000)\n",
      "Extra Trees MAPE: 2.6188 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 2.2762 (weight: 0.001)\n",
      "Ridge MAPE: 1.8681 (weight: 0.066)\n",
      "Elastic Net MAPE: 1.6173 (weight: 0.806)\n",
      "Huber MAPE: 3.1646 (weight: 0.000)\n",
      "Ensemble MAPE: 1.6104\n",
      "\n",
      "Training for BlendProperty8...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.660295\n",
      "[400]\tvalid_0's l1: 0.573394\n",
      "[600]\tvalid_0's l1: 0.508361\n",
      "[800]\tvalid_0's l1: 0.457803\n",
      "[1000]\tvalid_0's l1: 0.417956\n",
      "[1200]\tvalid_0's l1: 0.386557\n",
      "[1400]\tvalid_0's l1: 0.362385\n",
      "[1600]\tvalid_0's l1: 0.342409\n",
      "[1800]\tvalid_0's l1: 0.326093\n",
      "[2000]\tvalid_0's l1: 0.312518\n",
      "[2200]\tvalid_0's l1: 0.301525\n",
      "[2400]\tvalid_0's l1: 0.292372\n",
      "[2600]\tvalid_0's l1: 0.284715\n",
      "[2800]\tvalid_0's l1: 0.278162\n",
      "[3000]\tvalid_0's l1: 0.272373\n",
      "[3200]\tvalid_0's l1: 0.26742\n",
      "[3400]\tvalid_0's l1: 0.262986\n",
      "[3600]\tvalid_0's l1: 0.258826\n",
      "[3800]\tvalid_0's l1: 0.255093\n",
      "[4000]\tvalid_0's l1: 0.251614\n",
      "[4200]\tvalid_0's l1: 0.248726\n",
      "[4400]\tvalid_0's l1: 0.246526\n",
      "[4600]\tvalid_0's l1: 0.244184\n",
      "[4800]\tvalid_0's l1: 0.241596\n",
      "[5000]\tvalid_0's l1: 0.23919\n",
      "[5200]\tvalid_0's l1: 0.236509\n",
      "[5400]\tvalid_0's l1: 0.234613\n",
      "[5600]\tvalid_0's l1: 0.233003\n",
      "[5800]\tvalid_0's l1: 0.231623\n",
      "[6000]\tvalid_0's l1: 0.230387\n",
      "[6200]\tvalid_0's l1: 0.22914\n",
      "[6400]\tvalid_0's l1: 0.228269\n",
      "[6600]\tvalid_0's l1: 0.227078\n",
      "[6800]\tvalid_0's l1: 0.225511\n",
      "[7000]\tvalid_0's l1: 0.223796\n",
      "[7200]\tvalid_0's l1: 0.22243\n",
      "[7400]\tvalid_0's l1: 0.221396\n",
      "[7600]\tvalid_0's l1: 0.220579\n",
      "[7800]\tvalid_0's l1: 0.21995\n",
      "[8000]\tvalid_0's l1: 0.219192\n",
      "[8200]\tvalid_0's l1: 0.218373\n",
      "[8400]\tvalid_0's l1: 0.217767\n",
      "[8600]\tvalid_0's l1: 0.217273\n",
      "[8800]\tvalid_0's l1: 0.216839\n",
      "[9000]\tvalid_0's l1: 0.216401\n",
      "[9200]\tvalid_0's l1: 0.21595\n",
      "[9400]\tvalid_0's l1: 0.215707\n",
      "[9600]\tvalid_0's l1: 0.215398\n",
      "[9800]\tvalid_0's l1: 0.215007\n",
      "[10000]\tvalid_0's l1: 0.214383\n",
      "[10200]\tvalid_0's l1: 0.213855\n",
      "[10400]\tvalid_0's l1: 0.213176\n",
      "[10600]\tvalid_0's l1: 0.212715\n",
      "[10800]\tvalid_0's l1: 0.21191\n",
      "[11000]\tvalid_0's l1: 0.21109\n",
      "[11200]\tvalid_0's l1: 0.21059\n",
      "[11400]\tvalid_0's l1: 0.210183\n",
      "[11600]\tvalid_0's l1: 0.20982\n",
      "[11800]\tvalid_0's l1: 0.20946\n",
      "[12000]\tvalid_0's l1: 0.209037\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.209037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.693916\n",
      "[400]\tvalid_0's l1: 0.58918\n",
      "[600]\tvalid_0's l1: 0.51384\n",
      "[800]\tvalid_0's l1: 0.455401\n",
      "[1000]\tvalid_0's l1: 0.413216\n",
      "[1200]\tvalid_0's l1: 0.380473\n",
      "[1400]\tvalid_0's l1: 0.354666\n",
      "[1600]\tvalid_0's l1: 0.33374\n",
      "[1800]\tvalid_0's l1: 0.317715\n",
      "[2000]\tvalid_0's l1: 0.305123\n",
      "[2200]\tvalid_0's l1: 0.294943\n",
      "[2400]\tvalid_0's l1: 0.286303\n",
      "[2600]\tvalid_0's l1: 0.279139\n",
      "[2800]\tvalid_0's l1: 0.272801\n",
      "[3000]\tvalid_0's l1: 0.267359\n",
      "[3200]\tvalid_0's l1: 0.262972\n",
      "[3400]\tvalid_0's l1: 0.259186\n",
      "[3600]\tvalid_0's l1: 0.254988\n",
      "[3800]\tvalid_0's l1: 0.250995\n",
      "[4000]\tvalid_0's l1: 0.247831\n",
      "[4200]\tvalid_0's l1: 0.244532\n",
      "[4400]\tvalid_0's l1: 0.241767\n",
      "[4600]\tvalid_0's l1: 0.239312\n",
      "[4800]\tvalid_0's l1: 0.237411\n",
      "[5000]\tvalid_0's l1: 0.235731\n",
      "[5200]\tvalid_0's l1: 0.234408\n",
      "[5400]\tvalid_0's l1: 0.233298\n",
      "[5600]\tvalid_0's l1: 0.231398\n",
      "[5800]\tvalid_0's l1: 0.227956\n",
      "[6000]\tvalid_0's l1: 0.225036\n",
      "[6200]\tvalid_0's l1: 0.222678\n",
      "[6400]\tvalid_0's l1: 0.22076\n",
      "[6600]\tvalid_0's l1: 0.218635\n",
      "[6800]\tvalid_0's l1: 0.217148\n",
      "[7000]\tvalid_0's l1: 0.216008\n",
      "[7200]\tvalid_0's l1: 0.215141\n",
      "[7400]\tvalid_0's l1: 0.214537\n",
      "[7600]\tvalid_0's l1: 0.213892\n",
      "[7800]\tvalid_0's l1: 0.21325\n",
      "[8000]\tvalid_0's l1: 0.212643\n",
      "[8200]\tvalid_0's l1: 0.211628\n",
      "[8400]\tvalid_0's l1: 0.210811\n",
      "[8600]\tvalid_0's l1: 0.210032\n",
      "[8800]\tvalid_0's l1: 0.209308\n",
      "[9000]\tvalid_0's l1: 0.208657\n",
      "[9200]\tvalid_0's l1: 0.208149\n",
      "[9400]\tvalid_0's l1: 0.207669\n",
      "[9600]\tvalid_0's l1: 0.207118\n",
      "[9800]\tvalid_0's l1: 0.206806\n",
      "[10000]\tvalid_0's l1: 0.20649\n",
      "[10200]\tvalid_0's l1: 0.206185\n",
      "[10400]\tvalid_0's l1: 0.20588\n",
      "[10600]\tvalid_0's l1: 0.20559\n",
      "[10800]\tvalid_0's l1: 0.205378\n",
      "[11000]\tvalid_0's l1: 0.205185\n",
      "[11200]\tvalid_0's l1: 0.204898\n",
      "[11400]\tvalid_0's l1: 0.204481\n",
      "[11600]\tvalid_0's l1: 0.20396\n",
      "[11800]\tvalid_0's l1: 0.203367\n",
      "[12000]\tvalid_0's l1: 0.202712\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.202712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.638737\n",
      "[400]\tvalid_0's l1: 0.54896\n",
      "[600]\tvalid_0's l1: 0.483347\n",
      "[800]\tvalid_0's l1: 0.436031\n",
      "[1000]\tvalid_0's l1: 0.401583\n",
      "[1200]\tvalid_0's l1: 0.373951\n",
      "[1400]\tvalid_0's l1: 0.351097\n",
      "[1600]\tvalid_0's l1: 0.332623\n",
      "[1800]\tvalid_0's l1: 0.317068\n",
      "[2000]\tvalid_0's l1: 0.303604\n",
      "[2200]\tvalid_0's l1: 0.293009\n",
      "[2400]\tvalid_0's l1: 0.284288\n",
      "[2600]\tvalid_0's l1: 0.277336\n",
      "[2800]\tvalid_0's l1: 0.271344\n",
      "[3000]\tvalid_0's l1: 0.265869\n",
      "[3200]\tvalid_0's l1: 0.261386\n",
      "[3400]\tvalid_0's l1: 0.257347\n",
      "[3600]\tvalid_0's l1: 0.253913\n",
      "[3800]\tvalid_0's l1: 0.249935\n",
      "[4000]\tvalid_0's l1: 0.247273\n",
      "[4200]\tvalid_0's l1: 0.244779\n",
      "[4400]\tvalid_0's l1: 0.242061\n",
      "[4600]\tvalid_0's l1: 0.239799\n",
      "[4800]\tvalid_0's l1: 0.237831\n",
      "[5000]\tvalid_0's l1: 0.235956\n",
      "[5200]\tvalid_0's l1: 0.234392\n",
      "[5400]\tvalid_0's l1: 0.233016\n",
      "[5600]\tvalid_0's l1: 0.231058\n",
      "[5800]\tvalid_0's l1: 0.229315\n",
      "[6000]\tvalid_0's l1: 0.227579\n",
      "[6200]\tvalid_0's l1: 0.226182\n",
      "[6400]\tvalid_0's l1: 0.22514\n",
      "[6600]\tvalid_0's l1: 0.224218\n",
      "[6800]\tvalid_0's l1: 0.223256\n",
      "[7000]\tvalid_0's l1: 0.222479\n",
      "[7200]\tvalid_0's l1: 0.221887\n",
      "[7400]\tvalid_0's l1: 0.221396\n",
      "[7600]\tvalid_0's l1: 0.220987\n",
      "[7800]\tvalid_0's l1: 0.220404\n",
      "[8000]\tvalid_0's l1: 0.219346\n",
      "[8200]\tvalid_0's l1: 0.218188\n",
      "[8400]\tvalid_0's l1: 0.217186\n",
      "[8600]\tvalid_0's l1: 0.216339\n",
      "[8800]\tvalid_0's l1: 0.215674\n",
      "[9000]\tvalid_0's l1: 0.214923\n",
      "[9200]\tvalid_0's l1: 0.214308\n",
      "[9400]\tvalid_0's l1: 0.213815\n",
      "[9600]\tvalid_0's l1: 0.213324\n",
      "[9800]\tvalid_0's l1: 0.212872\n",
      "[10000]\tvalid_0's l1: 0.212361\n",
      "[10200]\tvalid_0's l1: 0.212006\n",
      "[10400]\tvalid_0's l1: 0.211781\n",
      "[10600]\tvalid_0's l1: 0.211676\n",
      "[10800]\tvalid_0's l1: 0.211595\n",
      "[11000]\tvalid_0's l1: 0.211397\n",
      "[11200]\tvalid_0's l1: 0.211206\n",
      "[11400]\tvalid_0's l1: 0.210968\n",
      "[11600]\tvalid_0's l1: 0.210785\n",
      "[11800]\tvalid_0's l1: 0.210535\n",
      "[12000]\tvalid_0's l1: 0.210351\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.210351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.693653\n",
      "[400]\tvalid_0's l1: 0.596927\n",
      "[600]\tvalid_0's l1: 0.525412\n",
      "[800]\tvalid_0's l1: 0.469883\n",
      "[1000]\tvalid_0's l1: 0.427463\n",
      "[1200]\tvalid_0's l1: 0.394687\n",
      "[1400]\tvalid_0's l1: 0.368898\n",
      "[1600]\tvalid_0's l1: 0.347935\n",
      "[1800]\tvalid_0's l1: 0.330733\n",
      "[2000]\tvalid_0's l1: 0.316469\n",
      "[2200]\tvalid_0's l1: 0.304672\n",
      "[2400]\tvalid_0's l1: 0.294891\n",
      "[2600]\tvalid_0's l1: 0.286606\n",
      "[2800]\tvalid_0's l1: 0.280063\n",
      "[3000]\tvalid_0's l1: 0.274626\n",
      "[3200]\tvalid_0's l1: 0.270198\n",
      "[3400]\tvalid_0's l1: 0.26626\n",
      "[3600]\tvalid_0's l1: 0.262594\n",
      "[3800]\tvalid_0's l1: 0.259007\n",
      "[4000]\tvalid_0's l1: 0.256161\n",
      "[4200]\tvalid_0's l1: 0.253737\n",
      "[4400]\tvalid_0's l1: 0.25099\n",
      "[4600]\tvalid_0's l1: 0.248121\n",
      "[4800]\tvalid_0's l1: 0.246004\n",
      "[5000]\tvalid_0's l1: 0.244278\n",
      "[5200]\tvalid_0's l1: 0.242762\n",
      "[5400]\tvalid_0's l1: 0.241525\n",
      "[5600]\tvalid_0's l1: 0.2399\n",
      "[5800]\tvalid_0's l1: 0.237516\n",
      "[6000]\tvalid_0's l1: 0.235182\n",
      "[6200]\tvalid_0's l1: 0.233241\n",
      "[6400]\tvalid_0's l1: 0.231462\n",
      "[6600]\tvalid_0's l1: 0.230035\n",
      "[6800]\tvalid_0's l1: 0.228758\n",
      "[7000]\tvalid_0's l1: 0.227575\n",
      "[7200]\tvalid_0's l1: 0.22644\n",
      "[7400]\tvalid_0's l1: 0.225653\n",
      "[7600]\tvalid_0's l1: 0.22534\n",
      "[7800]\tvalid_0's l1: 0.224805\n",
      "[8000]\tvalid_0's l1: 0.22394\n",
      "[8200]\tvalid_0's l1: 0.222799\n",
      "[8400]\tvalid_0's l1: 0.221677\n",
      "[8600]\tvalid_0's l1: 0.220849\n",
      "[8800]\tvalid_0's l1: 0.220192\n",
      "[9000]\tvalid_0's l1: 0.219516\n",
      "[9200]\tvalid_0's l1: 0.218951\n",
      "[9400]\tvalid_0's l1: 0.218421\n",
      "[9600]\tvalid_0's l1: 0.217975\n",
      "[9800]\tvalid_0's l1: 0.217632\n",
      "[10000]\tvalid_0's l1: 0.217297\n",
      "[10200]\tvalid_0's l1: 0.217016\n",
      "[10400]\tvalid_0's l1: 0.216765\n",
      "[10600]\tvalid_0's l1: 0.216529\n",
      "[10800]\tvalid_0's l1: 0.216325\n",
      "[11000]\tvalid_0's l1: 0.215569\n",
      "[11200]\tvalid_0's l1: 0.214903\n",
      "[11400]\tvalid_0's l1: 0.214317\n",
      "[11600]\tvalid_0's l1: 0.213936\n",
      "[11800]\tvalid_0's l1: 0.213403\n",
      "[12000]\tvalid_0's l1: 0.212775\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.212775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.683181\n",
      "[400]\tvalid_0's l1: 0.581694\n",
      "[600]\tvalid_0's l1: 0.504309\n",
      "[800]\tvalid_0's l1: 0.444869\n",
      "[1000]\tvalid_0's l1: 0.401235\n",
      "[1200]\tvalid_0's l1: 0.368625\n",
      "[1400]\tvalid_0's l1: 0.342456\n",
      "[1600]\tvalid_0's l1: 0.321385\n",
      "[1800]\tvalid_0's l1: 0.304442\n",
      "[2000]\tvalid_0's l1: 0.291138\n",
      "[2200]\tvalid_0's l1: 0.279738\n",
      "[2400]\tvalid_0's l1: 0.270412\n",
      "[2600]\tvalid_0's l1: 0.262487\n",
      "[2800]\tvalid_0's l1: 0.255881\n",
      "[3000]\tvalid_0's l1: 0.250221\n",
      "[3200]\tvalid_0's l1: 0.245437\n",
      "[3400]\tvalid_0's l1: 0.241595\n",
      "[3600]\tvalid_0's l1: 0.238057\n",
      "[3800]\tvalid_0's l1: 0.23511\n",
      "[4000]\tvalid_0's l1: 0.23251\n",
      "[4200]\tvalid_0's l1: 0.230204\n",
      "[4400]\tvalid_0's l1: 0.22773\n",
      "[4600]\tvalid_0's l1: 0.224423\n",
      "[4800]\tvalid_0's l1: 0.221732\n",
      "[5000]\tvalid_0's l1: 0.218832\n",
      "[5200]\tvalid_0's l1: 0.216394\n",
      "[5400]\tvalid_0's l1: 0.21454\n",
      "[5600]\tvalid_0's l1: 0.212994\n",
      "[5800]\tvalid_0's l1: 0.211708\n",
      "[6000]\tvalid_0's l1: 0.210464\n",
      "[6200]\tvalid_0's l1: 0.209489\n",
      "[6400]\tvalid_0's l1: 0.208103\n",
      "[6600]\tvalid_0's l1: 0.206582\n",
      "[6800]\tvalid_0's l1: 0.205144\n",
      "[7000]\tvalid_0's l1: 0.203966\n",
      "[7200]\tvalid_0's l1: 0.202699\n",
      "[7400]\tvalid_0's l1: 0.20171\n",
      "[7600]\tvalid_0's l1: 0.200962\n",
      "[7800]\tvalid_0's l1: 0.200272\n",
      "[8000]\tvalid_0's l1: 0.199619\n",
      "[8200]\tvalid_0's l1: 0.199061\n",
      "[8400]\tvalid_0's l1: 0.198598\n",
      "[8600]\tvalid_0's l1: 0.198241\n",
      "[8800]\tvalid_0's l1: 0.197843\n",
      "[9000]\tvalid_0's l1: 0.197499\n",
      "[9200]\tvalid_0's l1: 0.196804\n",
      "[9400]\tvalid_0's l1: 0.195985\n",
      "[9600]\tvalid_0's l1: 0.195229\n",
      "[9800]\tvalid_0's l1: 0.19444\n",
      "[10000]\tvalid_0's l1: 0.194056\n",
      "[10200]\tvalid_0's l1: 0.193639\n",
      "[10400]\tvalid_0's l1: 0.193081\n",
      "[10600]\tvalid_0's l1: 0.192487\n",
      "[10800]\tvalid_0's l1: 0.191977\n",
      "[11000]\tvalid_0's l1: 0.191529\n",
      "[11200]\tvalid_0's l1: 0.191186\n",
      "[11400]\tvalid_0's l1: 0.190884\n",
      "[11600]\tvalid_0's l1: 0.190546\n",
      "[11800]\tvalid_0's l1: 0.190232\n",
      "[12000]\tvalid_0's l1: 0.18987\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.18987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 1.3618 (weight: 0.001)\n",
      "Random Forest MAPE: 1.9416 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.8005 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 1.6468 (weight: 0.000)\n",
      "Ridge MAPE: 0.8145 (weight: 0.273)\n",
      "Elastic Net MAPE: 0.7168 (weight: 0.726)\n",
      "Huber MAPE: 1.8409 (weight: 0.000)\n",
      "Ensemble MAPE: 0.7374\n",
      "\n",
      "Training for BlendProperty9...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.689085\n",
      "[400]\tvalid_0's l1: 0.589239\n",
      "[600]\tvalid_0's l1: 0.513146\n",
      "[800]\tvalid_0's l1: 0.457975\n",
      "[1000]\tvalid_0's l1: 0.418915\n",
      "[1200]\tvalid_0's l1: 0.390011\n",
      "[1400]\tvalid_0's l1: 0.36749\n",
      "[1600]\tvalid_0's l1: 0.349758\n",
      "[1800]\tvalid_0's l1: 0.334967\n",
      "[2000]\tvalid_0's l1: 0.322878\n",
      "[2200]\tvalid_0's l1: 0.312673\n",
      "[2400]\tvalid_0's l1: 0.304101\n",
      "[2600]\tvalid_0's l1: 0.297143\n",
      "[2800]\tvalid_0's l1: 0.2913\n",
      "[3000]\tvalid_0's l1: 0.286514\n",
      "[3200]\tvalid_0's l1: 0.282799\n",
      "[3400]\tvalid_0's l1: 0.280086\n",
      "[3600]\tvalid_0's l1: 0.277677\n",
      "[3800]\tvalid_0's l1: 0.275772\n",
      "[4000]\tvalid_0's l1: 0.271706\n",
      "[4200]\tvalid_0's l1: 0.266343\n",
      "[4400]\tvalid_0's l1: 0.261458\n",
      "[4600]\tvalid_0's l1: 0.257473\n",
      "[4800]\tvalid_0's l1: 0.254005\n",
      "[5000]\tvalid_0's l1: 0.251251\n",
      "[5200]\tvalid_0's l1: 0.248914\n",
      "[5400]\tvalid_0's l1: 0.248208\n",
      "[5600]\tvalid_0's l1: 0.247422\n",
      "[5800]\tvalid_0's l1: 0.246613\n",
      "[6000]\tvalid_0's l1: 0.245505\n",
      "[6200]\tvalid_0's l1: 0.244583\n",
      "[6400]\tvalid_0's l1: 0.243956\n",
      "[6600]\tvalid_0's l1: 0.243368\n",
      "[6800]\tvalid_0's l1: 0.242869\n",
      "[7000]\tvalid_0's l1: 0.242389\n",
      "[7200]\tvalid_0's l1: 0.241768\n",
      "[7400]\tvalid_0's l1: 0.241303\n",
      "[7600]\tvalid_0's l1: 0.240847\n",
      "[7800]\tvalid_0's l1: 0.240386\n",
      "[8000]\tvalid_0's l1: 0.239555\n",
      "[8200]\tvalid_0's l1: 0.238567\n",
      "[8400]\tvalid_0's l1: 0.237339\n",
      "[8600]\tvalid_0's l1: 0.236196\n",
      "[8800]\tvalid_0's l1: 0.235062\n",
      "[9000]\tvalid_0's l1: 0.234114\n",
      "[9200]\tvalid_0's l1: 0.233089\n",
      "[9400]\tvalid_0's l1: 0.231985\n",
      "[9600]\tvalid_0's l1: 0.23108\n",
      "[9800]\tvalid_0's l1: 0.230245\n",
      "[10000]\tvalid_0's l1: 0.229323\n",
      "[10200]\tvalid_0's l1: 0.228456\n",
      "[10400]\tvalid_0's l1: 0.2277\n",
      "[10600]\tvalid_0's l1: 0.226957\n",
      "[10800]\tvalid_0's l1: 0.226193\n",
      "[11000]\tvalid_0's l1: 0.225971\n",
      "[11200]\tvalid_0's l1: 0.225936\n",
      "Early stopping, best iteration is:\n",
      "[11164]\tvalid_0's l1: 0.225932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.700115\n",
      "[400]\tvalid_0's l1: 0.604847\n",
      "[600]\tvalid_0's l1: 0.532577\n",
      "[800]\tvalid_0's l1: 0.476191\n",
      "[1000]\tvalid_0's l1: 0.432016\n",
      "[1200]\tvalid_0's l1: 0.39728\n",
      "[1400]\tvalid_0's l1: 0.370158\n",
      "[1600]\tvalid_0's l1: 0.348646\n",
      "[1800]\tvalid_0's l1: 0.331621\n",
      "[2000]\tvalid_0's l1: 0.317634\n",
      "[2200]\tvalid_0's l1: 0.305756\n",
      "[2400]\tvalid_0's l1: 0.295981\n",
      "[2600]\tvalid_0's l1: 0.287402\n",
      "[2800]\tvalid_0's l1: 0.280382\n",
      "[3000]\tvalid_0's l1: 0.274188\n",
      "[3200]\tvalid_0's l1: 0.269346\n",
      "[3400]\tvalid_0's l1: 0.265145\n",
      "[3600]\tvalid_0's l1: 0.261582\n",
      "[3800]\tvalid_0's l1: 0.258969\n",
      "[4000]\tvalid_0's l1: 0.256076\n",
      "[4200]\tvalid_0's l1: 0.252336\n",
      "[4400]\tvalid_0's l1: 0.248661\n",
      "[4600]\tvalid_0's l1: 0.245446\n",
      "[4800]\tvalid_0's l1: 0.243031\n",
      "[5000]\tvalid_0's l1: 0.241109\n",
      "[5200]\tvalid_0's l1: 0.239916\n",
      "[5400]\tvalid_0's l1: 0.238744\n",
      "[5600]\tvalid_0's l1: 0.237783\n",
      "[5800]\tvalid_0's l1: 0.236608\n",
      "[6000]\tvalid_0's l1: 0.235503\n",
      "[6200]\tvalid_0's l1: 0.234192\n",
      "[6400]\tvalid_0's l1: 0.233174\n",
      "[6600]\tvalid_0's l1: 0.232234\n",
      "[6800]\tvalid_0's l1: 0.23127\n",
      "[7000]\tvalid_0's l1: 0.230563\n",
      "[7200]\tvalid_0's l1: 0.229889\n",
      "[7400]\tvalid_0's l1: 0.229296\n",
      "[7600]\tvalid_0's l1: 0.228349\n",
      "[7800]\tvalid_0's l1: 0.227115\n",
      "[8000]\tvalid_0's l1: 0.226082\n",
      "[8200]\tvalid_0's l1: 0.224886\n",
      "[8400]\tvalid_0's l1: 0.223499\n",
      "[8600]\tvalid_0's l1: 0.222361\n",
      "[8800]\tvalid_0's l1: 0.221377\n",
      "[9000]\tvalid_0's l1: 0.220593\n",
      "[9200]\tvalid_0's l1: 0.219806\n",
      "[9400]\tvalid_0's l1: 0.219231\n",
      "[9600]\tvalid_0's l1: 0.218582\n",
      "[9800]\tvalid_0's l1: 0.218034\n",
      "[10000]\tvalid_0's l1: 0.217557\n",
      "[10200]\tvalid_0's l1: 0.217143\n",
      "[10400]\tvalid_0's l1: 0.216799\n",
      "[10600]\tvalid_0's l1: 0.216697\n",
      "[10800]\tvalid_0's l1: 0.21667\n",
      "[11000]\tvalid_0's l1: 0.21663\n",
      "[11200]\tvalid_0's l1: 0.216461\n",
      "[11400]\tvalid_0's l1: 0.216345\n",
      "[11600]\tvalid_0's l1: 0.216167\n",
      "[11800]\tvalid_0's l1: 0.216039\n",
      "[12000]\tvalid_0's l1: 0.215845\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.215845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.662247\n",
      "[400]\tvalid_0's l1: 0.561333\n",
      "[600]\tvalid_0's l1: 0.486603\n",
      "[800]\tvalid_0's l1: 0.430875\n",
      "[1000]\tvalid_0's l1: 0.389607\n",
      "[1200]\tvalid_0's l1: 0.357301\n",
      "[1400]\tvalid_0's l1: 0.333113\n",
      "[1600]\tvalid_0's l1: 0.314391\n",
      "[1800]\tvalid_0's l1: 0.300379\n",
      "[2000]\tvalid_0's l1: 0.289312\n",
      "[2200]\tvalid_0's l1: 0.279824\n",
      "[2400]\tvalid_0's l1: 0.271889\n",
      "[2600]\tvalid_0's l1: 0.265098\n",
      "[2800]\tvalid_0's l1: 0.259373\n",
      "[3000]\tvalid_0's l1: 0.254271\n",
      "[3200]\tvalid_0's l1: 0.250375\n",
      "[3400]\tvalid_0's l1: 0.246536\n",
      "[3600]\tvalid_0's l1: 0.244415\n",
      "[3800]\tvalid_0's l1: 0.242948\n",
      "[4000]\tvalid_0's l1: 0.241845\n",
      "[4200]\tvalid_0's l1: 0.240736\n",
      "[4400]\tvalid_0's l1: 0.23959\n",
      "[4600]\tvalid_0's l1: 0.237196\n",
      "[4800]\tvalid_0's l1: 0.23463\n",
      "[5000]\tvalid_0's l1: 0.232092\n",
      "[5200]\tvalid_0's l1: 0.229532\n",
      "[5400]\tvalid_0's l1: 0.227641\n",
      "[5600]\tvalid_0's l1: 0.22645\n",
      "[5800]\tvalid_0's l1: 0.225973\n",
      "[6000]\tvalid_0's l1: 0.225503\n",
      "[6200]\tvalid_0's l1: 0.224962\n",
      "[6400]\tvalid_0's l1: 0.224494\n",
      "[6600]\tvalid_0's l1: 0.224085\n",
      "[6800]\tvalid_0's l1: 0.223803\n",
      "[7000]\tvalid_0's l1: 0.22354\n",
      "[7200]\tvalid_0's l1: 0.22321\n",
      "[7400]\tvalid_0's l1: 0.222651\n",
      "[7600]\tvalid_0's l1: 0.222106\n",
      "[7800]\tvalid_0's l1: 0.221282\n",
      "[8000]\tvalid_0's l1: 0.220469\n",
      "[8200]\tvalid_0's l1: 0.219642\n",
      "[8400]\tvalid_0's l1: 0.219042\n",
      "[8600]\tvalid_0's l1: 0.218552\n",
      "[8800]\tvalid_0's l1: 0.218139\n",
      "[9000]\tvalid_0's l1: 0.217512\n",
      "[9200]\tvalid_0's l1: 0.216977\n",
      "[9400]\tvalid_0's l1: 0.216214\n",
      "[9600]\tvalid_0's l1: 0.21561\n",
      "[9800]\tvalid_0's l1: 0.215205\n",
      "[10000]\tvalid_0's l1: 0.214875\n",
      "[10200]\tvalid_0's l1: 0.214624\n",
      "[10400]\tvalid_0's l1: 0.214275\n",
      "[10600]\tvalid_0's l1: 0.213973\n",
      "[10800]\tvalid_0's l1: 0.213664\n",
      "[11000]\tvalid_0's l1: 0.213506\n",
      "[11200]\tvalid_0's l1: 0.213435\n",
      "Early stopping, best iteration is:\n",
      "[11175]\tvalid_0's l1: 0.213429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.640313\n",
      "[400]\tvalid_0's l1: 0.539897\n",
      "[600]\tvalid_0's l1: 0.468107\n",
      "[800]\tvalid_0's l1: 0.412837\n",
      "[1000]\tvalid_0's l1: 0.373263\n",
      "[1200]\tvalid_0's l1: 0.342251\n",
      "[1400]\tvalid_0's l1: 0.318694\n",
      "[1600]\tvalid_0's l1: 0.300933\n",
      "[1800]\tvalid_0's l1: 0.287068\n",
      "[2000]\tvalid_0's l1: 0.275683\n",
      "[2200]\tvalid_0's l1: 0.266745\n",
      "[2400]\tvalid_0's l1: 0.258254\n",
      "[2600]\tvalid_0's l1: 0.251017\n",
      "[2800]\tvalid_0's l1: 0.245189\n",
      "[3000]\tvalid_0's l1: 0.240396\n",
      "[3200]\tvalid_0's l1: 0.236684\n",
      "[3400]\tvalid_0's l1: 0.233018\n",
      "[3600]\tvalid_0's l1: 0.229398\n",
      "[3800]\tvalid_0's l1: 0.226477\n",
      "[4000]\tvalid_0's l1: 0.223857\n",
      "[4200]\tvalid_0's l1: 0.221732\n",
      "[4400]\tvalid_0's l1: 0.21994\n",
      "[4600]\tvalid_0's l1: 0.218288\n",
      "[4800]\tvalid_0's l1: 0.216907\n",
      "[5000]\tvalid_0's l1: 0.214637\n",
      "[5200]\tvalid_0's l1: 0.212156\n",
      "[5400]\tvalid_0's l1: 0.209926\n",
      "[5600]\tvalid_0's l1: 0.207872\n",
      "[5800]\tvalid_0's l1: 0.20678\n",
      "[6000]\tvalid_0's l1: 0.205627\n",
      "[6200]\tvalid_0's l1: 0.204659\n",
      "[6400]\tvalid_0's l1: 0.20377\n",
      "[6600]\tvalid_0's l1: 0.20291\n",
      "[6800]\tvalid_0's l1: 0.201599\n",
      "[7000]\tvalid_0's l1: 0.200359\n",
      "[7200]\tvalid_0's l1: 0.199521\n",
      "[7400]\tvalid_0's l1: 0.199073\n",
      "[7600]\tvalid_0's l1: 0.19821\n",
      "[7800]\tvalid_0's l1: 0.197382\n",
      "[8000]\tvalid_0's l1: 0.196754\n",
      "[8200]\tvalid_0's l1: 0.196015\n",
      "[8400]\tvalid_0's l1: 0.195243\n",
      "[8600]\tvalid_0's l1: 0.194535\n",
      "[8800]\tvalid_0's l1: 0.194041\n",
      "[9000]\tvalid_0's l1: 0.193596\n",
      "[9200]\tvalid_0's l1: 0.193228\n",
      "[9400]\tvalid_0's l1: 0.192918\n",
      "[9600]\tvalid_0's l1: 0.192642\n",
      "[9800]\tvalid_0's l1: 0.192263\n",
      "[10000]\tvalid_0's l1: 0.192006\n",
      "[10200]\tvalid_0's l1: 0.1918\n",
      "[10400]\tvalid_0's l1: 0.191588\n",
      "[10600]\tvalid_0's l1: 0.191426\n",
      "[10800]\tvalid_0's l1: 0.191198\n",
      "[11000]\tvalid_0's l1: 0.190792\n",
      "[11200]\tvalid_0's l1: 0.190459\n",
      "[11400]\tvalid_0's l1: 0.190165\n",
      "[11600]\tvalid_0's l1: 0.189868\n",
      "[11800]\tvalid_0's l1: 0.189543\n",
      "[12000]\tvalid_0's l1: 0.189325\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.189325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.675909\n",
      "[400]\tvalid_0's l1: 0.580201\n",
      "[600]\tvalid_0's l1: 0.508759\n",
      "[800]\tvalid_0's l1: 0.453433\n",
      "[1000]\tvalid_0's l1: 0.413886\n",
      "[1200]\tvalid_0's l1: 0.380985\n",
      "[1400]\tvalid_0's l1: 0.355235\n",
      "[1600]\tvalid_0's l1: 0.334187\n",
      "[1800]\tvalid_0's l1: 0.317434\n",
      "[2000]\tvalid_0's l1: 0.303385\n",
      "[2200]\tvalid_0's l1: 0.292332\n",
      "[2400]\tvalid_0's l1: 0.282756\n",
      "[2600]\tvalid_0's l1: 0.274537\n",
      "[2800]\tvalid_0's l1: 0.26789\n",
      "[3000]\tvalid_0's l1: 0.262611\n",
      "[3200]\tvalid_0's l1: 0.257719\n",
      "[3400]\tvalid_0's l1: 0.253271\n",
      "[3600]\tvalid_0's l1: 0.249522\n",
      "[3800]\tvalid_0's l1: 0.246265\n",
      "[4000]\tvalid_0's l1: 0.243773\n",
      "[4200]\tvalid_0's l1: 0.241541\n",
      "[4400]\tvalid_0's l1: 0.238912\n",
      "[4600]\tvalid_0's l1: 0.236397\n",
      "[4800]\tvalid_0's l1: 0.23471\n",
      "[5000]\tvalid_0's l1: 0.232833\n",
      "[5200]\tvalid_0's l1: 0.23118\n",
      "[5400]\tvalid_0's l1: 0.230344\n",
      "[5600]\tvalid_0's l1: 0.229295\n",
      "[5800]\tvalid_0's l1: 0.227855\n",
      "[6000]\tvalid_0's l1: 0.226293\n",
      "[6200]\tvalid_0's l1: 0.225031\n",
      "[6400]\tvalid_0's l1: 0.224146\n",
      "[6600]\tvalid_0's l1: 0.223181\n",
      "[6800]\tvalid_0's l1: 0.222524\n",
      "[7000]\tvalid_0's l1: 0.221922\n",
      "[7200]\tvalid_0's l1: 0.221296\n",
      "[7400]\tvalid_0's l1: 0.220835\n",
      "[7600]\tvalid_0's l1: 0.220407\n",
      "[7800]\tvalid_0's l1: 0.219934\n",
      "[8000]\tvalid_0's l1: 0.219249\n",
      "[8200]\tvalid_0's l1: 0.218332\n",
      "[8400]\tvalid_0's l1: 0.21759\n",
      "[8600]\tvalid_0's l1: 0.216595\n",
      "[8800]\tvalid_0's l1: 0.215891\n",
      "[9000]\tvalid_0's l1: 0.215159\n",
      "[9200]\tvalid_0's l1: 0.214446\n",
      "[9400]\tvalid_0's l1: 0.213896\n",
      "[9600]\tvalid_0's l1: 0.213507\n",
      "[9800]\tvalid_0's l1: 0.212924\n",
      "[10000]\tvalid_0's l1: 0.212482\n",
      "[10200]\tvalid_0's l1: 0.212194\n",
      "[10400]\tvalid_0's l1: 0.211953\n",
      "[10600]\tvalid_0's l1: 0.211817\n",
      "[10800]\tvalid_0's l1: 0.211642\n",
      "[11000]\tvalid_0's l1: 0.211547\n",
      "[11200]\tvalid_0's l1: 0.211432\n",
      "[11400]\tvalid_0's l1: 0.21133\n",
      "[11600]\tvalid_0's l1: 0.211051\n",
      "[11800]\tvalid_0's l1: 0.210953\n",
      "[12000]\tvalid_0's l1: 0.210803\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.210803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MAPE: 0.9990 (weight: 0.991)\n",
      "Random Forest MAPE: 1.9522 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.4699 (weight: 0.009)\n",
      "Gradient Boosting MAPE: 1.9927 (weight: 0.000)\n",
      "Ridge MAPE: 1.9583 (weight: 0.000)\n",
      "Elastic Net MAPE: 2.2114 (weight: 0.000)\n",
      "Huber MAPE: 1.8533 (weight: 0.000)\n",
      "Ensemble MAPE: 0.9952\n",
      "\n",
      "Training for BlendProperty10...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.677219\n",
      "[400]\tvalid_0's l1: 0.561473\n",
      "[600]\tvalid_0's l1: 0.474324\n",
      "[800]\tvalid_0's l1: 0.409344\n",
      "[1000]\tvalid_0's l1: 0.36365\n",
      "[1200]\tvalid_0's l1: 0.329616\n",
      "[1400]\tvalid_0's l1: 0.302537\n",
      "[1600]\tvalid_0's l1: 0.281484\n",
      "[1800]\tvalid_0's l1: 0.264699\n",
      "[2000]\tvalid_0's l1: 0.25113\n",
      "[2200]\tvalid_0's l1: 0.239702\n",
      "[2400]\tvalid_0's l1: 0.230561\n",
      "[2600]\tvalid_0's l1: 0.223428\n",
      "[2800]\tvalid_0's l1: 0.217141\n",
      "[3000]\tvalid_0's l1: 0.211803\n",
      "[3200]\tvalid_0's l1: 0.207595\n",
      "[3400]\tvalid_0's l1: 0.203956\n",
      "[3600]\tvalid_0's l1: 0.200799\n",
      "[3800]\tvalid_0's l1: 0.197697\n",
      "[4000]\tvalid_0's l1: 0.195031\n",
      "[4200]\tvalid_0's l1: 0.192444\n",
      "[4400]\tvalid_0's l1: 0.190378\n",
      "[4600]\tvalid_0's l1: 0.188555\n",
      "[4800]\tvalid_0's l1: 0.18655\n",
      "[5000]\tvalid_0's l1: 0.18337\n",
      "[5200]\tvalid_0's l1: 0.181276\n",
      "[5400]\tvalid_0's l1: 0.179531\n",
      "[5600]\tvalid_0's l1: 0.178036\n",
      "[5800]\tvalid_0's l1: 0.176699\n",
      "[6000]\tvalid_0's l1: 0.175779\n",
      "[6200]\tvalid_0's l1: 0.174242\n",
      "[6400]\tvalid_0's l1: 0.172404\n",
      "[6600]\tvalid_0's l1: 0.171376\n",
      "[6800]\tvalid_0's l1: 0.170511\n",
      "[7000]\tvalid_0's l1: 0.16952\n",
      "[7200]\tvalid_0's l1: 0.168927\n",
      "[7400]\tvalid_0's l1: 0.16837\n",
      "[7600]\tvalid_0's l1: 0.167817\n",
      "[7800]\tvalid_0's l1: 0.167355\n",
      "[8000]\tvalid_0's l1: 0.166976\n",
      "[8200]\tvalid_0's l1: 0.166673\n",
      "[8400]\tvalid_0's l1: 0.166327\n",
      "[8600]\tvalid_0's l1: 0.166161\n",
      "[8800]\tvalid_0's l1: 0.166009\n",
      "[9000]\tvalid_0's l1: 0.165549\n",
      "[9200]\tvalid_0's l1: 0.164751\n",
      "[9400]\tvalid_0's l1: 0.163997\n",
      "[9600]\tvalid_0's l1: 0.163137\n",
      "[9800]\tvalid_0's l1: 0.162438\n",
      "[10000]\tvalid_0's l1: 0.161706\n",
      "[10200]\tvalid_0's l1: 0.161184\n",
      "[10400]\tvalid_0's l1: 0.160625\n",
      "[10600]\tvalid_0's l1: 0.160144\n",
      "[10800]\tvalid_0's l1: 0.159975\n",
      "[11000]\tvalid_0's l1: 0.159914\n",
      "[11200]\tvalid_0's l1: 0.159853\n",
      "[11400]\tvalid_0's l1: 0.159668\n",
      "[11600]\tvalid_0's l1: 0.159571\n",
      "[11800]\tvalid_0's l1: 0.159436\n",
      "[12000]\tvalid_0's l1: 0.159286\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.159286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.655121\n",
      "[400]\tvalid_0's l1: 0.546537\n",
      "[600]\tvalid_0's l1: 0.461492\n",
      "[800]\tvalid_0's l1: 0.397903\n",
      "[1000]\tvalid_0's l1: 0.352474\n",
      "[1200]\tvalid_0's l1: 0.317112\n",
      "[1400]\tvalid_0's l1: 0.288005\n",
      "[1600]\tvalid_0's l1: 0.265649\n",
      "[1800]\tvalid_0's l1: 0.248182\n",
      "[2000]\tvalid_0's l1: 0.234294\n",
      "[2200]\tvalid_0's l1: 0.222691\n",
      "[2400]\tvalid_0's l1: 0.213556\n",
      "[2600]\tvalid_0's l1: 0.206483\n",
      "[2800]\tvalid_0's l1: 0.200604\n",
      "[3000]\tvalid_0's l1: 0.195508\n",
      "[3200]\tvalid_0's l1: 0.191253\n",
      "[3400]\tvalid_0's l1: 0.18774\n",
      "[3600]\tvalid_0's l1: 0.184678\n",
      "[3800]\tvalid_0's l1: 0.181843\n",
      "[4000]\tvalid_0's l1: 0.179647\n",
      "[4200]\tvalid_0's l1: 0.177916\n",
      "[4400]\tvalid_0's l1: 0.176242\n",
      "[4600]\tvalid_0's l1: 0.174674\n",
      "[4800]\tvalid_0's l1: 0.173332\n",
      "[5000]\tvalid_0's l1: 0.17226\n",
      "[5200]\tvalid_0's l1: 0.170104\n",
      "[5400]\tvalid_0's l1: 0.16759\n",
      "[5600]\tvalid_0's l1: 0.165735\n",
      "[5800]\tvalid_0's l1: 0.164272\n",
      "[6000]\tvalid_0's l1: 0.163036\n",
      "[6200]\tvalid_0's l1: 0.162061\n",
      "[6400]\tvalid_0's l1: 0.161659\n",
      "[6600]\tvalid_0's l1: 0.160881\n",
      "[6800]\tvalid_0's l1: 0.159905\n",
      "[7000]\tvalid_0's l1: 0.159141\n",
      "[7200]\tvalid_0's l1: 0.15837\n",
      "[7400]\tvalid_0's l1: 0.157715\n",
      "[7600]\tvalid_0's l1: 0.157098\n",
      "[7800]\tvalid_0's l1: 0.1565\n",
      "[8000]\tvalid_0's l1: 0.156099\n",
      "[8200]\tvalid_0's l1: 0.155746\n",
      "[8400]\tvalid_0's l1: 0.155449\n",
      "[8600]\tvalid_0's l1: 0.154982\n",
      "[8800]\tvalid_0's l1: 0.154177\n",
      "[9000]\tvalid_0's l1: 0.153257\n",
      "[9200]\tvalid_0's l1: 0.152448\n",
      "[9400]\tvalid_0's l1: 0.151628\n",
      "[9600]\tvalid_0's l1: 0.151117\n",
      "[9800]\tvalid_0's l1: 0.150528\n",
      "[10000]\tvalid_0's l1: 0.149992\n",
      "[10200]\tvalid_0's l1: 0.149448\n",
      "[10400]\tvalid_0's l1: 0.14903\n",
      "[10600]\tvalid_0's l1: 0.148596\n",
      "[10800]\tvalid_0's l1: 0.148143\n",
      "[11000]\tvalid_0's l1: 0.147812\n",
      "[11200]\tvalid_0's l1: 0.147573\n",
      "[11400]\tvalid_0's l1: 0.147353\n",
      "[11600]\tvalid_0's l1: 0.147096\n",
      "[11800]\tvalid_0's l1: 0.146786\n",
      "[12000]\tvalid_0's l1: 0.146562\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.146562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.62454\n",
      "[400]\tvalid_0's l1: 0.509581\n",
      "[600]\tvalid_0's l1: 0.4268\n",
      "[800]\tvalid_0's l1: 0.367313\n",
      "[1000]\tvalid_0's l1: 0.326378\n",
      "[1200]\tvalid_0's l1: 0.294806\n",
      "[1400]\tvalid_0's l1: 0.270286\n",
      "[1600]\tvalid_0's l1: 0.251339\n",
      "[1800]\tvalid_0's l1: 0.236209\n",
      "[2000]\tvalid_0's l1: 0.223967\n",
      "[2200]\tvalid_0's l1: 0.213231\n",
      "[2400]\tvalid_0's l1: 0.204624\n",
      "[2600]\tvalid_0's l1: 0.197645\n",
      "[2800]\tvalid_0's l1: 0.191668\n",
      "[3000]\tvalid_0's l1: 0.186671\n",
      "[3200]\tvalid_0's l1: 0.182564\n",
      "[3400]\tvalid_0's l1: 0.179035\n",
      "[3600]\tvalid_0's l1: 0.176076\n",
      "[3800]\tvalid_0's l1: 0.17351\n",
      "[4000]\tvalid_0's l1: 0.17085\n",
      "[4200]\tvalid_0's l1: 0.168622\n",
      "[4400]\tvalid_0's l1: 0.166918\n",
      "[4600]\tvalid_0's l1: 0.165488\n",
      "[4800]\tvalid_0's l1: 0.164004\n",
      "[5000]\tvalid_0's l1: 0.162669\n",
      "[5200]\tvalid_0's l1: 0.161222\n",
      "[5400]\tvalid_0's l1: 0.160074\n",
      "[5600]\tvalid_0's l1: 0.158998\n",
      "[5800]\tvalid_0's l1: 0.15801\n",
      "[6000]\tvalid_0's l1: 0.15708\n",
      "[6200]\tvalid_0's l1: 0.156286\n",
      "[6400]\tvalid_0's l1: 0.155771\n",
      "[6600]\tvalid_0's l1: 0.154631\n",
      "[6800]\tvalid_0's l1: 0.153631\n",
      "[7000]\tvalid_0's l1: 0.152693\n",
      "[7200]\tvalid_0's l1: 0.152026\n",
      "[7400]\tvalid_0's l1: 0.151425\n",
      "[7600]\tvalid_0's l1: 0.150845\n",
      "[7800]\tvalid_0's l1: 0.150247\n",
      "[8000]\tvalid_0's l1: 0.149785\n",
      "[8200]\tvalid_0's l1: 0.149415\n",
      "[8400]\tvalid_0's l1: 0.149046\n",
      "[8600]\tvalid_0's l1: 0.148637\n",
      "[8800]\tvalid_0's l1: 0.148287\n",
      "[9000]\tvalid_0's l1: 0.148106\n",
      "[9200]\tvalid_0's l1: 0.14802\n",
      "[9400]\tvalid_0's l1: 0.147896\n",
      "[9600]\tvalid_0's l1: 0.147577\n",
      "[9800]\tvalid_0's l1: 0.146848\n",
      "[10000]\tvalid_0's l1: 0.145916\n",
      "[10200]\tvalid_0's l1: 0.145247\n",
      "[10400]\tvalid_0's l1: 0.144712\n",
      "[10600]\tvalid_0's l1: 0.144279\n",
      "[10800]\tvalid_0's l1: 0.143744\n",
      "[11000]\tvalid_0's l1: 0.143369\n",
      "[11200]\tvalid_0's l1: 0.143104\n",
      "[11400]\tvalid_0's l1: 0.142723\n",
      "[11600]\tvalid_0's l1: 0.142461\n",
      "[11800]\tvalid_0's l1: 0.142244\n",
      "[12000]\tvalid_0's l1: 0.142086\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.142086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.649794\n",
      "[400]\tvalid_0's l1: 0.537043\n",
      "[600]\tvalid_0's l1: 0.452215\n",
      "[800]\tvalid_0's l1: 0.388429\n",
      "[1000]\tvalid_0's l1: 0.340973\n",
      "[1200]\tvalid_0's l1: 0.305124\n",
      "[1400]\tvalid_0's l1: 0.277213\n",
      "[1600]\tvalid_0's l1: 0.255683\n",
      "[1800]\tvalid_0's l1: 0.238048\n",
      "[2000]\tvalid_0's l1: 0.223482\n",
      "[2200]\tvalid_0's l1: 0.211581\n",
      "[2400]\tvalid_0's l1: 0.201559\n",
      "[2600]\tvalid_0's l1: 0.193432\n",
      "[2800]\tvalid_0's l1: 0.18705\n",
      "[3000]\tvalid_0's l1: 0.181866\n",
      "[3200]\tvalid_0's l1: 0.177519\n",
      "[3400]\tvalid_0's l1: 0.173907\n",
      "[3600]\tvalid_0's l1: 0.170605\n",
      "[3800]\tvalid_0's l1: 0.167774\n",
      "[4000]\tvalid_0's l1: 0.165794\n",
      "[4200]\tvalid_0's l1: 0.163952\n",
      "[4400]\tvalid_0's l1: 0.162378\n",
      "[4600]\tvalid_0's l1: 0.16038\n",
      "[4800]\tvalid_0's l1: 0.157629\n",
      "[5000]\tvalid_0's l1: 0.155334\n",
      "[5200]\tvalid_0's l1: 0.153407\n",
      "[5400]\tvalid_0's l1: 0.151904\n",
      "[5600]\tvalid_0's l1: 0.150661\n",
      "[5800]\tvalid_0's l1: 0.149798\n",
      "[6000]\tvalid_0's l1: 0.149391\n",
      "[6200]\tvalid_0's l1: 0.148655\n",
      "[6400]\tvalid_0's l1: 0.147834\n",
      "[6600]\tvalid_0's l1: 0.147069\n",
      "[6800]\tvalid_0's l1: 0.146705\n",
      "[7000]\tvalid_0's l1: 0.146195\n",
      "[7200]\tvalid_0's l1: 0.145688\n",
      "[7400]\tvalid_0's l1: 0.145299\n",
      "[7600]\tvalid_0's l1: 0.14499\n",
      "[7800]\tvalid_0's l1: 0.144707\n",
      "[8000]\tvalid_0's l1: 0.144507\n",
      "[8200]\tvalid_0's l1: 0.14421\n",
      "[8400]\tvalid_0's l1: 0.144106\n",
      "[8600]\tvalid_0's l1: 0.143782\n",
      "[8800]\tvalid_0's l1: 0.143077\n",
      "[9000]\tvalid_0's l1: 0.142426\n",
      "[9200]\tvalid_0's l1: 0.141684\n",
      "[9400]\tvalid_0's l1: 0.141022\n",
      "[9600]\tvalid_0's l1: 0.14024\n",
      "[9800]\tvalid_0's l1: 0.139749\n",
      "[10000]\tvalid_0's l1: 0.139189\n",
      "[10200]\tvalid_0's l1: 0.138744\n",
      "[10400]\tvalid_0's l1: 0.138527\n",
      "[10600]\tvalid_0's l1: 0.138225\n",
      "[10800]\tvalid_0's l1: 0.137982\n",
      "[11000]\tvalid_0's l1: 0.13771\n",
      "[11200]\tvalid_0's l1: 0.137378\n",
      "[11400]\tvalid_0's l1: 0.137084\n",
      "[11600]\tvalid_0's l1: 0.13684\n",
      "[11800]\tvalid_0's l1: 0.136674\n",
      "[12000]\tvalid_0's l1: 0.136496\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11989]\tvalid_0's l1: 0.136495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\tvalid_0's l1: 0.679962\n",
      "[400]\tvalid_0's l1: 0.5699\n",
      "[600]\tvalid_0's l1: 0.485704\n",
      "[800]\tvalid_0's l1: 0.421537\n",
      "[1000]\tvalid_0's l1: 0.373856\n",
      "[1200]\tvalid_0's l1: 0.338054\n",
      "[1400]\tvalid_0's l1: 0.309879\n",
      "[1600]\tvalid_0's l1: 0.286716\n",
      "[1800]\tvalid_0's l1: 0.267424\n",
      "[2000]\tvalid_0's l1: 0.251963\n",
      "[2200]\tvalid_0's l1: 0.239712\n",
      "[2400]\tvalid_0's l1: 0.229936\n",
      "[2600]\tvalid_0's l1: 0.221885\n",
      "[2800]\tvalid_0's l1: 0.214788\n",
      "[3000]\tvalid_0's l1: 0.208992\n",
      "[3200]\tvalid_0's l1: 0.204049\n",
      "[3400]\tvalid_0's l1: 0.199841\n",
      "[3600]\tvalid_0's l1: 0.196187\n",
      "[3800]\tvalid_0's l1: 0.192944\n",
      "[4000]\tvalid_0's l1: 0.190412\n",
      "[4200]\tvalid_0's l1: 0.1878\n",
      "[4400]\tvalid_0's l1: 0.185421\n",
      "[4600]\tvalid_0's l1: 0.183098\n",
      "[4800]\tvalid_0's l1: 0.180818\n",
      "[5000]\tvalid_0's l1: 0.178856\n",
      "[5200]\tvalid_0's l1: 0.177203\n",
      "[5400]\tvalid_0's l1: 0.17609\n",
      "[5600]\tvalid_0's l1: 0.174358\n",
      "[5800]\tvalid_0's l1: 0.17233\n",
      "[6000]\tvalid_0's l1: 0.171016\n",
      "[6200]\tvalid_0's l1: 0.169774\n",
      "[6400]\tvalid_0's l1: 0.168687\n",
      "[6600]\tvalid_0's l1: 0.167489\n",
      "[6800]\tvalid_0's l1: 0.166531\n",
      "[7000]\tvalid_0's l1: 0.165771\n",
      "[7200]\tvalid_0's l1: 0.165193\n",
      "[7400]\tvalid_0's l1: 0.164736\n",
      "[7600]\tvalid_0's l1: 0.164155\n",
      "[7800]\tvalid_0's l1: 0.163675\n",
      "[8000]\tvalid_0's l1: 0.163503\n",
      "[8200]\tvalid_0's l1: 0.163066\n",
      "[8400]\tvalid_0's l1: 0.162276\n",
      "[8600]\tvalid_0's l1: 0.161438\n",
      "[8800]\tvalid_0's l1: 0.160613\n",
      "[9000]\tvalid_0's l1: 0.159762\n",
      "[9200]\tvalid_0's l1: 0.158916\n",
      "[9400]\tvalid_0's l1: 0.158334\n",
      "[9600]\tvalid_0's l1: 0.157954\n",
      "[9800]\tvalid_0's l1: 0.157523\n",
      "[10000]\tvalid_0's l1: 0.157183\n",
      "[10200]\tvalid_0's l1: 0.156811\n",
      "[10400]\tvalid_0's l1: 0.156552\n",
      "[10600]\tvalid_0's l1: 0.156399\n",
      "[10800]\tvalid_0's l1: 0.156256\n",
      "[11000]\tvalid_0's l1: 0.15607\n",
      "[11200]\tvalid_0's l1: 0.155737\n",
      "[11400]\tvalid_0's l1: 0.155436\n",
      "[11600]\tvalid_0's l1: 0.155056\n",
      "[11800]\tvalid_0's l1: 0.154603\n",
      "[12000]\tvalid_0's l1: 0.154364\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[12000]\tvalid_0's l1: 0.154364\n",
      "LightGBM MAPE: 0.7599 (weight: 0.001)\n",
      "Random Forest MAPE: 1.4213 (weight: 0.000)\n",
      "Extra Trees MAPE: 1.2811 (weight: 0.000)\n",
      "Gradient Boosting MAPE: 1.0214 (weight: 0.000)\n",
      "Ridge MAPE: 0.0806 (weight: 0.465)\n",
      "Elastic Net MAPE: 0.0669 (weight: 0.534)\n",
      "Huber MAPE: 2.1474 (weight: 0.000)\n",
      "Ensemble MAPE: 0.0718\n",
      "\n",
      "Submission file created: submission_breakthrough_Hyper.csv\n",
      "\n",
      "Breakthrough Ensemble Summary:\n",
      "Features: 433 (selected: 227)\n",
      "Cross-validation: 5-fold\n",
      "Models: LightGBM, Random Forest, Extra Trees, Gradient Boosting, Ridge, Elastic Net, Huber\n",
      "Ensemble: Exponential weighting based on validation performance\n",
      "Scaling: Robust and Standard scaling for different models\n",
      "Target: 90+ score with breakthrough features and advanced ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mithun/Desktop/kushvinth/.venv/lib/python3.13/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, SelectKBest, f_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor, Lasso, BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import skew, kurtosis, boxcox\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "#########FIX###########\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# n_splits = 5\n",
    "#######################\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Enhanced Oil Properties Feature Engineering\n",
    "def create_enhanced_oil_features(df, poly_features=None, power_transformer=None, fit_transformers=True):\n",
    "    \"\"\"\n",
    "    Create advanced features specific to oil blending and properties\n",
    "    \"\"\"\n",
    "    features = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    features += [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "\n",
    "    # Oil-specific domain knowledge features\n",
    "    print(\"Creating oil-specific features...\")\n",
    "    \n",
    "    # 1. Viscosity Index calculations (Properties 1-3 typically relate to viscosity)\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_viscosity_index'] = (\n",
    "            df[f'Component{i}_Property1'] * 0.4 + \n",
    "            df[f'Component{i}_Property2'] * 0.4 + \n",
    "            df[f'Component{i}_Property3'] * 0.2\n",
    "        )\n",
    "    \n",
    "    # 2. Density estimations (Properties 4-6)\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_density_est'] = (\n",
    "            df[f'Component{i}_Property4'] * 0.5 + \n",
    "            df[f'Component{i}_Property5'] * 0.3 + \n",
    "            df[f'Component{i}_Property6'] * 0.2\n",
    "        )\n",
    "    \n",
    "    # 3. Octane-like properties (Properties 7-10)\n",
    "    for i in range(1, 6):\n",
    "        df[f'Component{i}_octane_est'] = (\n",
    "            df[f'Component{i}_Property7'] * 0.3 + \n",
    "            df[f'Component{i}_Property8'] * 0.3 + \n",
    "            df[f'Component{i}_Property9'] * 0.2 + \n",
    "            df[f'Component{i}_Property10'] * 0.2\n",
    "        )\n",
    "\n",
    "    # Enhanced interaction features with non-linear transformations\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 11):\n",
    "            df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
    "            df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
    "            df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
    "            df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
    "            df[f'frac{i}_prop{j}_cube'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 3)\n",
    "            \n",
    "            # Oil-specific transformations\n",
    "            df[f'frac{i}_prop{j}_exp'] = df[f'Component{i}_fraction'] * np.exp(df[f'Component{i}_Property{j}'] / 100)\n",
    "            df[f'frac{i}_prop{j}_inv'] = df[f'Component{i}_fraction'] / (np.abs(df[f'Component{i}_Property{j}']) + 1e-8)\n",
    "            \n",
    "            features.extend([\n",
    "                f'frac{i}_prop{j}', f'frac{i}_prop{j}_sqrt', f'frac{i}_prop{j}_log', \n",
    "                f'frac{i}_prop{j}_square', f'frac{i}_prop{j}_cube', f'frac{i}_prop{j}_exp', f'frac{i}_prop{j}_inv'\n",
    "            ])\n",
    "\n",
    "    # Advanced weighted features with multiple aggregation methods\n",
    "    for j in range(1, 11):\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "\n",
    "        # Multiple weighted aggregations\n",
    "        df[f'weighted_mean_prop{j}'] = sum(\n",
    "            df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}'] for i in range(1, 6)\n",
    "        )\n",
    "        mean = df[f'weighted_mean_prop{j}']\n",
    "        df[f'weighted_var_prop{j}'] = sum(\n",
    "            df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] - mean) ** 2 for i in range(1, 6)\n",
    "        )\n",
    "\n",
    "        # Harmonic mean (important for fuel properties)\n",
    "        safe_props = [np.maximum(df[f'Component{i}_Property{j}'], 1e-6) for i in range(1, 6)]\n",
    "        harmonic_mean = sum(df[f'Component{i}_fraction'] / safe_props[i-1] for i in range(1, 6))\n",
    "        df[f'harmonic_mean_prop{j}'] = 1 / harmonic_mean\n",
    "\n",
    "        # Geometric mean (for multiplicative properties)\n",
    "        log_geo_mean = sum(df[f'Component{i}_fraction'] * np.log(safe_props[i-1]) for i in range(1, 6))\n",
    "        df[f'geometric_mean_prop{j}'] = np.exp(log_geo_mean)\n",
    "\n",
    "        # Component dominance with ranking\n",
    "        frac_array = np.array([df[f'Component{i}_fraction'] for i in range(1, 6)])\n",
    "        dominant_idx = np.argmax(frac_array, axis=0)\n",
    "        df[f'dominant_prop{j}'] = df.apply(lambda row:\n",
    "            row[f'Component{dominant_idx[row.name] + 1}_Property{j}'], axis=1)\n",
    "\n",
    "        # Blend balance and diversity\n",
    "        frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "        df[f'blend_balance_prop{j}'] = 1 - df[frac_cols].std(axis=1)\n",
    "        df[f'blend_diversity_prop{j}'] = df[frac_cols].std(axis=1) / (df[frac_cols].mean(axis=1) + 1e-8)\n",
    "\n",
    "        # Advanced statistics\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
    "        df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
    "        df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
    "        df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
    "        df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
    "        df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
    "        df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
    "        df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
    "        df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
    "\n",
    "        features.extend([\n",
    "            f'min_prop{j}', f'max_prop{j}', f'mean_prop{j}',\n",
    "            f'std_prop{j}', f'median_prop{j}', f'skew_prop{j}', f'kurtosis_prop{j}',\n",
    "            f'range_prop{j}', f'iqr_prop{j}'\n",
    "        ])\n",
    "\n",
    "    # Oil-specific advanced blending rules\n",
    "    for j in range(1, 11):\n",
    "        fractions = [df[f'Component{i}_fraction'] for i in range(1, 6)]\n",
    "        props = [df[f'Component{i}_Property{j}'] for i in range(1, 6)]\n",
    "        safe_props = [np.maximum(p, 1e-6) for p in props]\n",
    "\n",
    "        # RON-like blending (non-linear octane)\n",
    "        ron_blend = sum(f * (r ** 1.5) for f, r in zip(fractions, safe_props)) ** (1 / 1.5)\n",
    "        df[f'ron_like_blend_prop{j}'] = ron_blend\n",
    "\n",
    "        # Viscosity-like blending (logarithmic)\n",
    "        log_visc_blend = sum(f * np.log(r) for f, r in zip(fractions, safe_props))\n",
    "        df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
    "\n",
    "        # Density-like blending (linear but with corrections)\n",
    "        density_blend = sum(f * r for f, r in zip(fractions, safe_props))\n",
    "        df[f'density_blend_prop{j}'] = density_blend\n",
    "\n",
    "        # Reid vapor pressure-like (exponential)\n",
    "        rvp_blend = sum(f * np.exp(r / 100) for f, r in zip(fractions, safe_props))\n",
    "        df[f'rvp_blend_prop{j}'] = rvp_blend\n",
    "\n",
    "        # Cetane number-like blending\n",
    "        cetane_blend = sum(f * np.sqrt(r) for f, r in zip(fractions, safe_props))\n",
    "        df[f'cetane_like_blend_prop{j}'] = cetane_blend\n",
    "\n",
    "        # Flash point-like blending\n",
    "        flash_blend = sum(f * (r ** 0.7) for f, r in zip(fractions, safe_props))\n",
    "        df[f'flash_like_blend_prop{j}'] = flash_blend\n",
    "\n",
    "        features.extend([\n",
    "            f'ron_like_blend_prop{j}', f'log_visc_blend_prop{j}',\n",
    "            f'density_blend_prop{j}', f'rvp_blend_prop{j}',\n",
    "            f'cetane_like_blend_prop{j}', f'flash_like_blend_prop{j}'\n",
    "        ])\n",
    "\n",
    "    # Cross-property interactions (enhanced for oil properties)\n",
    "    for j1 in range(1, 11):\n",
    "        for j2 in range(j1 + 1, 11):\n",
    "            df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
    "            df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
    "            df[f'prop{j1}_prop{j2}_diff'] = df[f'weighted_mean_prop{j1}'] - df[f'weighted_mean_prop{j2}']\n",
    "            df[f'prop{j1}_prop{j2}_harmonic'] = 2 / (1/df[f'weighted_mean_prop{j1}'] + 1/(df[f'weighted_mean_prop{j2}'] + 1e-8))\n",
    "            features.extend([\n",
    "                f'prop{j1}_prop{j2}_interaction', f'prop{j1}_prop{j2}_ratio',\n",
    "                f'prop{j1}_prop{j2}_diff', f'prop{j1}_prop{j2}_harmonic'\n",
    "            ])\n",
    "\n",
    "    # Enhanced PCA with more components\n",
    "    prop_features = [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "    if fit_transformers:\n",
    "        pca = PCA(n_components=15, random_state=42)\n",
    "        pca_feats = pca.fit_transform(df[prop_features])\n",
    "    else:\n",
    "        pca = pca_model\n",
    "        pca_feats = pca.transform(df[prop_features])\n",
    "\n",
    "    for k in range(15):\n",
    "        df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
    "        features.append(f'pca_prop_{k+1}')\n",
    "\n",
    "    # Polynomial features for key interactions\n",
    "    if fit_transformers:\n",
    "        key_features = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "        key_features += [f'weighted_mean_prop{j}' for j in range(1, 11)]\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        poly_feats = poly.fit_transform(df[key_features])\n",
    "        poly_feature_names = poly.get_feature_names_out(key_features)\n",
    "        \n",
    "        # Add only the most relevant polynomial features\n",
    "        for i, name in enumerate(poly_feature_names):\n",
    "            if 'weighted_mean_prop' in name and ('fraction' in name or 'weighted_mean_prop' in name):\n",
    "                df[f'poly_{name}'] = poly_feats[:, i]\n",
    "                features.append(f'poly_{name}')\n",
    "    else:\n",
    "        poly = poly_features\n",
    "        if poly is not None:\n",
    "            key_features = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "            key_features += [f'weighted_mean_prop{j}' for j in range(1, 11)]\n",
    "            poly_feats = poly.transform(df[key_features])\n",
    "            poly_feature_names = poly.get_feature_names_out(key_features)\n",
    "            \n",
    "            for i, name in enumerate(poly_feature_names):\n",
    "                if 'weighted_mean_prop' in name and ('fraction' in name or 'weighted_mean_prop' in name):\n",
    "                    df[f'poly_{name}'] = poly_feats[:, i]\n",
    "                    features.append(f'poly_{name}')\n",
    "\n",
    "    # Power transformations for skewed features\n",
    "    if fit_transformers:\n",
    "        skewed_features = []\n",
    "        for col in [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]:\n",
    "            if abs(skew(df[col].dropna())) > 0.5:\n",
    "                skewed_features.append(col)\n",
    "        \n",
    "        if skewed_features:\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            transformed_feats = pt.fit_transform(df[skewed_features])\n",
    "            for i, col in enumerate(skewed_features):\n",
    "                df[f'power_transform_{col}'] = transformed_feats[:, i]\n",
    "                features.append(f'power_transform_{col}')\n",
    "    else:\n",
    "        pt = power_transformer\n",
    "        if pt is not None:\n",
    "            skewed_features = []\n",
    "            for col in [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]:\n",
    "                if col in df.columns and abs(skew(df[col].dropna())) > 0.5:\n",
    "                    skewed_features.append(col)\n",
    "            \n",
    "            if skewed_features:\n",
    "                transformed_feats = pt.transform(df[skewed_features])\n",
    "                for i, col in enumerate(skewed_features):\n",
    "                    df[f'power_transform_{col}'] = transformed_feats[:, i]\n",
    "                    features.append(f'power_transform_{col}')\n",
    "\n",
    "    # Fraction-based advanced features\n",
    "    frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    df['frac_sum'] = df[frac_cols].sum(axis=1)\n",
    "    df['frac_std'] = df[frac_cols].std(axis=1)\n",
    "    df['frac_skew'] = df[frac_cols].apply(lambda row: skew(row), axis=1)\n",
    "    df['frac_kurtosis'] = df[frac_cols].apply(lambda row: kurtosis(row), axis=1)\n",
    "    df['frac_entropy'] = -sum(df[f'Component{i}_fraction'] * np.log(df[f'Component{i}_fraction'] + 1e-8) for i in range(1, 6))\n",
    "    df['frac_gini'] = 1 - sum(df[f'Component{i}_fraction'] ** 2 for i in range(1, 6))\n",
    "    df['frac_max'] = df[frac_cols].max(axis=1)\n",
    "    df['frac_min'] = df[frac_cols].min(axis=1)\n",
    "    df['frac_range'] = df['frac_max'] - df['frac_min']\n",
    "\n",
    "    features.extend(['frac_sum', 'frac_std', 'frac_skew', 'frac_kurtosis', 'frac_entropy', 'frac_gini',\n",
    "                     'frac_max', 'frac_min', 'frac_range'])\n",
    "\n",
    "    return df, features, pca, poly, pt\n",
    "\n",
    "# Custom Neural Network Regressor\n",
    "class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim=100, hidden_layers=3, neurons=128, dropout=0.3, learning_rate=0.001, epochs=100):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.neurons, input_dim=self.input_dim, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout))\n",
    "        \n",
    "        for _ in range(self.hidden_layers - 1):\n",
    "            model.add(Dense(self.neurons, activation='relu'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.dropout))\n",
    "        \n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.input_dim = X.shape[1]\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=32, validation_split=0.2,\n",
    "                      callbacks=[early_stop, reduce_lr], verbose=0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0).flatten()\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "print(\"Creating enhanced oil-specific features...\")\n",
    "train, feat_cols, pca_model, poly_model, pt_model = create_enhanced_oil_features(train, fit_transformers=True)\n",
    "test, _, _, _, _ = create_enhanced_oil_features(test, pca_model, poly_model, pt_model, fit_transformers=False)\n",
    "\n",
    "print(f\"Created {len(feat_cols)} features for oil property prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b23867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "TARGETS = [f'BlendProperty{i}' for i in range(1, 11)]\n",
    "X_train = train[feat_cols]\n",
    "y_train = train[TARGETS]\n",
    "X_test = test[feat_cols]\n",
    "\n",
    "# Handle NaN values and infinite values\n",
    "print(\"Handling NaN and infinite values...\")\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_train.median())\n",
    "\n",
    "# Feature scaling for different models\n",
    "scaler_robust = RobustScaler()\n",
    "X_train_robust = scaler_robust.fit_transform(X_train)\n",
    "X_test_robust = scaler_robust.transform(X_test)\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "# Advanced feature selection\n",
    "print(\"Performing advanced feature selection...\")\n",
    "\n",
    "# Multiple feature selection methods\n",
    "selector_lgb = SelectFromModel(\n",
    "    LGBMRegressor(n_estimators=200, random_state=42, verbose=-1),\n",
    "    prefit=False,\n",
    "    threshold='median'\n",
    ")\n",
    "\n",
    "selector_rf = SelectFromModel(\n",
    "    RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    prefit=False,\n",
    "    threshold='median'\n",
    ")\n",
    "\n",
    "# Fit selectors on first target\n",
    "selector_lgb.fit(X_train, y_train.iloc[:, 0])\n",
    "selector_rf.fit(X_train, y_train.iloc[:, 0])\n",
    "\n",
    "# Combine selected features\n",
    "lgb_selected = selector_lgb.get_support()\n",
    "rf_selected = selector_rf.get_support()\n",
    "combined_selected = lgb_selected | rf_selected\n",
    "\n",
    "selected_features = [feat_cols[i] for i in range(len(feat_cols)) if combined_selected[i]]\n",
    "X_train_selected = X_train.iloc[:, combined_selected]\n",
    "X_test_selected = X_test.iloc[:, combined_selected]\n",
    "\n",
    "print(f\"Original features: {len(feat_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Hyperparameter optimization with Optuna\n",
    "def optimize_lightgbm(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**params)\n",
    "    scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    return scores.mean()\n",
    "\n",
    "def optimize_xgboost(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    return scores.mean()\n",
    "\n",
    "def optimize_catboost(trial, X, y):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "    scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    return scores.mean()\n",
    "\n",
    "def optimize_neural_network(trial, X, y):\n",
    "    params = {\n",
    "        'hidden_layers': trial.suggest_int('hidden_layers', 2, 5),\n",
    "        'neurons': trial.suggest_int('neurons', 64, 512),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "        'epochs': 100\n",
    "    }\n",
    "    \n",
    "    model = KerasRegressor(input_dim=X.shape[1], **params)\n",
    "    scores = cross_val_score(model, X, y, cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    return scores.mean()\n",
    "\n",
    "# Optimize hyperparameters for key models\n",
    "print(\"Optimizing hyperparameters...\")\n",
    "best_params = {}\n",
    "\n",
    "# Use first target for optimization to save time\n",
    "target_for_optimization = TARGETS[0]\n",
    "y_opt = y_train[target_for_optimization]\n",
    "\n",
    "# Optimize LightGBM\n",
    "print(\"Optimizing LightGBM...\")\n",
    "study_lgb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_lgb.optimize(lambda trial: optimize_lightgbm(trial, X_train_selected, y_opt), n_trials=30)\n",
    "best_params['lgb'] = study_lgb.best_params\n",
    "\n",
    "# Optimize XGBoost\n",
    "print(\"Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_xgb.optimize(lambda trial: optimize_xgboost(trial, X_train_selected, y_opt), n_trials=30)\n",
    "best_params['xgb'] = study_xgb.best_params\n",
    "\n",
    "# Optimize CatBoost\n",
    "print(\"Optimizing CatBoost...\")\n",
    "study_cat = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_cat.optimize(lambda trial: optimize_catboost(trial, X_train_selected, y_opt), n_trials=30)\n",
    "best_params['cat'] = study_cat.best_params\n",
    "\n",
    "# Optimize Neural Network\n",
    "print(\"Optimizing Neural Network...\")\n",
    "study_nn = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study_nn.optimize(lambda trial: optimize_neural_network(trial, X_train_standard, y_opt), n_trials=20)\n",
    "best_params['nn'] = study_nn.best_params\n",
    "\n",
    "print(\"Hyperparameter optimization completed!\")\n",
    "print(\"Best parameters:\")\n",
    "for model_name, params in best_params.items():\n",
    "    print(f\"{model_name}: {params}\")\n",
    "\n",
    "# Cross-Validation Setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_preds = np.zeros((X_test.shape[0], len(TARGETS)))\n",
    "\n",
    "print(\"\\nTraining Enhanced Oil Property Prediction Ensemble...\")\n",
    "print(f\"Total features: {len(feat_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(f\"Models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, SVR, Neural Network, Ridge, Elastic Net, Huber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, target in enumerate(TARGETS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training for {target} ({i+1}/{len(TARGETS)})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Out-of-fold predictions for each model\n",
    "    models_oof = {}\n",
    "    models_test_preds = {}\n",
    "    \n",
    "    model_names = ['lgb', 'xgb', 'cat', 'rf', 'et', 'gb', 'svr', 'nn', 'ridge', 'elastic', 'huber', 'lasso', 'bayesian']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        models_oof[model_name] = np.zeros(X_train.shape[0])\n",
    "        models_test_preds[model_name] = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nFold {fold + 1}/5\")\n",
    "        \n",
    "        # Model 1: Optimized LightGBM\n",
    "        model_lgb = LGBMRegressor(**best_params['lgb'])\n",
    "        model_lgb.fit(\n",
    "            X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx],\n",
    "            eval_set=[(X_train_selected.iloc[val_idx], y_train[target].iloc[val_idx])],\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(500)]\n",
    "        )\n",
    "        models_oof['lgb'][val_idx] = model_lgb.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['lgb'] += model_lgb.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 2: Optimized XGBoost\n",
    "        model_xgb = XGBRegressor(**best_params['xgb'])\n",
    "        model_xgb.fit(\n",
    "            X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx],\n",
    "            eval_set=[(X_train_selected.iloc[val_idx], y_train[target].iloc[val_idx])],\n",
    "            early_stopping_rounds=100, verbose=False\n",
    "        )\n",
    "        models_oof['xgb'][val_idx] = model_xgb.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['xgb'] += model_xgb.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 3: Optimized CatBoost\n",
    "        model_cat = CatBoostRegressor(**best_params['cat'])\n",
    "        model_cat.fit(\n",
    "            X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx],\n",
    "            eval_set=[(X_train_selected.iloc[val_idx], y_train[target].iloc[val_idx])],\n",
    "            early_stopping_rounds=100, verbose=False\n",
    "        )\n",
    "        models_oof['cat'][val_idx] = model_cat.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['cat'] += model_cat.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 4: Enhanced Random Forest\n",
    "        model_rf = RandomForestRegressor(\n",
    "            n_estimators=1000, max_depth=25, min_samples_split=3,\n",
    "            min_samples_leaf=1, max_features='sqrt', random_state=fold, n_jobs=-1\n",
    "        )\n",
    "        model_rf.fit(X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['rf'][val_idx] = model_rf.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['rf'] += model_rf.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 5: Enhanced Extra Trees\n",
    "        model_et = ExtraTreesRegressor(\n",
    "            n_estimators=800, max_depth=22, min_samples_split=2,\n",
    "            min_samples_leaf=1, max_features='sqrt', random_state=fold, n_jobs=-1\n",
    "        )\n",
    "        model_et.fit(X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['et'][val_idx] = model_et.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['et'] += model_et.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 6: Enhanced Gradient Boosting\n",
    "        model_gb = GradientBoostingRegressor(\n",
    "            n_estimators=800, learning_rate=0.005, max_depth=7,\n",
    "            min_samples_split=4, min_samples_leaf=2, max_features='sqrt',\n",
    "            random_state=fold\n",
    "        )\n",
    "        model_gb.fit(X_train_selected.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['gb'][val_idx] = model_gb.predict(X_train_selected.iloc[val_idx])\n",
    "        models_test_preds['gb'] += model_gb.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "        # Model 7: Support Vector Regression\n",
    "        model_svr = SVR(\n",
    "            kernel='rbf', C=10.0, gamma='scale', epsilon=0.01\n",
    "        )\n",
    "        model_svr.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['svr'][val_idx] = model_svr.predict(X_train_standard[val_idx])\n",
    "        models_test_preds['svr'] += model_svr.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "        # Model 8: Optimized Neural Network\n",
    "        model_nn = KerasRegressor(\n",
    "            input_dim=X_train_standard.shape[1],\n",
    "            **best_params['nn']\n",
    "        )\n",
    "        model_nn.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['nn'][val_idx] = model_nn.predict(X_train_standard[val_idx])\n",
    "        models_test_preds['nn'] += model_nn.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "        # Model 9: Enhanced Ridge\n",
    "        model_ridge = Ridge(alpha=0.1, random_state=fold)\n",
    "        model_ridge.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['ridge'][val_idx] = model_ridge.predict(X_train_robust[val_idx])\n",
    "        models_test_preds['ridge'] += model_ridge.predict(X_test_robust) / kf.get_n_splits()\n",
    "\n",
    "        # Model 10: Enhanced Elastic Net\n",
    "        model_elastic = ElasticNet(alpha=0.005, l1_ratio=0.2, random_state=fold, max_iter=3000)\n",
    "        model_elastic.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['elastic'][val_idx] = model_elastic.predict(X_train_standard[val_idx])\n",
    "        models_test_preds['elastic'] += model_elastic.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "        # Model 11: Enhanced Huber\n",
    "        model_huber = HuberRegressor(alpha=0.005, epsilon=1.2)\n",
    "        model_huber.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['huber'][val_idx] = model_huber.predict(X_train_robust[val_idx])\n",
    "        models_test_preds['huber'] += model_huber.predict(X_test_robust) / kf.get_n_splits()\n",
    "\n",
    "        # Model 12: Lasso Regression\n",
    "        model_lasso = Lasso(alpha=0.01, random_state=fold, max_iter=3000)\n",
    "        model_lasso.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['lasso'][val_idx] = model_lasso.predict(X_train_standard[val_idx])\n",
    "        models_test_preds['lasso'] += model_lasso.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "        # Model 13: Bayesian Ridge\n",
    "        model_bayesian = BayesianRidge(alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6)\n",
    "        model_bayesian.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        models_oof['bayesian'][val_idx] = model_bayesian.predict(X_train_standard[val_idx])\n",
    "        models_test_preds['bayesian'] += model_bayesian.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "    # Calculate individual model MAPE scores\n",
    "    mape_scores = {}\n",
    "    for model_name in model_names:\n",
    "        mape_scores[model_name] = mean_absolute_percentage_error(y_train[target], models_oof[model_name])\n",
    "\n",
    "    # Advanced ensemble with multiple weighting strategies\n",
    "    \n",
    "    # Strategy 1: Exponential weighting based on validation performance\n",
    "    exp_weights = {name: np.exp(-score * 5) for name, score in mape_scores.items()}\n",
    "    total_exp_weight = sum(exp_weights.values())\n",
    "    exp_weights = {name: w / total_exp_weight for name, w in exp_weights.items()}\n",
    "    \n",
    "    # Strategy 2: Inverse MAPE weighting\n",
    "    inv_weights = {name: 1/score for name, score in mape_scores.items()}\n",
    "    total_inv_weight = sum(inv_weights.values())\n",
    "    inv_weights = {name: w / total_inv_weight for name, w in inv_weights.items()}\n",
    "    \n",
    "    # Strategy 3: Rank-based weighting\n",
    "    sorted_models = sorted(mape_scores.items(), key=lambda x: x[1])\n",
    "    rank_weights = {}\n",
    "    for rank, (model_name, _) in enumerate(sorted_models):\n",
    "        rank_weights[model_name] = (len(model_names) - rank) / sum(range(1, len(model_names) + 1))\n",
    "    \n",
    "    # Combine strategies\n",
    "    final_weights = {}\n",
    "    for model_name in model_names:\n",
    "        final_weights[model_name] = (\n",
    "            0.4 * exp_weights[model_name] + \n",
    "            0.4 * inv_weights[model_name] + \n",
    "            0.2 * rank_weights[model_name]\n",
    "        )\n",
    "\n",
    "    # Final ensemble prediction\n",
    "    final_preds[:, i] = sum(\n",
    "        final_weights[model_name] * models_test_preds[model_name] \n",
    "        for model_name in model_names\n",
    "    )\n",
    "\n",
    "    # Ensemble validation score\n",
    "    ensemble_oof = sum(\n",
    "        final_weights[model_name] * models_oof[model_name] \n",
    "        for model_name in model_names\n",
    "    )\n",
    "    ensemble_mape = mean_absolute_percentage_error(y_train[target], ensemble_oof)\n",
    "\n",
    "    # Print detailed results\n",
    "    print(f\"\\nIndividual Model Performance for {target}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for model_name in sorted(mape_scores.keys(), key=lambda x: mape_scores[x]):\n",
    "        print(f\"{model_name.upper():>12}: MAPE={mape_scores[model_name]:.6f}, Weight={final_weights[model_name]:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'ENSEMBLE':>12}: MAPE={ensemble_mape:.6f}\")\n",
    "    print(f\"Best Single Model: {min(mape_scores.keys(), key=lambda x: mape_scores[x]).upper()}\")\n",
    "    print(f\"Improvement: {min(mape_scores.values()) - ensemble_mape:.6f}\")\n",
    "\n",
    "# Create Enhanced Submission\n",
    "submission = pd.DataFrame(final_preds, columns=TARGETS)\n",
    "submission.insert(0, 'ID', test.get('ID', np.arange(1, len(test) + 1)))\n",
    "\n",
    "# Apply post-processing to ensure realistic oil property values\n",
    "for target in TARGETS:\n",
    "    # Remove extreme outliers\n",
    "    Q1 = submission[target].quantile(0.01)\n",
    "    Q3 = submission[target].quantile(0.99)\n",
    "    submission[target] = submission[target].clip(Q1, Q3)\n",
    "\n",
    "submission.to_csv('submission_enhanced_oil_properties.csv', index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ENHANCED OIL PROPERTY PREDICTION ENSEMBLE SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total features engineered: {len(feat_cols)}\")\n",
    "print(f\"Selected features used: {len(selected_features)}\")\n",
    "print(f\"Models in ensemble: {len(model_names)}\")\n",
    "print(\"Models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\")\n",
    "print(\"        Gradient Boosting, SVR, Neural Network, Ridge, Elastic Net,\")\n",
    "print(\"        Huber, Lasso, Bayesian Ridge\")\n",
    "print(\"Hyperparameter optimization: Optuna with 30+ trials per model\")\n",
    "print(\"Feature engineering: Oil-specific domain knowledge + polynomial features\")\n",
    "print(\"Ensemble strategy: Multi-weighted combination (exponential + inverse + rank)\")\n",
    "print(\"Cross-validation: 5-fold stratified\")\n",
    "print(\"Post-processing: Outlier clipping\")\n",
    "print(f\"Submission file: submission_enhanced_oil_properties.csv\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nFeature Importance Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get feature importance from best LightGBM model\n",
    "temp_lgb = LGBMRegressor(**best_params['lgb'])\n",
    "temp_lgb.fit(X_train_selected, y_train.iloc[:, 0])\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': temp_lgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(20).iterrows()):\n",
    "    print(f\"{i+1:2d}. {row['feature']:30s} {row['importance']:8.4f}\")\n",
    "\n",
    "print(f\"\\nModel training completed successfully!\")\n",
    "print(f\"Enhanced submission ready for oil property prediction competition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038a50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.033333333333335"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Performance Analysis and Validation\n",
    "print(\"Performing cross-validation analysis...\")\n",
    "\n",
    "# Quick validation on a subset to estimate performance\n",
    "sample_size = min(1000, len(X_train))\n",
    "sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "\n",
    "X_sample = X_train_selected.iloc[sample_idx]\n",
    "y_sample = y_train.iloc[sample_idx, 0]  # Use first target for quick validation\n",
    "\n",
    "# Test a few models quickly\n",
    "quick_models = {\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=200, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"\\nQuick validation on {sample_size} samples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, model in quick_models.items():\n",
    "    scores = cross_val_score(model, X_sample, y_sample, cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    print(f\"{name:15s}: CV MAPE = {-scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Total features: {len(feat_cols):,}\")\n",
    "print(f\"Targets to predict: {len(TARGETS)}\")\n",
    "\n",
    "# Memory usage estimate\n",
    "memory_usage = X_train.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Training data memory usage: {memory_usage:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Ensemble Analysis and Meta-Learning\n",
    "print(\"Setting up advanced ensemble optimization...\")\n",
    "\n",
    "# Meta-features for stacking (if you want to implement stacking later)\n",
    "def create_meta_features(X, models, cv_folds=3):\n",
    "    \"\"\"Create meta-features using cross-validation predictions\"\"\"\n",
    "    meta_features = np.zeros((X.shape[0], len(models)))\n",
    "    \n",
    "    kf_meta = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        meta_pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for train_idx, val_idx in kf_meta.split(X):\n",
    "            model.fit(X.iloc[train_idx], y_train.iloc[train_idx, 0])  # Use first target\n",
    "            meta_pred[val_idx] = model.predict(X.iloc[val_idx])\n",
    "        \n",
    "        meta_features[:, i] = meta_pred\n",
    "        print(f\"Generated meta-features for {name}\")\n",
    "    \n",
    "    return meta_features\n",
    "\n",
    "# Oil property-specific validation\n",
    "def oil_property_validation(predictions, targets):\n",
    "    \"\"\"Validate predictions using oil industry constraints\"\"\"\n",
    "    \n",
    "    # Check for reasonable ranges (these would need to be adjusted based on actual oil properties)\n",
    "    validation_results = {}\n",
    "    \n",
    "    for i, target in enumerate(targets):\n",
    "        pred_col = predictions[:, i]\n",
    "        \n",
    "        # Basic statistics\n",
    "        validation_results[target] = {\n",
    "            'mean': np.mean(pred_col),\n",
    "            'std': np.std(pred_col),\n",
    "            'min': np.min(pred_col),\n",
    "            'max': np.max(pred_col),\n",
    "            'outliers_count': np.sum(np.abs(pred_col - np.mean(pred_col)) > 3 * np.std(pred_col))\n",
    "        }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Model diversity analysis\n",
    "def analyze_model_diversity(predictions_dict):\n",
    "    \"\"\"Analyze how diverse the model predictions are\"\"\"\n",
    "    pred_matrix = np.column_stack(list(predictions_dict.values()))\n",
    "    \n",
    "    # Correlation between models\n",
    "    correlation_matrix = np.corrcoef(pred_matrix.T)\n",
    "    \n",
    "    print(\"Model Diversity Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average correlation between models: {np.mean(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]):.4f}\")\n",
    "    print(f\"Min correlation: {np.min(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]):.4f}\")\n",
    "    print(f\"Max correlation: {np.max(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]):.4f}\")\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# Feature interaction analysis\n",
    "def analyze_feature_interactions(X, y, top_features=10):\n",
    "    \"\"\"Analyze top feature interactions for oil properties\"\"\"\n",
    "    \n",
    "    # Use a simple model to get feature importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop {top_features} Features for Oil Property Prediction:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (_, row) in enumerate(feature_importance.head(top_features).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['feature'][:50]:50s} {row['importance']:8.6f}\")\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Residual analysis\n",
    "def analyze_residuals(y_true, y_pred, target_name):\n",
    "    \"\"\"Analyze prediction residuals\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    print(f\"\\nResidual Analysis for {target_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Mean residual: {np.mean(residuals):8.6f}\")\n",
    "    print(f\"Std residual:  {np.std(residuals):8.6f}\")\n",
    "    print(f\"Max residual:  {np.max(np.abs(residuals)):8.6f}\")\n",
    "    print(f\"Skewness:      {skew(residuals):8.6f}\")\n",
    "    print(f\"Kurtosis:      {kurtosis(residuals):8.6f}\")\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "# Oil blending physics validation\n",
    "def validate_blending_physics(component_fractions, predicted_properties):\n",
    "    \"\"\"Validate predictions against basic blending physics\"\"\"\n",
    "    \n",
    "    # Check that component fractions sum to 1 (approximately)\n",
    "    fraction_sums = component_fractions.sum(axis=1)\n",
    "    fraction_violations = np.sum(np.abs(fraction_sums - 1.0) > 0.01)\n",
    "    \n",
    "    # Check for conservation of mass principles\n",
    "    conservation_score = np.mean(np.abs(fraction_sums - 1.0))\n",
    "    \n",
    "    print(\"Blending Physics Validation:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Fraction sum violations: {fraction_violations} out of {len(fraction_sums)}\")\n",
    "    print(f\"Average deviation from unity: {conservation_score:.6f}\")\n",
    "    \n",
    "    return fraction_violations, conservation_score\n",
    "\n",
    "print(\"Advanced analysis functions ready!\")\n",
    "print(\"\\nTo use these functions after training:\")\n",
    "print(\"1. oil_property_validation(final_preds, TARGETS)\")\n",
    "print(\"2. analyze_feature_interactions(X_train_selected, y_train.iloc[:, 0])\")\n",
    "print(\"3. validate_blending_physics(train[[f'Component{i}_fraction' for i in range(1, 6)]], final_preds)\")\n",
    "\n",
    "# Optuna study continuation (for further optimization)\n",
    "def continue_optimization(study, objective_func, n_additional_trials=50):\n",
    "    \"\"\"Continue optimization with more trials\"\"\"\n",
    "    print(f\"Continuing optimization with {n_additional_trials} additional trials...\")\n",
    "    study.optimize(objective_func, n_trials=n_additional_trials)\n",
    "    print(f\"Best score after additional trials: {study.best_value:.6f}\")\n",
    "    return study.best_params\n",
    "\n",
    "print(\"\\nReady for enhanced oil property prediction!\")\n",
    "print(\"All advanced features, models, and analysis tools are prepared.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kushvinth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
