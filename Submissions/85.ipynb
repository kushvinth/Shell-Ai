{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline # Import Pipeline\n",
    "\n",
    "# Define a function to get the trained model for each property based on the analysis\n",
    "def get_trained_final_model(data, target, property_name):\n",
    "    \"\"\"\n",
    "    Trains the best performing model for a specific blend property on the full training data.\n",
    "    \"\"\"\n",
    "    # Define the final models and their parameters based on the analysis\n",
    "    final_model_info = {\n",
    "        'BlendProperty1': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty2': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty3': ('ElasticNet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)),\n",
    "        'BlendProperty4': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty5': ('Random_Forest', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        'BlendProperty6': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty7': ('SVR_Poly', make_pipeline(StandardScaler(), SVR(kernel='poly', C=1.0, epsilon=0.1))),\n",
    "        'BlendProperty8': ('ElasticNet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)),\n",
    "        'BlendProperty9': ('ElasticNet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)),\n",
    "        'BlendProperty10': ('Neural_Network', Sequential([Dense(64, activation='relu', input_shape=(data.shape[1],)), Dropout(0.2), Dense(64, activation='relu'), Dense(1)]))\n",
    "    }\n",
    "\n",
    "    model_name, model = final_model_info[property_name]\n",
    "\n",
    "    X = data\n",
    "    y = target\n",
    "\n",
    "    print(f\"Training {model_name} for {property_name} on full dataset...\")\n",
    "\n",
    "    if model_name == 'Neural_Network':\n",
    "        model.compile(optimizer='adam', loss='mae')\n",
    "        model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "    elif model_name == 'TabNet':\n",
    "         # TabNet requires numpy and potential scaling\n",
    "         X_np = X.values\n",
    "         y_np = y.values.reshape(-1, 1)\n",
    "         scaler = StandardScaler()\n",
    "         X_scaled = scaler.fit_transform(X_np)\n",
    "         model.fit(X_scaled, y_np, max_epochs=200, patience=20, batch_size=256, virtual_batch_size=128, verbose=0)\n",
    "         # Wrap TabNet model and scaler in a pipeline for consistent prediction interface\n",
    "         class TabNetPipeline:\n",
    "             def __init__(self, scaler, tabnet_model):\n",
    "                 self.scaler = scaler\n",
    "                 self.tabnet_model = tabnet_model\n",
    "             def predict(self, X):\n",
    "                 X_scaled = self.scaler.transform(X.values)\n",
    "                 return self.tabnet_model.predict(X_scaled).flatten()\n",
    "         model = TabNetPipeline(scaler, model) # Return the wrapped model\n",
    "    elif isinstance(model, Pipeline): # Check against the Pipeline class\n",
    "        model.fit(X, y) # Pipeline handles scaling internally\n",
    "    else:\n",
    "        model.fit(X, y)\n",
    "\n",
    "    print(f\"Training complete for {property_name}.\")\n",
    "    return model\n",
    "\n",
    "# Load test data and sample submission\n",
    "# Assuming test.csv and sample_solution.csv are in the current directory\n",
    "try:\n",
    "  test_df = pd.read_csv(\"test.csv\")\n",
    "  submission_df = pd.read_csv(\"sample_solution.csv\")\n",
    "  test_ids = test_df['ID']\n",
    "  test_df_features = test_df.drop(columns=['ID'])\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure 'test.csv' and 'sample_solution.csv' are uploaded to your Colab session.\")\n",
    "\n",
    "\n",
    "if 'test_df_features' in locals(): # Check if test data was loaded\n",
    "  # Generate predictions using the best model for each property\n",
    "  for i in range(1, 11):\n",
    "      property_name = f'BlendProperty{i}'\n",
    "      print(f\"\\nProcessing {property_name} for final submission...\")\n",
    "\n",
    "      # Define features for this property\n",
    "      features = ['Component1_fraction', 'Component2_fraction', 'Component3_fraction',\n",
    "                 'Component4_fraction', 'Component5_fraction'] + \\\n",
    "                [f'Component{j}_Property{i}' for j in range(1, 6)]\n",
    "\n",
    "      # Train the best model for this property on the full training data\n",
    "      trained_model = get_trained_final_model(df[features], df[property_name], property_name)\n",
    "\n",
    "      # Make predictions on the test data\n",
    "      test_predictions = trained_model.predict(test_df_features[features])\n",
    "\n",
    "      # Update the submission DataFrame\n",
    "      submission_df[property_name] = test_predictions\n",
    "\n",
    "  # Save the final submission file\n",
    "  submission_df.to_csv('final_model_submission.csv', index=False)\n",
    "\n",
    "  print(\"\\n\" + \"=\"*80)\n",
    "  print(\"Final submission file 'final_model_submission.csv' created successfully.\")\n",
    "  print(\"=\"*80)\n",
    "  \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
