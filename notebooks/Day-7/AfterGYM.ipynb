{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb607d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "#########FIX###########\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# n_splits = 5\n",
    "#######################\n",
    "\n",
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breakthrough Feature Engineering\n",
    "def create_breakthrough_features(df, pca_model=None, scaler=None, fit_transformers=True):\n",
    "    features = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    features += [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "\n",
    "    # Enhanced interaction features with non-linear transformations\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 11):\n",
    "            df[f'frac{i}_prop{j}'] = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
    "            df[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
    "            df[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
    "            df[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
    "            features.extend([f'frac{i}_prop{j}', f'frac{i}_prop{j}_sqrt', f'frac{i}_prop{j}_log', f'frac{i}_prop{j}_square'])\n",
    "\n",
    "    # Advanced weighted features with multiple aggregation methods\n",
    "    for j in range(1, 11):\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "\n",
    "        # Multiple weighted aggregations\n",
    "        df[f'weighted_mean_prop{j}'] = sum(\n",
    "            df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}'] for i in range(1, 6)\n",
    "        )\n",
    "        mean = df[f'weighted_mean_prop{j}']\n",
    "        df[f'weighted_var_prop{j}'] = sum(\n",
    "            df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] - mean) ** 2 for i in range(1, 6)\n",
    "        )\n",
    "\n",
    "    # Harmonic mean (important for fuel properties)\n",
    "    safe_props = [np.maximum(df[f'Component{i}_Property{j}'], 1e-6) for i in range(1, 6)]\n",
    "    harmonic_mean = sum(df[f'Component{i}_fraction'] / safe_props[i-1] for i in range(1, 6))\n",
    "    df[f'harmonic_mean_prop{j}'] = 1 / harmonic_mean\n",
    "\n",
    "    # Geometric mean (for multiplicative properties)\n",
    "    log_geo_mean = sum(df[f'Component{i}_fraction'] * np.log(safe_props[i-1]) for i in range(1, 6))\n",
    "    df[f'geometric_mean_prop{j}'] = np.exp(log_geo_mean)\n",
    "\n",
    "    # Component dominance with ranking\n",
    "    frac_array = np.array([df[f'Component{i}_fraction'] for i in range(1, 6)])\n",
    "    dominant_idx = np.argmax(frac_array, axis=0)\n",
    "    df[f'dominant_prop{j}'] = df.apply(lambda row:\n",
    "        row[f'Component{dominant_idx[row.name] + 1}_Property{j}'], axis=1)\n",
    "\n",
    "    # Blend balance and diversity\n",
    "    frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    df[f'blend_balance_prop{j}'] = 1 - df[frac_cols].std(axis=1)\n",
    "    df[f'blend_diversity_prop{j}'] = df[frac_cols].std(axis=1) / (df[frac_cols].mean(axis=1) + 1e-8)\n",
    "\n",
    "    # Advanced statistics\n",
    "    for j in range(1, 11):\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        df[f'min_prop{j}'] = df[prop_cols].min(axis=1)\n",
    "        df[f'max_prop{j}'] = df[prop_cols].max(axis=1)\n",
    "        df[f'mean_prop{j}'] = df[prop_cols].mean(axis=1)\n",
    "        df[f'std_prop{j}'] = df[prop_cols].std(axis=1)\n",
    "        df[f'median_prop{j}'] = df[prop_cols].median(axis=1)\n",
    "        df[f'skew_prop{j}'] = df[prop_cols].apply(lambda row: skew(row), axis=1)\n",
    "        df[f'kurtosis_prop{j}'] = df[prop_cols].apply(lambda row: kurtosis(row), axis=1)\n",
    "        df[f'range_prop{j}'] = df[f'max_prop{j}'] - df[f'min_prop{j}']\n",
    "        df[f'iqr_prop{j}'] = df[prop_cols].quantile(0.75, axis=1) - df[prop_cols].quantile(0.25, axis=1)\n",
    "\n",
    "        features.extend([\n",
    "            f'min_prop{j}', f'max_prop{j}', f'mean_prop{j}',\n",
    "            f'std_prop{j}', f'median_prop{j}', f'skew_prop{j}', f'kurtosis_prop{j}',\n",
    "            f'range_prop{j}', f'iqr_prop{j}'\n",
    "        ])\n",
    "\n",
    "\n",
    "    # Shell-specific advanced features\n",
    "    for j in range(1, 11):\n",
    "        fractions = [df[f'Component{i}_fraction'] for i in range(1, 6)]\n",
    "        props = [df[f'Component{i}_Property{j}'] for i in range(1, 6)]\n",
    "        safe_props = [np.maximum(p, 1e-6) for p in props]\n",
    "\n",
    "        # RON-like blending (non-linear octane)\n",
    "        ron_blend = sum(f * (r ** 1.5) for f, r in zip(fractions, safe_props)) ** (1 / 1.5)\n",
    "        df[f'ron_like_blend_prop{j}'] = ron_blend\n",
    "\n",
    "        # Viscosity-like blending (logarithmic)\n",
    "        log_visc_blend = sum(f * np.log(r) for f, r in zip(fractions, safe_props))\n",
    "        df[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
    "\n",
    "        # Density-like blending (linear but with corrections)\n",
    "        density_blend = sum(f * r for f, r in zip(fractions, safe_props))\n",
    "        df[f'density_blend_prop{j}'] = density_blend\n",
    "\n",
    "    # Reid vapor pressure-like (exponential)\n",
    "        rvp_blend = sum(f * np.exp(r / 100) for f, r in zip(fractions, safe_props))\n",
    "        df[f'rvp_blend_prop{j}'] = rvp_blend\n",
    "\n",
    "        features.extend([\n",
    "            f'ron_like_blend_prop{j}', f'log_visc_blend_prop{j}',\n",
    "            f'density_blend_prop{j}', f'rvp_blend_prop{j}'\n",
    "        ])\n",
    "\n",
    "    # Cross-property interactions (most important combinations)\n",
    "    for j1 in range(1, 6):\n",
    "        for j2 in range(j1 + 1, 7):\n",
    "            df[f'prop{j1}_prop{j2}_interaction'] = df[f'weighted_mean_prop{j1}'] * df[f'weighted_mean_prop{j2}']\n",
    "            df[f'prop{j1}_prop{j2}_ratio'] = df[f'weighted_mean_prop{j1}'] / (df[f'weighted_mean_prop{j2}'] + 1e-8)\n",
    "            features.extend([f'prop{j1}_prop{j2}_interaction', f'prop{j1}_prop{j2}_ratio'])\n",
    "\n",
    "    # Enhanced PCA with more components\n",
    "    prop_features = [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "    if fit_transformers:\n",
    "        pca = PCA(n_components=12, random_state=42)\n",
    "        pca_feats = pca.fit_transform(df[prop_features])\n",
    "    else:\n",
    "        pca = pca_model\n",
    "        pca_feats = pca.transform(df[prop_features])\n",
    "\n",
    "\n",
    "    for k in range(12):\n",
    "        df[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
    "        features.append(f'pca_prop_{k+1}')\n",
    "\n",
    "    # Fraction-based advanced features\n",
    "    frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    df['frac_sum'] = df[frac_cols].sum(axis=1)\n",
    "    df['frac_std'] = df[frac_cols].std(axis=1)\n",
    "    df['frac_skew'] = df[frac_cols].apply(lambda row: skew(row), axis=1)\n",
    "    df['frac_kurtosis'] = df[frac_cols].apply(lambda row: kurtosis(row), axis=1)\n",
    "    df['frac_entropy'] = -sum(df[f'Component{i}_fraction'] * np.log(df[f'Component{i}_fraction'] + 1e-8) for i in range(1, 6))\n",
    "    df['frac_gini'] = 1 - sum(df[f'Component{i}_fraction'] ** 2 for i in range(1, 6))\n",
    "\n",
    "    features.extend(['frac_sum', 'frac_std', 'frac_skew', 'frac_kurtosis', 'frac_entropy', 'frac_gini'])\n",
    "\n",
    "    return df, features, pca\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"Creating breakthrough features...\")\n",
    "train, feat_cols, pca_model = create_breakthrough_features(train, fit_transformers=True)\n",
    "test, _, _ = create_breakthrough_features(test, pca_model=pca_model, fit_transformers=False)\n",
    "\n",
    "# Prepare Data\n",
    "TARGETS = [f'BlendProperty{i}' for i in range(1, 11)]\n",
    "X_train = train[feat_cols]\n",
    "y_train = train[TARGETS]\n",
    "X_test = test[feat_cols]\n",
    "\n",
    "# Handle NaN values\n",
    "print(\"Handling NaN values...\")\n",
    "\n",
    "# Handle NaN values\n",
    "print(\"Handling NaN values...\")\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "# Feature scaling for different models\n",
    "scaler_robust = RobustScaler()\n",
    "X_train_robust = scaler_robust.fit_transform(X_train)\n",
    "X_test_robust = scaler_robust.transform(X_test)\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "# Feature selection for some models\n",
    "print(\"Performing feature selection...\")\n",
    "selector = SelectFromModel(\n",
    "    LGBMRegressor(n_estimators=200, random_state=42, verbose=-1),\n",
    "    prefit=False,\n",
    "    threshold='median'\n",
    ")\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train, y_train.iloc[:, 0])\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = [feat_cols[i] for i in range(len(feat_cols)) if selector.get_support()[i]]\n",
    "print(f\"Original features: {len(feat_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "\n",
    "# Cross-Validation Setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_preds = np.zeros((X_test.shape[0], len(TARGETS)))\n",
    "\n",
    "print(\"Training Breakthrough Ensemble...\")\n",
    "print(f\"Features: {len(feat_cols)} (selected: {len(selected_features)})\")\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    print(f\"\\nTraining for {target}...\")\n",
    "\n",
    "    # Out-of-fold predictions for each model\n",
    "    lgb_oof = np.zeros(X_train.shape[0])\n",
    "    rf_oof = np.zeros(X_train.shape[0])\n",
    "    et_oof = np.zeros(X_train.shape[0])\n",
    "    gb_oof = np.zeros(X_train.shape[0])\n",
    "    ridge_oof = np.zeros(X_train.shape[0])\n",
    "    elastic_oof = np.zeros(X_train.shape[0])\n",
    "    huber_oof = np.zeros(X_train.shape[0])\n",
    "    gpr_oof = np.zeros(X_train.shape[0])\n",
    "\n",
    "    # Test predictions for each model\n",
    "    lgb_test_preds = np.zeros(X_test.shape[0])\n",
    "    rf_test_preds = np.zeros(X_test.shape[0])\n",
    "    et_test_preds = np.zeros(X_test.shape[0])\n",
    "    gb_test_preds = np.zeros(X_test.shape[0])\n",
    "    ridge_test_preds = np.zeros(X_test.shape[0])\n",
    "    elastic_test_preds = np.zeros(X_test.shape[0])\n",
    "    huber_test_preds = np.zeros(X_test.shape[0])\n",
    "    gpr_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "\n",
    "        #############################################################################\n",
    "        #                     Changed kf.n_split to kf.get_n_splits()               #\n",
    "        #############################################################################\n",
    "\n",
    "        # Model 1: Optimized LightGBM\n",
    "        model_lgb = LGBMRegressor(\n",
    "            n_estimators=12000, learning_rate=0.0015, random_state=fold,\n",
    "            num_leaves=31, subsample=0.85, colsample_bytree=0.85,\n",
    "            reg_alpha=0.01, reg_lambda=0.01, min_child_samples=20,\n",
    "            objective='regression_l1'  # Use MAE for better MAPE alignment\n",
    "            )\n",
    "\n",
    "        model_lgb.fit(\n",
    "            X_train.iloc[tr_idx], y_train[target].iloc[tr_idx],\n",
    "            eval_set=[(X_train.iloc[val_idx], y_train[target].iloc[val_idx])],\n",
    "            callbacks=[early_stopping(stopping_rounds=150), log_evaluation(200)]\n",
    "        )\n",
    "\n",
    "        #############################################################################\n",
    "        #                     # ...existing code...\n",
    "        #\n",
    "        #       # Convert predictions to dense arrays before assignment\n",
    "        #       lgb_oof[val_idx] = np.array(model_lgb.predict(X_train.iloc[val_idx]))\n",
    "        #       lgb_test_preds += np.array(model_lgb.predict(X_test)) / kf.get_n_splits()\n",
    "        #       rf_oof[val_idx] = np.array(model_rf.predict(X_train.iloc[val_idx]))\n",
    "        #       rf_test_preds += np.array(model_rf.predict(X_test)) / kf.get_n_splits()\n",
    "        #\n",
    "        #      et_oof[val_idx] = np.array(model_et.predict(X_train.iloc[val_idx]))\n",
    "        #      et_test_preds += np.array(model_et.predict(X_test)) / kf.get_n_splits()\n",
    "        #\n",
    "        #      gb_oof[val_idx] = np.array(model_gb.predict(X_train.iloc[val_idx]))                    #\n",
    "        #      gb_test_preds += np.array(model_gb.predict(X_test)) / kf.get_n_splits()                #\n",
    "        #                                                                                             #\n",
    "        #        ridge_oof[val_idx] = np.array(model_ridge.predict(X_train_robust[val_idx]))          #\n",
    "        #        ridge_test_preds += np.array(model_ridge.predict(X_test_robust)) / kf.get_n_splits() #\n",
    "        #        # ...existing code...                                                                #\n",
    "        ###############################################################################################\n",
    "\n",
    "        # lgb_oof[val_idx] = np.array(model_lgb.predict(X_train.iloc[val_idx]))\n",
    "        # lgb_test_preds += np.array(model_lgb.predict(X_test) / kf.get_n_splits())\n",
    "        # Convert predictions to dense arrays before assignment\n",
    "        lgb_oof[val_idx] = np.array(model_lgb.predict(X_train.iloc[val_idx]))\n",
    "        lgb_test_preds += np.array(model_lgb.predict(X_test)) / kf.get_n_splits()\n",
    "\n",
    "        # Model 2: Optimized Random Forest\n",
    "        model_rf = RandomForestRegressor(\n",
    "            n_estimators=800, max_depth=20, min_samples_split=5,\n",
    "            min_samples_leaf=2, random_state=fold, n_jobs=-1\n",
    "        )\n",
    "        model_rf.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        rf_oof[val_idx] = model_rf.predict(X_train.iloc[val_idx])\n",
    "        rf_test_preds += model_rf.predict(X_test) / kf.get_n_splits()\n",
    "\n",
    "        # Model 3: Extra Trees\n",
    "        model_et = ExtraTreesRegressor(\n",
    "            n_estimators=600, max_depth=18, min_samples_split=3,\n",
    "            min_samples_leaf=1, random_state=fold, n_jobs=-1\n",
    "        )\n",
    "        model_et.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        et_oof[val_idx] = model_et.predict(X_train.iloc[val_idx])\n",
    "        et_test_preds += model_et.predict(X_test) / kf.get_n_splits()\n",
    "\n",
    "        # Model 4: Gradient Boosting\n",
    "        model_gb = GradientBoostingRegressor(\n",
    "            n_estimators=500, learning_rate=0.01, max_depth=6,\n",
    "            min_samples_split=5, min_samples_leaf=2, random_state=fold\n",
    "        )\n",
    "\n",
    "        model_gb.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        gb_oof[val_idx] = model_gb.predict(X_train.iloc[val_idx])\n",
    "        gb_test_preds += model_gb.predict(X_test) / kf.get_n_splits()\n",
    "\n",
    "        # Model 5: Ridge (with robust scaling)\n",
    "        model_ridge = Ridge(alpha=0.03, random_state=fold)\n",
    "        model_ridge.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        ridge_oof[val_idx] = model_ridge.predict(X_train_robust[val_idx])\n",
    "        ridge_test_preds += model_ridge.predict(X_test_robust) / kf.get_n_splits()\n",
    "\n",
    "        # Model 6: Elastic Net (with standard scaling)\n",
    "        model_elastic = ElasticNet(alpha=0.008, l1_ratio=0.3, random_state=fold, max_iter=2000)\n",
    "        model_elastic.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        elastic_oof[val_idx] = model_elastic.predict(X_train_standard[val_idx])\n",
    "        elastic_test_preds += model_elastic.predict(X_test_standard) / kf.get_n_splits()\n",
    "\n",
    "        # Model 7: Huber (robust to outliers)\n",
    "        model_huber = HuberRegressor(alpha=0.01, epsilon=1.35)\n",
    "        model_huber.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        huber_oof[val_idx] = model_huber.predict(X_train_robust[val_idx])\n",
    "        huber_test_preds += model_huber.predict(X_test_robust) / kf.get_n_splits()\n",
    "\n",
    "        # Model 8: Gaussian Process Regression (with feature selection and standard scaling)\n",
    "        # Use selected features for GPR to manage computational complexity\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2)) + WhiteKernel(1e-5, (1e-6, 1e-1))\n",
    "        model_gpr = GaussianProcessRegressor(\n",
    "            kernel=kernel, \n",
    "            alpha=1e-6, \n",
    "            random_state=fold,\n",
    "            n_restarts_optimizer=2  # Reduced for speed\n",
    "        )\n",
    "        model_gpr.fit(X_train_selected[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        gpr_oof[val_idx] = model_gpr.predict(X_train_selected[val_idx])\n",
    "        gpr_test_preds += model_gpr.predict(X_test_selected) / kf.get_n_splits()\n",
    "\n",
    "    # Calculate individual model MAPE\n",
    "    lgb_mape = mean_absolute_percentage_error(y_train[target], lgb_oof)\n",
    "    rf_mape = mean_absolute_percentage_error(y_train[target], rf_oof)\n",
    "    et_mape = mean_absolute_percentage_error(y_train[target], et_oof)\n",
    "    gb_mape = mean_absolute_percentage_error(y_train[target], gb_oof)\n",
    "    ridge_mape = mean_absolute_percentage_error(y_train[target], ridge_oof)\n",
    "    elastic_mape = mean_absolute_percentage_error(y_train[target], elastic_oof)\n",
    "    huber_mape = mean_absolute_percentage_error(y_train[target], huber_oof)\n",
    "    gpr_mape = mean_absolute_percentage_error(y_train[target], gpr_oof)\n",
    "\n",
    "\n",
    "    #-----\n",
    "    # Advanced ensemble: Exponential weighting based on validation performance\n",
    "    mape_scores = [lgb_mape, rf_mape, et_mape, gb_mape, ridge_mape, elastic_mape, huber_mape, gpr_mape]\n",
    "    weights = [np.exp(-score * 10) for score in mape_scores]  # Exponential weighting\n",
    "    total_weight = sum(weights)\n",
    "    weights = [w / total_weight for w in weights]\n",
    "\n",
    "    # Final predictions\n",
    "    final_preds[:, i] = (\n",
    "        weights[0] * lgb_test_preds +\n",
    "        weights[1] * rf_test_preds +\n",
    "        weights[2] * et_test_preds +\n",
    "        weights[3] * gb_test_preds +\n",
    "        weights[4] * ridge_test_preds +\n",
    "        weights[5] * elastic_test_preds +\n",
    "        weights[6] * huber_test_preds +\n",
    "        weights[7] * gpr_test_preds\n",
    "    )\n",
    "\n",
    "    # Ensemble validation score\n",
    "    ensemble_oof = (\n",
    "        weights[0] * lgb_oof +\n",
    "        weights[1] * rf_oof +\n",
    "        weights[2] * et_oof +\n",
    "        weights[3] * gb_oof +\n",
    "        weights[4] * ridge_oof +\n",
    "        weights[5] * elastic_oof +\n",
    "        weights[6] * huber_oof +\n",
    "        weights[7] * gpr_oof\n",
    "    )\n",
    "\n",
    "    ensemble_mape = mean_absolute_percentage_error(y_train[target], ensemble_oof)\n",
    "\n",
    "    print(f\"LightGBM MAPE: {lgb_mape:.4f} (weight: {weights[0]:.3f})\")\n",
    "    print(f\"Random Forest MAPE: {rf_mape:.4f} (weight: {weights[1]:.3f})\")\n",
    "    print(f\"Extra Trees MAPE: {et_mape:.4f} (weight: {weights[2]:.3f})\")\n",
    "    print(f\"Gradient Boosting MAPE: {gb_mape:.4f} (weight: {weights[3]:.3f})\")\n",
    "    print(f\"Ridge MAPE: {ridge_mape:.4f} (weight: {weights[4]:.3f})\")\n",
    "    print(f\"Elastic Net MAPE: {elastic_mape:.4f} (weight: {weights[5]:.3f})\")\n",
    "    print(f\"Huber MAPE: {huber_mape:.4f} (weight: {weights[6]:.3f})\")\n",
    "    print(f\"Gaussian Process MAPE: {gpr_mape:.4f} (weight: {weights[7]:.3f})\")\n",
    "    print(f\"Ensemble MAPE: {ensemble_mape:.4f}\")\n",
    "\n",
    "# Create Submission\n",
    "submission = pd.DataFrame(final_preds, columns=TARGETS)\n",
    "submission.insert(0, 'ID', test.get('ID', np.arange(1, len(test) + 1)))\n",
    "submission.to_csv('submission_breakthrough_90plus.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file created: submission_breakthrough_90plus.csv\")\n",
    "print(\"\\nBreakthrough Ensemble Summary:\")\n",
    "print(f\"Features: {len(feat_cols)} (selected: {len(selected_features)})\")\n",
    "print(\"Cross-validation: 5-fold\")\n",
    "print(\"Models: LightGBM, Random Forest, Extra Trees, Gradient Boosting, Ridge, Elastic Net, Huber, Gaussian Process\")\n",
    "print(\"Ensemble: Exponential weighting based on validation performance\")\n",
    "print(\"Scaling: Robust and Standard scaling for different models\")\n",
    "print(\"Target: 90+ score with breakthrough features and advanced ensemble\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
