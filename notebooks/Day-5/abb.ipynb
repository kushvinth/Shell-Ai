{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e191e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2000, 65), Test Shape: (500, 56)\n",
      "                      count      mean       std       min       25%       50%  \\\n",
      "Component1_fraction  2000.0  0.180690  0.163200  0.000000  0.030000  0.140000   \n",
      "Component2_fraction  2000.0  0.182910  0.163704  0.000000  0.040000  0.150000   \n",
      "Component3_fraction  2000.0  0.179820  0.166283  0.000000  0.020000  0.140000   \n",
      "Component4_fraction  2000.0  0.342090  0.141119  0.010000  0.220000  0.350000   \n",
      "Component5_fraction  2000.0  0.114490  0.080219  0.000000  0.050000  0.120000   \n",
      "...                     ...       ...       ...       ...       ...       ...   \n",
      "BlendProperty6       2000.0 -0.003497  1.009126 -2.808210 -0.697379 -0.011649   \n",
      "BlendProperty7       2000.0 -0.013568  1.000613 -2.994571 -0.622453  0.133470   \n",
      "BlendProperty8       2000.0 -0.017236  0.998759 -3.621080 -0.725564 -0.001548   \n",
      "BlendProperty9       2000.0 -0.001507  1.001096 -3.292727 -0.702384 -0.002604   \n",
      "BlendProperty10      2000.0 -0.001795  0.990433 -2.476429 -0.733653 -0.010459   \n",
      "\n",
      "                          75%       max  \n",
      "Component1_fraction  0.290000  0.500000  \n",
      "Component2_fraction  0.300000  0.500000  \n",
      "Component3_fraction  0.290000  0.500000  \n",
      "Component4_fraction  0.500000  0.500000  \n",
      "Component5_fraction  0.180000  0.290000  \n",
      "...                       ...       ...  \n",
      "BlendProperty6       0.695182  3.433292  \n",
      "BlendProperty7       0.704130  3.293228  \n",
      "BlendProperty8       0.684894  3.340657  \n",
      "BlendProperty9       0.706084  3.276199  \n",
      "BlendProperty10      0.693839  2.708703  \n",
      "\n",
      "[65 rows x 8 columns]\n",
      "Epoch 1/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 9.5945 - mape_loss: 9.5860 - val_loss: 7.3418 - val_mape_loss: 6.2178 - learning_rate: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.1441 - mape_loss: 15.0769 - val_loss: 6.2881 - val_mape_loss: 5.2980 - learning_rate: 0.0010\n",
      "Epoch 3/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.9022 - mape_loss: 10.8917 - val_loss: 4.4312 - val_mape_loss: 3.8452 - learning_rate: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 50.4833 - mape_loss: 50.4570 - val_loss: 3.2091 - val_mape_loss: 2.8890 - learning_rate: 0.0010\n",
      "Epoch 5/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1434 - mape_loss: 4.1376 - val_loss: 4.1658 - val_mape_loss: 3.6742 - learning_rate: 0.0010\n",
      "Epoch 6/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8093 - mape_loss: 4.7871 - val_loss: 2.4765 - val_mape_loss: 2.3486 - learning_rate: 0.0010\n",
      "Epoch 7/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3621 - mape_loss: 3.3578 - val_loss: 2.1159 - val_mape_loss: 1.8949 - learning_rate: 0.0010\n",
      "Epoch 8/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.3501 - mape_loss: 12.3301 - val_loss: 1.1495 - val_mape_loss: 1.1230 - learning_rate: 0.0010\n",
      "Epoch 9/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8520 - mape_loss: 1.8489 - val_loss: 1.0528 - val_mape_loss: 1.0390 - learning_rate: 0.0010\n",
      "Epoch 10/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3734 - mape_loss: 1.3718 - val_loss: 1.2698 - val_mape_loss: 1.2076 - learning_rate: 0.0010\n",
      "Epoch 11/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1421 - mape_loss: 1.1419 - val_loss: 1.0248 - val_mape_loss: 1.0188 - learning_rate: 0.0010\n",
      "Epoch 12/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0666 - mape_loss: 1.0664 - val_loss: 1.0062 - val_mape_loss: 1.0052 - learning_rate: 0.0010\n",
      "Epoch 13/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1005 - mape_loss: 1.1004 - val_loss: 1.0133 - val_mape_loss: 1.0107 - learning_rate: 0.0010\n",
      "Epoch 14/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0053 - mape_loss: 1.0053 - val_loss: 1.0095 - val_mape_loss: 1.0084 - learning_rate: 0.0010\n",
      "Epoch 15/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0321 - mape_loss: 1.0321 - val_loss: 1.0134 - val_mape_loss: 1.0103 - learning_rate: 0.0010\n",
      "Epoch 16/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6295 - mape_loss: 1.6277 - val_loss: 1.0051 - val_mape_loss: 1.0041 - learning_rate: 0.0010\n",
      "Epoch 17/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0405 - mape_loss: 1.0403 - val_loss: 1.0058 - val_mape_loss: 1.0056 - learning_rate: 0.0010\n",
      "Epoch 18/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0037 - mape_loss: 1.0037 - val_loss: 1.0110 - val_mape_loss: 1.0085 - learning_rate: 0.0010\n",
      "Epoch 19/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1827 - mape_loss: 2.1815 - val_loss: 1.0069 - val_mape_loss: 1.0039 - learning_rate: 0.0010\n",
      "Epoch 20/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1116 - mape_loss: 1.1115 - val_loss: 1.0040 - val_mape_loss: 1.0047 - learning_rate: 0.0010\n",
      "Epoch 21/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0174 - mape_loss: 1.0174 - val_loss: 1.0057 - val_mape_loss: 1.0045 - learning_rate: 0.0010\n",
      "Epoch 22/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0069 - mape_loss: 1.0069 - val_loss: 1.0020 - val_mape_loss: 1.0015 - learning_rate: 0.0010\n",
      "Epoch 23/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0575 - mape_loss: 1.0574 - val_loss: 1.0181 - val_mape_loss: 1.0144 - learning_rate: 0.0010\n",
      "Epoch 24/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0071 - mape_loss: 1.0071 - val_loss: 1.0158 - val_mape_loss: 1.0127 - learning_rate: 0.0010\n",
      "Epoch 25/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0216 - mape_loss: 1.0216 - val_loss: 1.0085 - val_mape_loss: 1.0065 - learning_rate: 0.0010\n",
      "Epoch 26/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0152 - mape_loss: 1.0152 - val_loss: 1.0168 - val_mape_loss: 1.0129 - learning_rate: 0.0010\n",
      "Epoch 27/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0718 - mape_loss: 1.0718 - val_loss: 1.0132 - val_mape_loss: 1.0105 - learning_rate: 0.0010\n",
      "Epoch 28/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0022 - mape_loss: 1.0022 - val_loss: 1.0115 - val_mape_loss: 1.0089 - learning_rate: 0.0010\n",
      "Epoch 29/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0233 - mape_loss: 1.0233 - val_loss: 1.0014 - val_mape_loss: 1.0011 - learning_rate: 0.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0362 - mape_loss: 1.0362 - val_loss: 1.0014 - val_mape_loss: 1.0016 - learning_rate: 0.0010\n",
      "Epoch 31/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0072 - mape_loss: 1.0071 - val_loss: 1.0090 - val_mape_loss: 1.0078 - learning_rate: 0.0010\n",
      "Epoch 32/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0047 - mape_loss: 1.0047 - val_loss: 1.0152 - val_mape_loss: 1.0124 - learning_rate: 0.0010\n",
      "Epoch 33/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0079 - mape_loss: 1.0079 - val_loss: 1.0132 - val_mape_loss: 1.0115 - learning_rate: 0.0010\n",
      "Epoch 34/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0063 - mape_loss: 1.0062 - val_loss: 1.0024 - val_mape_loss: 1.0022 - learning_rate: 0.0010\n",
      "Epoch 35/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0127 - mape_loss: 1.0127 - val_loss: 1.0089 - val_mape_loss: 1.0071 - learning_rate: 0.0010\n",
      "Epoch 36/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0197 - mape_loss: 1.0196 - val_loss: 1.0071 - val_mape_loss: 1.0063 - learning_rate: 0.0010\n",
      "Epoch 37/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0219 - mape_loss: 1.0219 - val_loss: 1.0153 - val_mape_loss: 1.0115 - learning_rate: 0.0010\n",
      "Epoch 38/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0121 - mape_loss: 1.0121 - val_loss: 1.0023 - val_mape_loss: 1.0024 - learning_rate: 0.0010\n",
      "Epoch 39/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0025 - mape_loss: 1.0025 - val_loss: 1.0085 - val_mape_loss: 1.0070 - learning_rate: 0.0010\n",
      "Epoch 40/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0032 - mape_loss: 1.0032 - val_loss: 1.0024 - val_mape_loss: 1.0022 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0114 - mape_loss: 1.0114 - val_loss: 1.0002 - val_mape_loss: 1.0001 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0183 - mape_loss: 1.0183 - val_loss: 1.0071 - val_mape_loss: 1.0057 - learning_rate: 5.0000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0052 - mape_loss: 1.0052 - val_loss: 1.0002 - val_mape_loss: 1.0005 - learning_rate: 5.0000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0100 - mape_loss: 1.0100 - val_loss: 1.0031 - val_mape_loss: 1.0025 - learning_rate: 5.0000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0021 - mape_loss: 1.0021 - val_loss: 1.0026 - val_mape_loss: 1.0021 - learning_rate: 5.0000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0042 - mape_loss: 1.0042 - val_loss: 1.0022 - val_mape_loss: 1.0018 - learning_rate: 5.0000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0037 - mape_loss: 1.0037 - val_loss: 1.0000 - val_mape_loss: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0089 - mape_loss: 1.0089 - val_loss: 1.0051 - val_mape_loss: 1.0041 - learning_rate: 5.0000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0040 - mape_loss: 1.0040 - val_loss: 1.0012 - val_mape_loss: 1.0013 - learning_rate: 5.0000e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0089 - mape_loss: 1.0089 - val_loss: 1.0065 - val_mape_loss: 1.0053 - learning_rate: 5.0000e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0052 - mape_loss: 1.0051 - val_loss: 1.0054 - val_mape_loss: 1.0043 - learning_rate: 5.0000e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0115 - mape_loss: 1.0115 - val_loss: 1.0054 - val_mape_loss: 1.0044 - learning_rate: 5.0000e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0122 - mape_loss: 1.0122 - val_loss: 1.0014 - val_mape_loss: 1.0009 - learning_rate: 5.0000e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0082 - mape_loss: 1.0082 - val_loss: 1.0032 - val_mape_loss: 1.0029 - learning_rate: 5.0000e-04\n",
      "Epoch 55/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0021 - mape_loss: 1.0021 - val_loss: 1.0035 - val_mape_loss: 1.0030 - learning_rate: 5.0000e-04\n",
      "Epoch 56/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0043 - mape_loss: 1.0042 - val_loss: 1.0033 - val_mape_loss: 1.0026 - learning_rate: 5.0000e-04\n",
      "Epoch 57/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0103 - mape_loss: 1.0103 - val_loss: 1.0064 - val_mape_loss: 1.0054 - learning_rate: 5.0000e-04\n",
      "Epoch 58/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0086 - mape_loss: 1.0086 - val_loss: 1.0036 - val_mape_loss: 1.0032 - learning_rate: 2.5000e-04\n",
      "Epoch 59/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9996 - mape_loss: 0.9996 - val_loss: 0.9994 - val_mape_loss: 0.9998 - learning_rate: 2.5000e-04\n",
      "Epoch 60/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0021 - mape_loss: 1.0020 - val_loss: 1.0036 - val_mape_loss: 1.0030 - learning_rate: 2.5000e-04\n",
      "Epoch 61/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0011 - mape_loss: 1.0011 - val_loss: 1.0024 - val_mape_loss: 1.0021 - learning_rate: 2.5000e-04\n",
      "Epoch 62/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0053 - mape_loss: 1.0053 - val_loss: 1.0036 - val_mape_loss: 1.0031 - learning_rate: 2.5000e-04\n",
      "Epoch 63/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0015 - mape_loss: 1.0015 - val_loss: 1.0020 - val_mape_loss: 1.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 64/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0035 - mape_loss: 1.0035 - val_loss: 1.0040 - val_mape_loss: 1.0033 - learning_rate: 2.5000e-04\n",
      "Epoch 65/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0002 - mape_loss: 1.0002 - val_loss: 1.0021 - val_mape_loss: 1.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 66/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0019 - mape_loss: 1.0019 - val_loss: 1.0022 - val_mape_loss: 1.0021 - learning_rate: 2.5000e-04\n",
      "Epoch 67/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0017 - mape_loss: 1.0017 - val_loss: 0.9994 - val_mape_loss: 0.9997 - learning_rate: 2.5000e-04\n",
      "Epoch 68/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0047 - mape_loss: 1.0046 - val_loss: 1.0061 - val_mape_loss: 1.0051 - learning_rate: 2.5000e-04\n",
      "Epoch 69/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.0006 - mape_loss: 1.0006 - val_loss: 1.0017 - val_mape_loss: 1.0016 - learning_rate: 2.5000e-04\n",
      "Epoch 70/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0012 - mape_loss: 1.0012 - val_loss: 1.0015 - val_mape_loss: 1.0015 - learning_rate: 1.2500e-04\n",
      "Epoch 71/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0007 - mape_loss: 1.0007 - val_loss: 1.0010 - val_mape_loss: 1.0009 - learning_rate: 1.2500e-04\n",
      "Epoch 72/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0011 - mape_loss: 1.0011 - val_loss: 1.0032 - val_mape_loss: 1.0029 - learning_rate: 1.2500e-04\n",
      "Epoch 73/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0026 - mape_loss: 1.0026 - val_loss: 1.0009 - val_mape_loss: 1.0010 - learning_rate: 1.2500e-04\n",
      "Epoch 74/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0011 - mape_loss: 1.0011 - val_loss: 1.0000 - val_mape_loss: 1.0003 - learning_rate: 1.2500e-04\n",
      "Epoch 75/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0002 - mape_loss: 1.0002 - val_loss: 1.0021 - val_mape_loss: 1.0018 - learning_rate: 1.2500e-04\n",
      "Epoch 76/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0006 - mape_loss: 1.0006 - val_loss: 1.0005 - val_mape_loss: 1.0007 - learning_rate: 1.2500e-04\n",
      "Epoch 77/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0006 - mape_loss: 1.0006 - val_loss: 1.0010 - val_mape_loss: 1.0011 - learning_rate: 1.2500e-04\n",
      "Epoch 78/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0022 - mape_loss: 1.0022 - val_loss: 1.0020 - val_mape_loss: 1.0019 - learning_rate: 1.2500e-04\n",
      "Epoch 79/300\n",
      "\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0021 - mape_loss: 1.0021 - val_loss: 1.0011 - val_mape_loss: 1.0011 - learning_rate: 1.2500e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "ğŸ“‰ Validation MAPE: 1.35098\n",
      "Epoch 1/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.2079 - mape_loss: 9.1995    \n",
      "Epoch 2/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9172 - mape_loss: 5.9055\n",
      "Epoch 3/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.4034 - mape_loss: 25.3791\n",
      "Epoch 4/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1083 - mape_loss: 4.1041\n",
      "Epoch 5/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19.5580 - mape_loss: 19.5322\n",
      "Epoch 6/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7135 - mape_loss: 5.7087\n",
      "Epoch 7/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9069 - mape_loss: 3.9025\n",
      "Epoch 8/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.1190 - mape_loss: 7.1048\n",
      "Epoch 9/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3785 - mape_loss: 4.3715\n",
      "Epoch 10/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.8686 - mape_loss: 30.8423\n",
      "Epoch 11/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.8540 - mape_loss: 10.8275\n",
      "Epoch 12/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.5550 - mape_loss: 11.5298\n",
      "Epoch 13/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6525 - mape_loss: 3.6467\n",
      "Epoch 14/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.3473 - mape_loss: 10.3207\n",
      "Epoch 15/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3541 - mape_loss: 3.3463\n",
      "Epoch 16/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.3193 - mape_loss: 14.3136 \n",
      "Epoch 17/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4377 - mape_loss: 2.4343\n",
      "Epoch 18/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 25.5809 - mape_loss: 25.5592\n",
      "Epoch 19/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5898 - mape_loss: 5.5855\n",
      "Epoch 20/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8714 - mape_loss: 2.8685\n",
      "Epoch 21/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6406 - mape_loss: 2.6396\n",
      "Epoch 22/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0779 - mape_loss: 1.0780\n",
      "Epoch 23/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1381 - mape_loss: 5.1333\n",
      "Epoch 24/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0795 - mape_loss: 7.0705\n",
      "Epoch 25/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9463 - mape_loss: 2.9435\n",
      "Epoch 26/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0535 - mape_loss: 2.0493\n",
      "Epoch 27/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0415 - mape_loss: 1.0414\n",
      "Epoch 28/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0312 - mape_loss: 1.0311\n",
      "Epoch 29/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0213 - mape_loss: 1.0212\n",
      "Epoch 30/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0225 - mape_loss: 1.0225\n",
      "Epoch 31/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0173 - mape_loss: 1.0173\n",
      "Epoch 32/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0093 - mape_loss: 1.0093\n",
      "Epoch 33/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3826 - mape_loss: 1.3817\n",
      "Epoch 34/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0353 - mape_loss: 1.0352\n",
      "Epoch 35/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0082 - mape_loss: 1.0082\n",
      "Epoch 36/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0072 - mape_loss: 1.0072\n",
      "Epoch 37/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0342 - mape_loss: 1.0339\n",
      "Epoch 38/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0173 - mape_loss: 1.0173\n",
      "Epoch 39/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0310 - mape_loss: 1.0310\n",
      "Epoch 40/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0136 - mape_loss: 1.0135\n",
      "Epoch 41/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0241 - mape_loss: 1.0240\n",
      "Epoch 42/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0277 - mape_loss: 1.0277\n",
      "Epoch 43/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0139 - mape_loss: 1.0139\n",
      "Epoch 44/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0347 - mape_loss: 1.0347\n",
      "Epoch 45/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0086 - mape_loss: 1.0085\n",
      "Epoch 46/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0129 - mape_loss: 1.0129\n",
      "Epoch 47/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0069 - mape_loss: 1.0069\n",
      "Epoch 48/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0286 - mape_loss: 1.0286\n",
      "Epoch 49/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0129 - mape_loss: 1.0129\n",
      "Epoch 50/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0049 - mape_loss: 1.0049\n",
      "Epoch 51/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0093 - mape_loss: 1.0092\n",
      "Epoch 52/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0138 - mape_loss: 1.0138\n",
      "Epoch 53/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0461 - mape_loss: 1.0461\n",
      "Epoch 54/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0044 - mape_loss: 1.0044\n",
      "Epoch 55/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0080 - mape_loss: 1.0080\n",
      "Epoch 56/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0093 - mape_loss: 1.0093\n",
      "Epoch 57/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0089 - mape_loss: 1.0089\n",
      "Epoch 58/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0102 - mape_loss: 1.0102\n",
      "Epoch 59/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0150 - mape_loss: 1.0150\n",
      "Epoch 60/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0112 - mape_loss: 1.0112\n",
      "Epoch 61/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0186 - mape_loss: 1.0186\n",
      "Epoch 62/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0036 - mape_loss: 1.0036\n",
      "Epoch 63/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0230 - mape_loss: 1.0230\n",
      "Epoch 64/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0040 - mape_loss: 1.0040\n",
      "Epoch 65/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0076 - mape_loss: 1.0075\n",
      "Epoch 66/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0017 - mape_loss: 1.0017\n",
      "Epoch 67/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0188 - mape_loss: 1.0187\n",
      "Epoch 68/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0083 - mape_loss: 1.0083\n",
      "Epoch 69/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0072 - mape_loss: 1.0072\n",
      "Epoch 70/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0071 - mape_loss: 1.0071\n",
      "Epoch 71/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0129 - mape_loss: 1.0129\n",
      "Epoch 72/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0117 - mape_loss: 1.0116\n",
      "Epoch 73/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0093 - mape_loss: 1.0093\n",
      "Epoch 74/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0226 - mape_loss: 1.0227\n",
      "Epoch 75/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0191 - mape_loss: 1.0191\n",
      "Epoch 76/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0167 - mape_loss: 1.0167\n",
      "Epoch 77/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0201 - mape_loss: 1.0201\n",
      "Epoch 78/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0148 - mape_loss: 1.0148\n",
      "Epoch 79/79\n",
      "\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0079 - mape_loss: 1.0079\n",
      "WARNING:tensorflow:5 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x17ddb7560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "âœ… Submission saved as `submission_fuelblendnet.csv`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, Input\n",
    "\n",
    "train_path = \"/Users/MacbookPro/LocalStorage/Developer/ShellAi/dataset/train.csv\"\n",
    "test_path = \"/Users/MacbookPro/LocalStorage/Developer/ShellAi/dataset/test.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")\n",
    "print(train_df.describe().T)\n",
    "\n",
    "# ========== Step 2: Preprocessing ==========\n",
    "X = train_df.iloc[:, :55].values  # 5 composition + 50 component properties\n",
    "Y = train_df.iloc[:, 55:].values  # 10 blend properties\n",
    "X_test = test_df.iloc[:, 1:].values  # skip ID\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y)\n",
    "\n",
    "# ========== Step 3: Residual ANN Model ==========\n",
    "def mape_loss(y_true, y_pred):\n",
    "    diff = tf.abs((y_true - y_pred) / tf.clip_by_value(tf.abs(y_true), 1e-8, tf.float32.max))\n",
    "    return tf.reduce_mean(diff)\n",
    "\n",
    "def residual_block(x, units):\n",
    "    shortcut = x\n",
    "    x = layers.Dense(units, activation='relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dense(units)(x)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_dim):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = layers.Dense(256, activation='relu')(inputs)\n",
    "    x = residual_block(x, 256)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = residual_block(x, 256)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(10)(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=mape_loss, metrics=[mape_loss])\n",
    "    return model\n",
    "\n",
    "# ========== Step 4: Train/Validation Split ==========\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_scaled, Y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "model = build_model(X_train.shape[1])\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========== Step 5: Validation MAPE ==========\n",
    "val_pred = model.predict(X_val)\n",
    "val_true = scaler_Y.inverse_transform(Y_val)\n",
    "val_pred_inv = scaler_Y.inverse_transform(val_pred)\n",
    "val_mape = mean_absolute_percentage_error(val_true, val_pred_inv)\n",
    "print(f\"ğŸ“‰ Validation MAPE: {val_mape:.5f}\")\n",
    "\n",
    "# ========== Step 6: Retrain on Full Dataset (team_size = 0) ==========\n",
    "model_full = build_model(X_scaled.shape[1])\n",
    "model_full.fit(\n",
    "    X_scaled, Y_scaled,\n",
    "    epochs=int(len(history.history['loss'])),  # same as before\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========== Step 7: Test Predictions ==========\n",
    "Y_test_scaled = model_full.predict(X_test_scaled)\n",
    "Y_test_final = scaler_Y.inverse_transform(Y_test_scaled)\n",
    "\n",
    "# ========== Step 8: Submission ==========\n",
    "submission = pd.DataFrame(Y_test_final, columns=[f'BlendProperty{i+1}' for i in range(10)])\n",
    "submission.insert(0, 'ID', test_df['ID'])\n",
    "submission.to_csv(\"submission_fuelblendnet.csv\", index=False)\n",
    "print(\"âœ… Submission saved as `submission_fuelblendnet.csv`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ShellAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
