{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaddef72",
   "metadata": {},
   "source": [
    "# Enhanced Feature Engineering for Fuel Blend Property Prediction\n",
    "\n",
    "This notebook implements advanced feature engineering techniques for predicting fuel blend properties. It includes:\n",
    "\n",
    "- **Breakthrough Feature Engineering**: Non-linear transformations, advanced aggregations\n",
    "- **Shell-specific Features**: RON-like blending, viscosity modeling, density corrections\n",
    "- **Statistical Features**: Comprehensive descriptive statistics, skewness, kurtosis\n",
    "- **Cross-property Interactions**: Component interactions and ratios\n",
    "- **Advanced Scaling**: Multiple scaling strategies for different model types\n",
    "- **Robust Ensemble**: Multiple algorithms with performance-based weighting\n",
    "\n",
    "The goal is to achieve 90+ score through sophisticated feature engineering and ensemble modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ba3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor, BayesianRidge, TheilSenRegressor, RANSACRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready for advanced feature engineering and ensemble modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6018f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing packages if needed\n",
    "try:\n",
    "    import lightgbm\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing LightGBM...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing XGBoost...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "\n",
    "try:\n",
    "    import catboost\n",
    "    print(\"‚úÖ CatBoost available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing CatBoost...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"catboost\"])\n",
    "\n",
    "try:\n",
    "    import pytorch_tabnet\n",
    "    print(\"‚úÖ TabNet available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ TabNet not available - will skip this model\")\n",
    "    \n",
    "print(\"‚úÖ Package check complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c77115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports (always available)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Optional imports with graceful handling\n",
    "available_models = {\n",
    "    'lightgbm': False,\n",
    "    'xgboost': False, \n",
    "    'catboost': False,\n",
    "    'tabnet': False\n",
    "}\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "    available_models['lightgbm'] = True\n",
    "    print(\"‚úÖ LightGBM imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not available - using RandomForest as primary model\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    available_models['xgboost'] = True\n",
    "    print(\"‚úÖ XGBoost imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    available_models['catboost'] = True\n",
    "    print(\"‚úÖ CatBoost imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CatBoost not available\")\n",
    "\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    available_models['tabnet'] = True\n",
    "    print(\"‚úÖ TabNet imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TabNet not available\")\n",
    "\n",
    "print(f\"\\nüìä Available models: {sum(available_models.values())}/4 optional models loaded\")\n",
    "print(\"üöÄ Core sklearn models always available - pipeline will work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_breakthrough_features(df, pca_model=None, scaler=None, fit_transformers=True):\n",
    "    \"\"\"\n",
    "    Optimized advanced feature engineering function for fuel blend property prediction.\n",
    "    \n",
    "    This function creates a comprehensive set of features including:\n",
    "    - Non-linear transformations (sqrt, log, square)\n",
    "    - Advanced weighted aggregations (harmonic, geometric means)\n",
    "    - Shell-specific blending rules (RON, viscosity, density, RVP)\n",
    "    - Statistical features (min, max, std, skew, kurtosis, IQR)\n",
    "    - Cross-property interactions\n",
    "    - PCA components\n",
    "    - Fraction-based features (entropy, Gini coefficient)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid fragmenting the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Base features\n",
    "    features = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    features += [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "    \n",
    "    print(f\"Starting with {len(features)} base features\")\n",
    "    \n",
    "    # Prepare all new features in dictionaries first, then concat all at once\n",
    "    new_features = {}\n",
    "    \n",
    "    # Enhanced interaction features with non-linear transformations\n",
    "    print(\"Creating enhanced interaction features...\")\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 11):\n",
    "            base_interaction = df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}']\n",
    "            new_features[f'frac{i}_prop{j}'] = base_interaction\n",
    "            new_features[f'frac{i}_prop{j}_sqrt'] = df[f'Component{i}_fraction'] * np.sqrt(np.abs(df[f'Component{i}_Property{j}']))\n",
    "            new_features[f'frac{i}_prop{j}_log'] = df[f'Component{i}_fraction'] * np.log(np.abs(df[f'Component{i}_Property{j}']) + 1)\n",
    "            new_features[f'frac{i}_prop{j}_square'] = df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] ** 2)\n",
    "            features.extend([f'frac{i}_prop{j}', f'frac{i}_prop{j}_sqrt', f'frac{i}_prop{j}_log', f'frac{i}_prop{j}_square'])\n",
    "    \n",
    "    # Advanced weighted features with multiple aggregation methods\n",
    "    print(\"Creating advanced weighted features...\")\n",
    "    for j in range(1, 11):\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "        \n",
    "        # Multiple weighted aggregations\n",
    "        weighted_mean = sum(df[f'Component{i}_fraction'] * df[f'Component{i}_Property{j}'] for i in range(1, 6))\n",
    "        new_features[f'weighted_mean_prop{j}'] = weighted_mean\n",
    "        \n",
    "        weighted_var = sum(df[f'Component{i}_fraction'] * (df[f'Component{i}_Property{j}'] - weighted_mean) ** 2 for i in range(1, 6))\n",
    "        new_features[f'weighted_var_prop{j}'] = weighted_var\n",
    "        \n",
    "        # Harmonic mean (important for fuel properties)\n",
    "        safe_props = [np.maximum(df[f'Component{i}_Property{j}'], 1e-6) for i in range(1, 6)]\n",
    "        harmonic_mean = sum(df[f'Component{i}_fraction'] / safe_props[i-1] for i in range(1, 6))\n",
    "        new_features[f'harmonic_mean_prop{j}'] = 1 / harmonic_mean\n",
    "        \n",
    "        # Geometric mean (for multiplicative properties)\n",
    "        log_geo_mean = sum(df[f'Component{i}_fraction'] * np.log(safe_props[i-1]) for i in range(1, 6))\n",
    "        new_features[f'geometric_mean_prop{j}'] = np.exp(log_geo_mean)\n",
    "        \n",
    "        # Component dominance with ranking\n",
    "        frac_array = np.array([df[f'Component{i}_fraction'] for i in range(1, 6)])\n",
    "        dominant_idx = np.argmax(frac_array, axis=0)\n",
    "        new_features[f'dominant_prop{j}'] = df.apply(lambda row:\n",
    "            row[f'Component{dominant_idx[row.name] + 1}_Property{j}'], axis=1)\n",
    "        \n",
    "        # Blend balance and diversity\n",
    "        frac_data = df[frac_cols]\n",
    "        new_features[f'blend_balance_prop{j}'] = 1 - frac_data.std(axis=1)\n",
    "        new_features[f'blend_diversity_prop{j}'] = frac_data.std(axis=1) / (frac_data.mean(axis=1) + 1e-8)\n",
    "        \n",
    "        features.extend([\n",
    "            f'weighted_mean_prop{j}', f'weighted_var_prop{j}', f'harmonic_mean_prop{j}',\n",
    "            f'geometric_mean_prop{j}', f'dominant_prop{j}', f'blend_balance_prop{j}', f'blend_diversity_prop{j}'\n",
    "        ])\n",
    "    \n",
    "    # Advanced statistics\n",
    "    print(\"Creating advanced statistical features...\")\n",
    "    for j in range(1, 11):\n",
    "        prop_cols = [f'Component{i}_Property{j}' for i in range(1, 6)]\n",
    "        prop_data = df[prop_cols]\n",
    "        \n",
    "        new_features[f'min_prop{j}'] = prop_data.min(axis=1)\n",
    "        new_features[f'max_prop{j}'] = prop_data.max(axis=1)\n",
    "        new_features[f'mean_prop{j}'] = prop_data.mean(axis=1)\n",
    "        new_features[f'std_prop{j}'] = prop_data.std(axis=1)\n",
    "        new_features[f'median_prop{j}'] = prop_data.median(axis=1)\n",
    "        new_features[f'skew_prop{j}'] = prop_data.apply(lambda row: skew(row), axis=1)\n",
    "        new_features[f'kurtosis_prop{j}'] = prop_data.apply(lambda row: kurtosis(row), axis=1)\n",
    "        new_features[f'range_prop{j}'] = new_features[f'max_prop{j}'] - new_features[f'min_prop{j}']\n",
    "        new_features[f'iqr_prop{j}'] = prop_data.quantile(0.75, axis=1) - prop_data.quantile(0.25, axis=1)\n",
    "        \n",
    "        features.extend([\n",
    "            f'min_prop{j}', f'max_prop{j}', f'mean_prop{j}',\n",
    "            f'std_prop{j}', f'median_prop{j}', f'skew_prop{j}', f'kurtosis_prop{j}',\n",
    "            f'range_prop{j}', f'iqr_prop{j}'\n",
    "        ])\n",
    "    \n",
    "    # Shell-specific advanced features\n",
    "    print(\"Creating Shell-specific blending features...\")\n",
    "    for j in range(1, 11):\n",
    "        fractions = [df[f'Component{i}_fraction'] for i in range(1, 6)]\n",
    "        props = [df[f'Component{i}_Property{j}'] for i in range(1, 6)]\n",
    "        safe_props = [np.maximum(p, 1e-6) for p in props]\n",
    "        \n",
    "        # RON-like blending (non-linear octane)\n",
    "        ron_blend = sum(f * (r ** 1.5) for f, r in zip(fractions, safe_props)) ** (1 / 1.5)\n",
    "        new_features[f'ron_like_blend_prop{j}'] = ron_blend\n",
    "        \n",
    "        # Viscosity-like blending (logarithmic)\n",
    "        log_visc_blend = sum(f * np.log(r) for f, r in zip(fractions, safe_props))\n",
    "        new_features[f'log_visc_blend_prop{j}'] = log_visc_blend\n",
    "        \n",
    "        # Density-like blending (linear but with corrections)\n",
    "        density_blend = sum(f * r for f, r in zip(fractions, safe_props))\n",
    "        new_features[f'density_blend_prop{j}'] = density_blend\n",
    "        \n",
    "        # Reid vapor pressure-like (exponential)\n",
    "        rvp_blend = sum(f * np.exp(r / 100) for f, r in zip(fractions, safe_props))\n",
    "        new_features[f'rvp_blend_prop{j}'] = rvp_blend\n",
    "        \n",
    "        features.extend([\n",
    "            f'ron_like_blend_prop{j}', f'log_visc_blend_prop{j}',\n",
    "            f'density_blend_prop{j}', f'rvp_blend_prop{j}'\n",
    "        ])\n",
    "    \n",
    "    # Cross-property interactions (most important combinations)\n",
    "    print(\"Creating cross-property interactions...\")\n",
    "    for j1 in range(1, 6):\n",
    "        for j2 in range(j1 + 1, 7):\n",
    "            if j2 <= 10:  # Make sure we don't exceed property range\n",
    "                interaction = new_features[f'weighted_mean_prop{j1}'] * new_features[f'weighted_mean_prop{j2}']\n",
    "                ratio = new_features[f'weighted_mean_prop{j1}'] / (new_features[f'weighted_mean_prop{j2}'] + 1e-8)\n",
    "                new_features[f'prop{j1}_prop{j2}_interaction'] = interaction\n",
    "                new_features[f'prop{j1}_prop{j2}_ratio'] = ratio\n",
    "                features.extend([f'prop{j1}_prop{j2}_interaction', f'prop{j1}_prop{j2}_ratio'])\n",
    "    \n",
    "    # Enhanced PCA with more components\n",
    "    print(\"Creating PCA features...\")\n",
    "    prop_features = [f'Component{i}_Property{j}' for i in range(1, 6) for j in range(1, 11)]\n",
    "    if fit_transformers:\n",
    "        pca = PCA(n_components=12, random_state=42)\n",
    "        pca_feats = pca.fit_transform(df[prop_features])\n",
    "    else:\n",
    "        pca = pca_model\n",
    "        pca_feats = pca.transform(df[prop_features])\n",
    "    \n",
    "    for k in range(12):\n",
    "        new_features[f'pca_prop_{k+1}'] = pca_feats[:, k]\n",
    "        features.append(f'pca_prop_{k+1}')\n",
    "    \n",
    "    # Fraction-based advanced features\n",
    "    print(\"Creating fraction-based features...\")\n",
    "    frac_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    frac_data = df[frac_cols]\n",
    "    \n",
    "    new_features['frac_sum'] = frac_data.sum(axis=1)\n",
    "    new_features['frac_std'] = frac_data.std(axis=1)\n",
    "    new_features['frac_skew'] = frac_data.apply(lambda row: skew(row), axis=1)\n",
    "    new_features['frac_kurtosis'] = frac_data.apply(lambda row: kurtosis(row), axis=1)\n",
    "    new_features['frac_entropy'] = -sum(df[f'Component{i}_fraction'] * np.log(df[f'Component{i}_fraction'] + 1e-8) for i in range(1, 6))\n",
    "    new_features['frac_gini'] = 1 - sum(df[f'Component{i}_fraction'] ** 2 for i in range(1, 6))\n",
    "    \n",
    "    features.extend(['frac_sum', 'frac_std', 'frac_skew', 'frac_kurtosis', 'frac_entropy', 'frac_gini'])\n",
    "    \n",
    "    # Convert new features to DataFrame and concatenate all at once\n",
    "    print(\"Combining all features...\")\n",
    "    new_features_df = pd.DataFrame(new_features, index=df.index)\n",
    "    df_enhanced = pd.concat([df, new_features_df], axis=1)\n",
    "    \n",
    "    print(f\"Feature engineering complete! Created {len(features)} total features.\")\n",
    "    return df_enhanced, features, pca\n",
    "\n",
    "print(\"Optimized feature engineering function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # Try to load from current directory first\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    print(\"‚úÖ Data loaded from current directory\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Try to load from dataset directory\n",
    "        train = pd.read_csv('../../dataset/train.csv')\n",
    "        test = pd.read_csv('../../dataset/test.csv')\n",
    "        print(\"‚úÖ Data loaded from dataset directory\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: train.csv and test.csv not found. Please ensure the data files are available.\")\n",
    "        raise\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Training columns: {list(train.columns)}\")\n",
    "\n",
    "# Apply breakthrough feature engineering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING BREAKTHROUGH FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_enhanced, feat_cols, pca_model = create_breakthrough_features(train, fit_transformers=True)\n",
    "test_enhanced, _, _ = create_breakthrough_features(test, pca_model=pca_model, fit_transformers=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering complete!\")\n",
    "print(f\"Original features: 55 (5 fractions + 50 properties)\")\n",
    "print(f\"Enhanced features: {len(feat_cols)}\")\n",
    "print(f\"Feature increase: {len(feat_cols) - 55}x more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION AND FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define target variables\n",
    "TARGETS = [f'BlendProperty{i}' for i in range(1, 11)]\n",
    "print(f\"Target variables: {TARGETS}\")\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train = train_enhanced[feat_cols]\n",
    "y_train = train_enhanced[TARGETS]\n",
    "X_test = test_enhanced[feat_cols]\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training targets shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "\n",
    "# Handle NaN values\n",
    "print(\"\\nHandling NaN values...\")\n",
    "nan_before_train = X_train.isnull().sum().sum()\n",
    "nan_before_test = X_test.isnull().sum().sum()\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "print(f\"NaN values in training data: {nan_before_train} -> 0\")\n",
    "print(f\"NaN values in test data: {nan_before_test} -> 0\")\n",
    "\n",
    "# Feature scaling for different model types\n",
    "print(\"\\nCreating multiple scaling strategies...\")\n",
    "\n",
    "# Robust scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_train_robust = scaler_robust.fit_transform(X_train)\n",
    "X_test_robust = scaler_robust.transform(X_test)\n",
    "print(\"‚úÖ Robust scaling complete\")\n",
    "\n",
    "# Standard scaling (for linear models)\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_standard = scaler_standard.transform(X_test)\n",
    "print(\"‚úÖ Standard scaling complete\")\n",
    "\n",
    "# Feature selection for computational efficiency\n",
    "print(\"\\nPerforming feature selection...\")\n",
    "selector = SelectFromModel(\n",
    "    LGBMRegressor(n_estimators=200, random_state=42, verbose=-1),\n",
    "    prefit=False,\n",
    "    threshold='median'\n",
    ")\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train, y_train.iloc[:, 0])\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = [feat_cols[i] for i in range(len(feat_cols)) if selector.get_support()[i]]\n",
    "print(f\"‚úÖ Feature selection complete:\")\n",
    "print(f\"  Original features: {len(feat_cols)}\")\n",
    "print(f\"  Selected features: {len(selected_features)}\")\n",
    "print(f\"  Reduction: {(1 - len(selected_features)/len(feat_cols))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Ensemble Training with Cross-Validation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BREAKTHROUGH ENSEMBLE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_preds = np.zeros((X_test.shape[0], len(TARGETS)))\n",
    "\n",
    "# Check if LightGBM is available\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "    lgb_available = True\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    lgb_available = False\n",
    "    print(\"‚ö†Ô∏è LightGBM not available - using RandomForest as primary model\")\n",
    "\n",
    "print(f\"Cross-validation: {kf.get_n_splits()}-fold\")\n",
    "print(f\"Features: {len(feat_cols)} (selected: {len(selected_features)})\")\n",
    "print(f\"Models: 7 core models with advanced ensemble weighting\")\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training for {target} ({i+1}/{len(TARGETS)})\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Initialize out-of-fold predictions for each model\n",
    "    primary_oof = np.zeros(X_train.shape[0])  # LightGBM or RandomForest\n",
    "    rf_oof = np.zeros(X_train.shape[0])\n",
    "    et_oof = np.zeros(X_train.shape[0])\n",
    "    gb_oof = np.zeros(X_train.shape[0])\n",
    "    ridge_oof = np.zeros(X_train.shape[0])\n",
    "    elastic_oof = np.zeros(X_train.shape[0])\n",
    "    huber_oof = np.zeros(X_train.shape[0])\n",
    "    \n",
    "    # Initialize test predictions for each model\n",
    "    primary_test_preds = np.zeros(X_test.shape[0])\n",
    "    rf_test_preds = np.zeros(X_test.shape[0])\n",
    "    et_test_preds = np.zeros(X_test.shape[0])\n",
    "    gb_test_preds = np.zeros(X_test.shape[0])\n",
    "    ridge_test_preds = np.zeros(X_test.shape[0])\n",
    "    elastic_test_preds = np.zeros(X_test.shape[0])\n",
    "    huber_test_preds = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    # Cross-validation training\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"  Fold {fold + 1}/{kf.get_n_splits()}\", end=\" - \")\n",
    "        \n",
    "        # Model 1: Primary model (LightGBM if available, otherwise RandomForest)\n",
    "        if lgb_available:\n",
    "            model_primary = LGBMRegressor(\n",
    "                n_estimators=5000,  # Reduced from 12000\n",
    "                learning_rate=0.01,  # Increased from 0.0015 for faster convergence\n",
    "                random_state=fold,\n",
    "                num_leaves=31, \n",
    "                subsample=0.85, \n",
    "                colsample_bytree=0.85,\n",
    "                reg_alpha=0.01, \n",
    "                reg_lambda=0.01, \n",
    "                min_child_samples=20,\n",
    "                objective='regression_l1',\n",
    "                verbose=-1  # Silent training\n",
    "            )\n",
    "            \n",
    "            model_primary.fit(\n",
    "                X_train.iloc[tr_idx], y_train[target].iloc[tr_idx],\n",
    "                eval_set=[(X_train.iloc[val_idx], y_train[target].iloc[val_idx])],\n",
    "                callbacks=[early_stopping(stopping_rounds=100), log_evaluation(0)]\n",
    "            )\n",
    "            primary_name = \"LightGBM\"\n",
    "        else:\n",
    "            model_primary = RandomForestRegressor(\n",
    "                n_estimators=1000, max_depth=20, min_samples_split=5,\n",
    "                min_samples_leaf=2, random_state=fold, n_jobs=-1\n",
    "            )\n",
    "            model_primary.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "            primary_name = \"RandomForest\"\n",
    "        \n",
    "        primary_oof[val_idx] = model_primary.predict(X_train.iloc[val_idx])\n",
    "        primary_test_preds += model_primary.predict(X_test) / kf.get_n_splits()\n",
    "        \n",
    "        # Model 2: Random Forest (if not primary)\n",
    "        if lgb_available:\n",
    "            model_rf = RandomForestRegressor(\n",
    "                n_estimators=800, max_depth=20, min_samples_split=5,\n",
    "                min_samples_leaf=2, random_state=fold, n_jobs=-1\n",
    "            )\n",
    "            model_rf.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "            rf_oof[val_idx] = model_rf.predict(X_train.iloc[val_idx])\n",
    "            rf_test_preds += model_rf.predict(X_test) / kf.get_n_splits()\n",
    "        else:\n",
    "            # Use Extra Trees as second model if LightGBM not available\n",
    "            rf_oof[val_idx] = primary_oof[val_idx]  # Duplicate primary for ensemble\n",
    "            rf_test_preds += primary_test_preds / kf.get_n_splits()\n",
    "        \n",
    "        # Model 3: Extra Trees\n",
    "        model_et = ExtraTreesRegressor(\n",
    "            n_estimators=600, max_depth=18, min_samples_split=3,\n",
    "            min_samples_leaf=1, random_state=fold, n_jobs=-1\n",
    "        )\n",
    "        model_et.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        et_oof[val_idx] = model_et.predict(X_train.iloc[val_idx])\n",
    "        et_test_preds += model_et.predict(X_test) / kf.get_n_splits()\n",
    "        \n",
    "        # Model 4: Gradient Boosting\n",
    "        model_gb = GradientBoostingRegressor(\n",
    "            n_estimators=300,  # Reduced for faster training\n",
    "            learning_rate=0.05,  # Increased for faster convergence\n",
    "            max_depth=6,\n",
    "            min_samples_split=5, \n",
    "            min_samples_leaf=2, \n",
    "            random_state=fold\n",
    "        )\n",
    "        model_gb.fit(X_train.iloc[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        gb_oof[val_idx] = model_gb.predict(X_train.iloc[val_idx])\n",
    "        gb_test_preds += model_gb.predict(X_test) / kf.get_n_splits()\n",
    "        \n",
    "        # Model 5: Ridge (with robust scaling)\n",
    "        model_ridge = Ridge(alpha=0.1, random_state=fold)  # Increased alpha\n",
    "        model_ridge.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        ridge_oof[val_idx] = model_ridge.predict(X_train_robust[val_idx])\n",
    "        ridge_test_preds += model_ridge.predict(X_test_robust) / kf.get_n_splits()\n",
    "        \n",
    "        # Model 6: Elastic Net (with standard scaling)\n",
    "        model_elastic = ElasticNet(\n",
    "            alpha=0.01, \n",
    "            l1_ratio=0.3, \n",
    "            random_state=fold, \n",
    "            max_iter=5000  # Increased iterations\n",
    "        )\n",
    "        model_elastic.fit(X_train_standard[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        elastic_oof[val_idx] = model_elastic.predict(X_train_standard[val_idx])\n",
    "        elastic_test_preds += model_elastic.predict(X_test_standard) / kf.get_n_splits()\n",
    "        \n",
    "        # Model 7: Huber (robust to outliers) - Fixed convergence issue\n",
    "        model_huber = HuberRegressor(\n",
    "            alpha=0.1,  # Increased from 0.01 \n",
    "            epsilon=1.35,\n",
    "            max_iter=500,  # Increased from default\n",
    "            tol=1e-3  # More lenient tolerance\n",
    "        )\n",
    "        model_huber.fit(X_train_robust[tr_idx], y_train[target].iloc[tr_idx])\n",
    "        huber_oof[val_idx] = model_huber.predict(X_train_robust[val_idx])\n",
    "        huber_test_preds += model_huber.predict(X_test_robust) / kf.get_n_splits()\n",
    "        \n",
    "        print(\"‚úÖ\")\n",
    "    \n",
    "    # Calculate individual model MAPE scores\n",
    "    primary_mape = mean_absolute_percentage_error(y_train[target], primary_oof)\n",
    "    rf_mape = mean_absolute_percentage_error(y_train[target], rf_oof)\n",
    "    et_mape = mean_absolute_percentage_error(y_train[target], et_oof)\n",
    "    gb_mape = mean_absolute_percentage_error(y_train[target], gb_oof)\n",
    "    ridge_mape = mean_absolute_percentage_error(y_train[target], ridge_oof)\n",
    "    elastic_mape = mean_absolute_percentage_error(y_train[target], elastic_oof)\n",
    "    huber_mape = mean_absolute_percentage_error(y_train[target], huber_oof)\n",
    "    \n",
    "    # Advanced ensemble: Exponential weighting based on validation performance\n",
    "    mape_scores = [primary_mape, rf_mape, et_mape, gb_mape, ridge_mape, elastic_mape, huber_mape]\n",
    "    weights = [np.exp(-score * 10) for score in mape_scores]  # Exponential weighting\n",
    "    total_weight = sum(weights)\n",
    "    weights = [w / total_weight for w in weights]\n",
    "    \n",
    "    # Final ensemble predictions\n",
    "    final_preds[:, i] = (\n",
    "        weights[0] * primary_test_preds +\n",
    "        weights[1] * rf_test_preds +\n",
    "        weights[2] * et_test_preds +\n",
    "        weights[3] * gb_test_preds +\n",
    "        weights[4] * ridge_test_preds +\n",
    "        weights[5] * elastic_test_preds +\n",
    "        weights[6] * huber_test_preds\n",
    "    )\n",
    "    \n",
    "    # Calculate ensemble validation score\n",
    "    ensemble_oof = (\n",
    "        weights[0] * primary_oof +\n",
    "        weights[1] * rf_oof +\n",
    "        weights[2] * et_oof +\n",
    "        weights[3] * gb_oof +\n",
    "        weights[4] * ridge_oof +\n",
    "        weights[5] * elastic_oof +\n",
    "        weights[6] * huber_oof\n",
    "    )\n",
    "    \n",
    "    ensemble_mape = mean_absolute_percentage_error(y_train[target], ensemble_oof)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n  üìä {target} Results:\")\n",
    "    print(f\"     {primary_name}:     {primary_mape:.4f} (weight: {weights[0]:.3f})\")\n",
    "    print(f\"     Random Forest: {rf_mape:.4f} (weight: {weights[1]:.3f})\")\n",
    "    print(f\"     Extra Trees:   {et_mape:.4f} (weight: {weights[2]:.3f})\")\n",
    "    print(f\"     Gradient Boost: {gb_mape:.4f} (weight: {weights[3]:.3f})\")\n",
    "    print(f\"     Ridge:        {ridge_mape:.4f} (weight: {weights[4]:.3f})\")\n",
    "    print(f\"     Elastic Net:  {elastic_mape:.4f} (weight: {weights[5]:.3f})\")\n",
    "    print(f\"     Huber:        {huber_mape:.4f} (weight: {weights[6]:.3f})\")\n",
    "    print(f\"     üéØ ENSEMBLE:   {ensemble_mape:.4f}\")\n",
    "\n",
    "print(f\"\\nüéâ Training complete! All {len(TARGETS)} targets processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb228f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING SUBMISSION FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame(final_preds, columns=TARGETS)\n",
    "\n",
    "# Add ID column (assuming test data has ID column, otherwise create sequential IDs)\n",
    "if 'ID' in test_enhanced.columns:\n",
    "    submission.insert(0, 'ID', test_enhanced['ID'])\n",
    "    print(\"‚úÖ Using existing ID column from test data\")\n",
    "else:\n",
    "    submission.insert(0, 'ID', np.arange(1, len(test_enhanced) + 1))\n",
    "    print(\"‚úÖ Created sequential ID column\")\n",
    "\n",
    "# Save submission\n",
    "submission_filename = 'submission_enhanced_breakthrough.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"‚úÖ Submission saved as: {submission_filename}\")\n",
    "print(f\"üìä Submission shape: {submission.shape}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\\\nüìã Sample predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ BREAKTHROUGH ENSEMBLE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üî¨ Features: {len(feat_cols)} (selected: {len(selected_features)})\")\n",
    "print(f\"üîÑ Cross-validation: {kf.get_n_splits()}-fold\")\n",
    "print(f\"ü§ñ Models: LightGBM, Random Forest, Extra Trees, Gradient Boosting, Ridge, Elastic Net, Huber\")\n",
    "print(f\"‚öñÔ∏è  Ensemble: Exponential weighting based on validation performance\")\n",
    "print(f\"üìè Scaling: Robust and Standard scaling for different model types\")\n",
    "print(f\"üéØ Target: 90+ score with breakthrough features and advanced ensemble\")\n",
    "print(f\"üìÑ Output: {submission_filename}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\\\nüöÄ Ready for submission! The enhanced pipeline includes:\")\n",
    "print(\"   ‚Ä¢ Advanced non-linear transformations\")\n",
    "print(\"   ‚Ä¢ Shell-specific fuel blending rules\")\n",
    "print(\"   ‚Ä¢ Comprehensive statistical features\")\n",
    "print(\"   ‚Ä¢ Cross-property interactions\")\n",
    "print(\"   ‚Ä¢ Multiple scaling strategies\")\n",
    "print(\"   ‚Ä¢ Performance-weighted ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (Optional)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train a single LightGBM model on all data to get feature importance\n",
    "print(\"Training feature importance model...\")\n",
    "importance_model = LGBMRegressor(\n",
    "    n_estimators=1000, \n",
    "    random_state=42, \n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Use the first target for importance analysis\n",
    "importance_model.fit(X_train, y_train.iloc[:, 0])\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = importance_model.feature_importances_\n",
    "feature_names = feat_cols\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\\\nüîù Top 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Analyze feature types\n",
    "feature_types = {\n",
    "    'base': 0, 'interaction': 0, 'statistical': 0, \n",
    "    'shell_specific': 0, 'pca': 0, 'fraction': 0\n",
    "}\n",
    "\n",
    "for feature in importance_df['feature'].head(50):  # Top 50 features\n",
    "    if 'frac' in feature and 'prop' in feature:\n",
    "        feature_types['interaction'] += 1\n",
    "    elif any(stat in feature for stat in ['min_', 'max_', 'mean_', 'std_', 'skew_', 'kurtosis_']):\n",
    "        feature_types['statistical'] += 1\n",
    "    elif any(blend in feature for blend in ['ron_like', 'log_visc', 'density_blend', 'rvp_blend']):\n",
    "        feature_types['shell_specific'] += 1\n",
    "    elif 'pca' in feature:\n",
    "        feature_types['pca'] += 1\n",
    "    elif feature.startswith('frac_'):\n",
    "        feature_types['fraction'] += 1\n",
    "    else:\n",
    "        feature_types['base'] += 1\n",
    "\n",
    "print(\"\\\\nüìä Feature Type Distribution (Top 50):\")\n",
    "for ftype, count in feature_types.items():\n",
    "    print(f\"   {ftype.replace('_', ' ').title()}: {count}\")\n",
    "\n",
    "print(\"\\\\nüí° Insights:\")\n",
    "if feature_types['interaction'] > 10:\n",
    "    print(\"   ‚Ä¢ Interaction features are highly valuable\")\n",
    "if feature_types['shell_specific'] > 5:\n",
    "    print(\"   ‚Ä¢ Shell-specific blending rules are important\")\n",
    "if feature_types['statistical'] > 8:\n",
    "    print(\"   ‚Ä¢ Statistical aggregations provide good signal\")\n",
    "if feature_types['pca'] > 3:\n",
    "    print(\"   ‚Ä¢ PCA components capture meaningful patterns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
