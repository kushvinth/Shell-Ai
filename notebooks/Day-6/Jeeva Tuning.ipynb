{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd166e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'C' from 'sklearn.gaussian_process.kernels' (/Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgaussian_process\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianProcessRegressor\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgaussian_process\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkernels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m C, RBF\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ElasticNet\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'C' from 'sklearn.gaussian_process.kernels' (/Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import C, RBF\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define a function to get the trained model for each property based on the analysis\n",
    "def get_trained_final_model(data, target, property_name):\n",
    "    \"\"\"\n",
    "    Trains the best performing model for a specific blend property on the full training data.\n",
    "    \"\"\"\n",
    "    # Define the final models and their parameters based on the analysis\n",
    "    final_model_info = {\n",
    "        'BlendProperty1': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=2.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty2': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=2.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty3': ('ElasticNet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)),\n",
    "        'BlendProperty4': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=2.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty5': ('Random_Forest', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        'BlendProperty6': ('Gaussian_Process', make_pipeline(StandardScaler(), GaussianProcessRegressor(kernel=C(1.0, (1e-3, 1e3)) * RBF(length_scale=2.0), n_restarts_optimizer=5, random_state=42))),\n",
    "        'BlendProperty7': ('SVR_Poly', make_pipeline(StandardScaler(), SVR(kernel='poly', C=1.0, epsilon=0.1))),\n",
    "        'BlendProperty8': ('ElasticNet', ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)),\n",
    "        'BlendProperty9': ('Advanced_Ensemble', 'ADVANCED_HYPERTUNED'),  # Special handling for BlendProperty9\n",
    "        'BlendProperty10': ('Neural_Network', Sequential([Dense(64, activation='relu', input_shape=(data.shape[1],)), Dropout(0.2), Dense(64, activation='relu'), Dense(1)]))\n",
    "    }\n",
    "\n",
    "    model_name, model = final_model_info[property_name]\n",
    "\n",
    "    X = data\n",
    "    y = target\n",
    "\n",
    "    print(f\"Training {model_name} for {property_name} on full dataset...\")\n",
    "\n",
    "    if property_name == 'BlendProperty9':\n",
    "        # Special advanced hyperparameter tuning for BlendProperty9\n",
    "        return train_advanced_blendproperty9_model(X, y)\n",
    "    elif model_name == 'Neural_Network':\n",
    "        model.compile(optimizer='adam', loss='mae')\n",
    "        model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "    elif model_name == 'TabNet':\n",
    "         # TabNet requires numpy and potential scaling\n",
    "         X_np = X.values\n",
    "         y_np = y.values.reshape(-1, 1)\n",
    "         scaler = StandardScaler()\n",
    "         X_scaled = scaler.fit_transform(X_np)\n",
    "         model.fit(X_scaled, y_np, max_epochs=200, patience=20, batch_size=256, virtual_batch_size=128, verbose=0)\n",
    "         # Wrap TabNet model and scaler in a pipeline for consistent prediction interface\n",
    "         class TabNetPipeline:\n",
    "             def __init__(self, scaler, tabnet_model):\n",
    "                 self.scaler = scaler\n",
    "                 self.tabnet_model = tabnet_model\n",
    "             def predict(self, X):\n",
    "                 X_scaled = self.scaler.transform(X.values)\n",
    "                 return self.tabnet_model.predict(X_scaled).flatten()\n",
    "         model = TabNetPipeline(scaler, model) # Return the wrapped model\n",
    "    elif isinstance(model, Pipeline): # Check against the Pipeline class\n",
    "        model.fit(X, y) # Pipeline handles scaling internally\n",
    "    else:\n",
    "        model.fit(X, y)\n",
    "\n",
    "    print(f\"Training complete for {property_name}.\")\n",
    "    return model\n",
    "\n",
    "# Load test data and sample submission\n",
    "# Assuming test.csv and sample_solution.csv are in the current directory\n",
    "try:\n",
    "  test_df = pd.read_csv(\"test.csv\")\n",
    "  submission_df = pd.read_csv(\"sample_solution.csv\")\n",
    "  test_ids = test_df['ID']\n",
    "  test_df_features = test_df.drop(columns=['ID'])\n",
    "except FileNotFoundError:\n",
    "    print(\"Make sure 'test.csv' and 'sample_solution.csv' are uploaded to your Colab session.\")\n",
    "\n",
    "\n",
    "if 'test_df_features' in locals(): # Check if test data was loaded\n",
    "  # Generate predictions using the best model for each property\n",
    "  for i in range(1, 11):\n",
    "      property_name = f'BlendProperty{i}'\n",
    "      print(f\"\\nProcessing {property_name} for final submission...\")\n",
    "\n",
    "      # Define features for this property\n",
    "      features = ['Component1_fraction', 'Component2_fraction', 'Component3_fraction',\n",
    "                 'Component4_fraction', 'Component5_fraction'] + \\\n",
    "                [f'Component{j}_Property{i}' for j in range(1, 6)]\n",
    "\n",
    "      # Train the best model for this property on the full training data\n",
    "      trained_model = get_trained_final_model(df[features], df[property_name], property_name)\n",
    "\n",
    "      # Make predictions on the test data\n",
    "      test_predictions = trained_model.predict(test_df_features[features])\n",
    "\n",
    "      # Update the submission DataFrame\n",
    "      submission_df[property_name] = test_predictions\n",
    "\n",
    "  # Save the final submission file\n",
    "  submission_df.to_csv('final_model_submission.csv', index=False)\n",
    "\n",
    "  print(\"\\n\" + \"=\"*80)\n",
    "  print(\"Final submission file 'final_model_submission.csv' created successfully.\")\n",
    "  print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aeeff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering Functions\n",
    "def create_ratio_features(df):\n",
    "    \"\"\"Create ratio features between component fractions\"\"\"\n",
    "    comp_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    \n",
    "    for i in range(len(comp_cols)):\n",
    "        for j in range(i+1, len(comp_cols)):\n",
    "            df[f'ratio_{comp_cols[i]}_to_{comp_cols[j]}'] = df[comp_cols[i]] / (df[comp_cols[j]] + 1e-5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_interaction_features(df, property_idx):\n",
    "    \"\"\"Create interaction features between component fractions and properties\"\"\"\n",
    "    comp_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    prop_cols = [f'Component{i}_Property{property_idx}' for i in range(1, 6)]\n",
    "    \n",
    "    for comp in comp_cols:\n",
    "        for prop in prop_cols:\n",
    "            df[f'{comp}_x_{prop}'] = df[comp] * df[prop]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_polynomial_features(df, degree=2):\n",
    "    \"\"\"Create polynomial features for component fractions\"\"\"\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    comp_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False, interaction_only=True)\n",
    "    poly_features = poly.fit_transform(df[comp_cols])\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = poly.get_feature_names_out(comp_cols)\n",
    "    \n",
    "    # Add polynomial features to dataframe\n",
    "    for i, name in enumerate(feature_names):\n",
    "        if name not in comp_cols:  # Skip original features\n",
    "            df[f'poly_{name}'] = poly_features[:, i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_statistical_features(df, property_idx):\n",
    "    \"\"\"Create statistical features across components for each property\"\"\"\n",
    "    prop_cols = [f'Component{i}_Property{property_idx}' for i in range(1, 6)]\n",
    "    comp_cols = [f'Component{i}_fraction' for i in range(1, 6)]\n",
    "    \n",
    "    # Statistical features for properties\n",
    "    df[f'Property{property_idx}_mean'] = df[prop_cols].mean(axis=1)\n",
    "    df[f'Property{property_idx}_std'] = df[prop_cols].std(axis=1)\n",
    "    df[f'Property{property_idx}_min'] = df[prop_cols].min(axis=1)\n",
    "    df[f'Property{property_idx}_max'] = df[prop_cols].max(axis=1)\n",
    "    df[f'Property{property_idx}_range'] = df[f'Property{property_idx}_max'] - df[f'Property{property_idx}_min']\n",
    "    \n",
    "    # Weighted averages using component fractions\n",
    "    df[f'Property{property_idx}_weighted_avg'] = sum(df[f'Component{i}_fraction'] * df[f'Component{i}_Property{property_idx}'] \n",
    "                                                    for i in range(1, 6))\n",
    "    \n",
    "    # Component fraction statistics\n",
    "    df['Component_fraction_mean'] = df[comp_cols].mean(axis=1)\n",
    "    df['Component_fraction_std'] = df[comp_cols].std(axis=1)\n",
    "    df['Component_fraction_entropy'] = -sum(df[col] * np.log(df[col] + 1e-10) for col in comp_cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea46dd08",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <D44045CD-B874-3A27-9A61-F131D99AACE4> /Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_percentage_error\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# .basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# and its dependencies as early as possible\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/basic.py:9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Wrapper for C API of LightGBM.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This import causes lib_lightgbm.{dll,dylib,so} to be loaded.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# It's intentionally done here, as early as possible, to avoid issues like\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# \"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB  \u001b[38;5;66;03m# isort: skip\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/libpath.py:49\u001b[39m\n\u001b[32m     47\u001b[39m     _LIB = Mock(ctypes.CDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     _LIB = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_lib_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ctypes/__init__.py:460\u001b[39m, in \u001b[36mLibraryLoader.LoadLibrary\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ctypes/__init__.py:379\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <D44045CD-B874-3A27-9A61-F131D99AACE4> /Users/MacbookPro/LocalStorage/Developer/ShellAi/.venv/lib/python3.12/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "# Advanced Hyperparameter Tuning for BlendProperty9\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_advanced_ratio_features(df):\n",
    "    \"\"\"Create advanced ratio features between component fractions\"\"\"\n",
    "    comp_cols = df.columns[:5]\n",
    "    for i in range(len(comp_cols)):\n",
    "        for j in range(i+1, len(comp_cols)):\n",
    "            df[f'ratio_{comp_cols[i]}_to_{comp_cols[j]}'] = df[comp_cols[i]] / (df[comp_cols[j]] + 1e-5)\n",
    "    return df\n",
    "\n",
    "def create_advanced_interaction_features(df):\n",
    "    \"\"\"Create advanced interaction features between component fractions and properties\"\"\"\n",
    "    comp_cols = df.columns[:5]\n",
    "    prop_cols = df.columns[5:55]\n",
    "    for comp in comp_cols:\n",
    "        for prop in prop_cols:\n",
    "            df[f'{comp}_x_{prop}'] = df[comp] * df[prop]\n",
    "    return df\n",
    "\n",
    "def tune_model_blendproperty9(trial, model_type, X_scaled, y):\n",
    "    \"\"\"Hyperparameter tuning function for BlendProperty9\"\"\"\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 80),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300)\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "    elif model_type == 'xgb':\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300)\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    elif model_type == 'cat':\n",
    "        params = {\n",
    "            'depth': trial.suggest_int('depth', 4, 8),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),\n",
    "            'iterations': trial.suggest_int('iterations', 100, 300)\n",
    "        }\n",
    "        model = CatBoostRegressor(verbose=0, **params)\n",
    "\n",
    "    score = cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_mean_absolute_percentage_error', n_jobs=-1).mean()\n",
    "    return -score\n",
    "\n",
    "def train_advanced_blendproperty9_model(X, y):\n",
    "    \"\"\"\n",
    "    Advanced training function specifically for BlendProperty9 with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(\"Starting advanced hyperparameter tuning for BlendProperty9...\")\n",
    "    \n",
    "    # Feature Engineering for BlendProperty9\n",
    "    X_enhanced = X.copy()\n",
    "    X_enhanced = create_advanced_ratio_features(X_enhanced)\n",
    "    X_enhanced = create_advanced_interaction_features(X_enhanced)\n",
    "    \n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_enhanced)\n",
    "    \n",
    "    # Hyperparameter tuning with Optuna\n",
    "    best_params = {}\n",
    "    for model_type in ['lgb', 'xgb', 'cat']:\n",
    "        print(f\"Tuning {model_type} for BlendProperty9...\")\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: tune_model_blendproperty9(trial, model_type, X_scaled, y), n_trials=10)\n",
    "        best_params[model_type] = study.best_params\n",
    "        print(f\"Best {model_type} params: {study.best_params}\")\n",
    "    \n",
    "    # Train optimized models\n",
    "    lgb_model = LGBMRegressor(**best_params['lgb']).fit(X_scaled, y)\n",
    "    xgb_model = XGBRegressor(**best_params['xgb']).fit(X_scaled, y)\n",
    "    cat_model = CatBoostRegressor(verbose=0, **best_params['cat']).fit(X_scaled, y)\n",
    "    \n",
    "    # Train other models with optimized parameters\n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42).fit(X_scaled, y)\n",
    "    elastic = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42).fit(X_scaled, y)\n",
    "    ridge = Ridge(alpha=1.0).fit(X_scaled, y)\n",
    "    lasso = Lasso(alpha=0.001).fit(X_scaled, y)\n",
    "    gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42).fit(X_scaled, y)\n",
    "    \n",
    "    # Create ensemble model wrapper\n",
    "    class BlendProperty9EnsembleModel:\n",
    "        def __init__(self, models, scaler, feature_enhancer):\n",
    "            self.models = models\n",
    "            self.scaler = scaler\n",
    "            self.feature_enhancer = feature_enhancer\n",
    "        \n",
    "        def predict(self, X):\n",
    "            # Apply same feature engineering as training\n",
    "            X_enhanced = X.copy()\n",
    "            X_enhanced = create_advanced_ratio_features(X_enhanced)\n",
    "            X_enhanced = create_advanced_interaction_features(X_enhanced)\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.transform(X_enhanced)\n",
    "            \n",
    "            # Get predictions from all models\n",
    "            preds = []\n",
    "            for model in self.models:\n",
    "                preds.append(model.predict(X_scaled))\n",
    "            \n",
    "            # Return ensemble average\n",
    "            return np.mean(preds, axis=0)\n",
    "    \n",
    "    # Create ensemble model\n",
    "    models = [lgb_model, xgb_model, cat_model, rf, elastic, ridge, lasso, gbr]\n",
    "    ensemble_model = BlendProperty9EnsembleModel(models, scaler, None)\n",
    "    \n",
    "    # Evaluate ensemble on training data\n",
    "    y_pred_train = ensemble_model.predict(X)\n",
    "    mape = mean_absolute_percentage_error(y, y_pred_train)\n",
    "    print(f'BlendProperty9 Ensemble MAPE: {mape:.4f}')\n",
    "    \n",
    "    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for BlendProperty9 training\n",
    "try:\n",
    "    # Load training data\n",
    "    df = pd.read_csv('../../../dataset/train.csv')\n",
    "    print(f\"Training data loaded successfully. Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if BlendProperty9 exists\n",
    "    if 'BlendProperty9' in df.columns:\n",
    "        print(\"BlendProperty9 found in training data.\")\n",
    "        print(f\"BlendProperty9 statistics:\")\n",
    "        print(df['BlendProperty9'].describe())\n",
    "    else:\n",
    "        print(\"Warning: BlendProperty9 not found in training data.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Training data not found. Please ensure train.csv is in the correct location.\")\n",
    "    print(\"Expected location: ../../../dataset/train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ShellAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
